{"path":"Libros/apuntesfisicas.pdf","text":"F´ısicas Primer Curso ´ALGEBRA LINEAL Juan A. Navarro Gonz´alez 31 de enero de 2023 ´Indice 1. Preliminares 3 1.1. N´umeros Complejos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2. Permutaciones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.3. Matrices y Determinantes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4. Grupos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2. Espacios Vectoriales 11 2.1. Espacios Vectoriales y Subespacios Vectoriales . . . . . . . . . . . . . . . . . . 11 2.2. Teor´ıa de la Dimensi´on . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.3. Suma Directa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3. Aplicaciones Lineales 20 3.1. Matriz de una Aplicaci´on Lineal . . . . . . . . . . . . . . . . . . . . . . . . . 21 3.2. N´ucleo e Imagen de una Aplicaci´on Lineal . . . . . . . . . . . . . . . . . . . . 22 4. Geometr´ıa Eucl´ıdea 25 4.1. Producto Escalar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.2. Espacios Vectoriales Eucl´ıdeos . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 4.3. Proyecci´on Ortogonal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 5. Endomorﬁsmos 30 5.1. Valores y Vectores Propios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 5.2. Diagonalizaci´on de Endomorﬁsmos . . . . . . . . . . . . . . . . . . . . . . . . 32 5.3. Operadores Lineales Autoadjuntos . . . . . . . . . . . . . . . . . . . . . . . . 34 6. El Espacio Dual 37 6.1. Incidencia y Bidualidad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 6.2. Adjunto de un Operador Lineal . . . . . . . . . . . . . . . . . . . . . . . . . . 40 7. Formas Cuadr´aticas 42 7.1. M´etricas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 7.2. Clasiﬁcaci´on de M´etricas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 7.3. Cu´adricas Centrales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 8. Tensores 51 8.1. Producto Tensorial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 8.2. Contracci´on de ´Indices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 9. Tensores Alternados 57 9.1. Producto Exterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 9.2. Formas de Volumen y Orientaciones . . . . . . . . . . . . . . . . . . . . . . . 61 9.3. Determinantes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 1. Preliminares Un conjunto X est´a totalmente determinado por sus elementos, que se pueden contar, y su cardinal jXj es el n´umero de elementos de X (puede ser inﬁnito). El conjunto vac´ıo, que no tiene ning´un elemento, se denota ;. Otros conjuntos son: El conjunto de los N´umeros naturales N = f0, 1, 2, 3, . . .g El conjunto de los N´umeros enteros Z = f. . . , \u00003, \u00002, \u00001, 0, 1, 2, 3, . . .g El conjunto de los N´umeros racionales Q = fa/b : 9a, b 2 Z, b 6= 0g El conjunto de los N´umeros reales R = fn´umeros decimales inﬁnitosg Si X es un conjunto, x 2 X signiﬁca que x es un elemento de X. Si X e Y son conjuntos, Y \u0012 X signiﬁca que Y es un subconjunto de X, que todos los elementos de Y son elementos de X; i.e. y 2 Y ) y 2 X. Si Z es otro subconjunto de X, la uni´on Y [ Z e intersecci´on Y \\ Z son los siguientes subconjuntos de X: Y [ Z := fx 2 X : x 2 Y ´o x 2 Zg , Y \\ Z := fx 2 X : x 2 Y y x 2 Zg. 1.1. N´umeros Complejos Deﬁniciones: El conjunto C de los n´umeros complejos est´a formado por las parejas ordenadas de n´umeros reales x + yi (donde x 2 R se llama parte real de z e y 2 R se llama parte imaginaria) que se suman y multiplican con las siguientes reglas (i2 = \u00001): (x1 + y1i)+(x2 + y2i) := (x1 + x2) + (y1 + y2)i, (x1 + y1i)\u0001(x2 + y2i) := (x1x2 \u0000 y1y2) + (x1y2 + x2y1)i, y cada n´umero real x se identiﬁca con el n´umero complejo x + 0i, de modo que R ˆ C. Estas dos operaciones son asociativas, (z1 +z2)+z3 = z1 +(z2 +z3) y (z1z2)z3 = z1(z2z3), conmutativas, z1 + z2 = z2 + z1 y z1z2 = z2z1, y adem´as z1(z2 + z3) = z1z2 + z1z3. Si x, y 2 R, el conjugado de z = x + yi es el n´umero complejo ¯z := x \u0000 yi, de modo que un n´umero complejo z es real si y s´olo si z = ¯z. Si z = x + yi no es nulo, tenemos que z ¯z = x2 + y2 > 0, as´ı que su inverso existe y es z−1 = x x2 + y2 \u0000 y x2 + y2 i = ¯z z ¯z \u0001 El cociente de n´umeros complejos se deﬁne como u/z := uz−1 = (u¯z)/(z ¯z). Propiedades de la Conjugaci´on: ¯¯z = z, z + u = ¯z + ¯u, zu = ¯z ¯u, z−1 = ¯z−1, u/z = ¯u/¯z. Demostraci´on: Veamos las dos ´ultimas. Como 1 = zz−1, tenemos que 1 = zz−1 = ¯z \u0001 z−1. Luego z−1 = ¯z−1, y u/z = uz−1 = ¯u¯z−1 = ¯u/¯z. Deﬁnici´on: Si x, y 2 R, el m´odulo del n´umero complejo z = x + yi es el n´umero real jzj := +pz \u0001 ¯z = +p x2 + y2 , jzj2 = z \u0001 ¯z y es el valor absoluto cuando z es real. Un n´umero complejo z es nulo si y s´olo si jzj = 0. Propiedades del M´odulo: jzj = j¯zj, jzuj = jzj \u0001 juj, j 1 z j = 1 |z| , j u z j = |u| |z| , jz + uj \u0014 jzj + juj. Demostraci´on: Veamos la ´ultima. Pongamos z = x + yi con x, y 2 R. z + ¯z = x + yi + x \u0000 yi = 2x \u0014 2jxj = 2px2 \u0014 2p x2 + y2 = 2jzj. jz + uj 2 = (z + u)(z + u) = (z + u)(¯z + ¯u) = jzj 2 + juj 2 + z ¯u + ¯zu = jzj 2 + juj 2 + z ¯u + z ¯u \u0014 jzj2 + juj2 + 2jz ¯uj = jzj2 + juj2 + 2jzj \u0001 j¯uj = jzj 2 + juj 2 + 2jzj \u0001 juj = (jzj + juj)2 . 3 Deﬁnici´on: Para cada n´umero real t 2 R pondremos eti := cos t + i sen t donde el seno y coseno se consideran en radianes para que d dt (eit) = ieit. Luego e 0 = 1, y e2πi = 1 f´ormula de Euler (1707-1783). Las f´ormulas del seno y coseno de una suma expresan que etiet ′i = e(t+t ′)i etiet′i = (cos t + i sen t)(cos t′ + i sen t′) = \u0000(cos t)(cos t′) \u0000 (sen t)(sen t′) \u0001+ + i \u0000(cos t)(sen t′) + (sen t)(cos t ′)\u0001 = cos(t + t ′) + i sen(t + t′) = e (t+t′)i. En general jetij = pcos2 t + sen2 t = 1, y todo n´umero complejo de m´odulo 1 es eθi para alg´un n´umero real θ, ´unico salvo la adici´on de un m´ultiplo entero de 2π. Deﬁnici´on: Si z 2 C es de m´odulo ρ 6= 0, entonces el m´odulo de z/ρ es 1, as´ı que z = ρeθi = ρ(cos θ + i sen θ) para alg´un n´umero real θ = arg z, llamado argumento de z (bien deﬁnido salvo la adici´on de un m´ultiplo entero de 2π). Cuando z = x + yi, con x, y 2 R, por deﬁnici´on el argumento de z es cualquier n´umero real θ que cumpla cos θ = x ρ = x px2 + y2 , sen θ = y ρ = y px2 + y2 , tg θ = y x \u0001 arg z = \u0006 arc cos(x/ρ) , donde el signo es el de la parte imaginaria y. Ejemplos: Si a 2 R es positivo, arg a = 0, arg (ai) = π/2, arg (\u0000a) = π, arg (\u0000ai) = 3π/2. Propiedades del Argumento: arg (z \u0001 z′) = (arg z) + (arg z′), arg z−1 = arg ¯z = \u0000arg z, arg (u/z) = arg u \u0000 arg z. Demostraci´on: Se sigue de las igualdades zz′ = (ρe θi)(ρ′eθ′i) = ρρ ′e (θ+θ′)i, jzz′j = ρρ′; y de que z−1z = 1 y z ¯z = jzj 2 tienen argumento nulo, porque son n´umeros reales positivos. Funciones Elementales: Dado un n´umero complejo z = x + yi, x, y 2 R, pondremos ez := exeyi = ex(cos y + i sen y) de modo que ez 6= 0, y ez′+z = ez′ez para cualesquiera n´umeros complejos z′, z. Deﬁnimos el seno y coseno de un n´umero complejo z por las igualdades cos z := ezi + e−zi 2 , sen z := ezi \u0000 e−zi 2i \u0001 Cuando eu = z, decimos que u es un logaritmo (neperiano) de z, y ponemos u = ln z. El logaritmo de un n´umero complejo no nulo z = ρe θi = eln ρe θi = eln ρ+θi es ln z = ln ρ + (θ + 2kπ)i , k 2 Z, y toma inﬁnitos valores, que se diferencian en un m´ultiplo entero de 2πi. Por ejemplo, cos i = 1 2 (e −1 + e) ˇ 1 ′543 es un n´umero real mayor que 1, y ln(\u00001) = πi. 4 Ra´ıces n-´esimas Complejas: Dado un n´umero complejo no nulo a, pondremos a z := ez ln a = ez(ln a+2kπi) , k 2 Z, y puede tomar inﬁnitos valores. Sea z = ρeθi un n´umero complejo no nulo de m´odulo ρ y argumento θ. Sus ra´ıces n-´esimas complejas (n 2 N, n \u0015 2) son z1/n = e 1 n (ln ρ+(θ+2kπ)i) = npρ e( θ+2kπ n )i = \u0000 npρ e θ n i\u0001e 2kπ n i , k 2 Z, y, como k y k + n dan la misma ra´ız n-´esima, basta tomar k = 0, . . . , n \u0000 1. Vemos pues que todo n´umero complejo no nulo tiene n ra´ıces n-´esimas complejas, que forman un pol´ıgono regular inscrito en el c´ırculo centrado en el 0. En particular, las ra´ıces n-´esimas de la unidad son e 2kπ n i = (e 2π n i)k, k = 0, . . . , n \u0000 1; as´ı que las ra´ıces n-´esimas de un n´umero complejo no nulo se obtienen multiplicando una de ellas por las n ra´ıces n-´esimas de la unidad, que son las sucesivas potencias de e 2π n i. 1.2. Permutaciones Deﬁniciones: Sean X e Y dos conjuntos. Dar una aplicaci´on f : X ! Y es asignar a cada elemento x 2 X un ´unico elemento f (x) 2 Y , llamado imagen de x por la aplicaci´on f . Si g : Y ! Z es otra aplicaci´on, la composici´on de g y f es la aplicaci´on g \u000e f : X \u0000! Z, (g \u000e f )(x) := g\u0000f (x) \u0001. La identidad de un conjunto X es la aplicaci´on IdX : X ! X, IdX (x) = x. Sea f : X ! Y una aplicaci´on. Si A \u0012 X, ponemos f (A) := fy 2 Y : y = f (x), 9x 2 Ag = ff (x)gx∈A \u0012 Y y si B \u0012 Y , ponemos f −1(B) := fx 2 X : f (x) 2 Bg \u0012 X. Si y 2 Y , puede ocurrir que f −1(y) no tenga ning´un elemento o tenga m´as de uno, de modo que, en general, f −1 no es una aplicaci´on de Y en X. Diremos que f : X ! Y es inyectiva si elementos distintos tienen im´agenes distintas: x, y 2 X, f (x) = f (y) ) x = y (i.e., cuando, para cada y 2 Y se tiene que f −1(y) tiene un elemento o ninguno) y diremos que f es epiyectiva si todo elemento de Y es imagen de alg´un elemento de X: y 2 Y ) y = f (x) para alg´un x 2 X , es decir, cuando f (X) = Y o, lo que es igual, cuando para cada y 2 Y se cumple que f −1(y) tiene al menos un elemento. Diremos que f : X ! Y es biyectiva cuando es inyectiva y epiyectiva, cuando cada elemento y 2 Y es imagen de un ´unico elemento de X, de modo que f −1(y) tiene un ´unico elemento, y en tal caso f −1 : Y ! X s´ı es una aplicaci´on, llamada aplicaci´on inversa de f porque f −1 \u000e f = IdX y f \u000e f −1 = IdY . Deﬁniciones: Sea n un n´umero natural, n \u0015 2. Las permutaciones de n elementos son las aplicaciones biyectivas σ : f1, . . . , ng \u0000! f1, . . . , ng. El conjunto de todas las permutaciones de n elementos se denota Sn, y su cardinal es n! = n \u0001 (n \u0000 1) \u0001 . . . \u0001 2 \u0001 1. El producto de permutaciones es la composici´on de aplicaciones, y como son aplicaciones biyectivas, toda permutaci´on σ tienen una permutaci´on inversa σ−1, de modo que σ−1(j ) = i cuando σ(i) = j. Adem´as, (στ )−1 = τ −1σ−1. 5 Dados a1, . . . , ad 2 f1, . . . , ng distintos, (a1 . . . ad) denota la permutaci´on σ 2 Sn tal que σ(ai) = ai+1, entendiendo que σ(ad) = a1, y deja ﬁjos los restantes elementos. Dire- mos que (a1 . . . ad) es un ciclo de longitud d, y los ciclos (a1a2) de longitud 2 se llaman trasposiciones. El inverso de un ciclo σ = (a1 . . . ad) es σ−1 = (ad . . . a1). Diremos que dos ciclos (a1 . . . ad) y (b1 . . . bk) son disjuntos cuando ai 6= bj para todo par de ´ındices i, j; en cuyo caso conmutan: (a1 . . . ad)(b1 . . . bk) = (b1 . . . bk)(a1 . . . ad). Toda permutaci´on descompone en producto de ciclos disjuntos, y tambi´en en producto de trasposiciones, porque todo ciclo es producto de trasposiciones: (a1a2a3 . . . ad) = (a1a2)(a2a3) \u0001 \u0001 \u0001 (ad−1ad). (1) Deﬁnici´on: Consideremos el siguiente polinomio con coeﬁcientes enteros: ∆(x1, . . . , xn) = Q 1≤i<j≤n(xj \u0000 xi). Dada una permutaci´on σ 2 Sn, los factores de ∆(xσ(1), . . . , xσ(n)) = Q i<j(xσ(j) \u0000 xσ(i)) coinciden, eventualmente salvo el signo, con los de ∆(x1, . . . , xn). Luego ambos polinomios coinciden o diﬁeren en un signo, ∆(xσ(1), . . . , xσ(n)) = \u0006∆(x1, . . . , xn), y llamaremos signo de σ al n´umero entero sgn(σ) = \u00061 tal que ∆(xσ(1), . . . , xσ(n)) = sgn(σ) \u0001 ∆(x1, . . . , xn). (2) Teorema 1.1 El signo de cualquier producto de permutaciones es el producto de los signos de los factores: sgn(τ σ) = (sgn τ )(sgn σ) . El signo de las trasposiciones es –1. El signo de los ciclos de longitud d es (\u00001)d−1, y por eso llamaremos pares a las per- mutaciones de signo 1, e impares a las de signo –1. Demostraci´on: Sean σ, τ 2 Sn. Aplicando τ a los ´ındices de las indeterminadas x1, . . . , xn en la igualdad 2, obtenemos que ∆(x(τ σ)(1), . . . , x(τ σ)(n)) = (sgn σ) \u0001 ∆(xτ (1), . . . , xτ (n)) = (sgn σ)(sgn τ ) \u0001 ∆(x1, . . . , xn). Luego sgn(τ σ) = (sgn σ)(sgn τ ) = (sgn τ )(sgn σ). El ´unico factor de ∆(x1, . . . , xn) que cambia de signo con la trasposici´on (12) es el factor (x2 \u0000 x1); luego el signo de la trasposici´on (12) es –1. Si (ij) es otra trasposici´on, tomamos una permutaci´on τ tal que τ (1) = i, τ (2) = j, de modo que (ij) = τ \u0001 (12) \u0001 τ −1, y sgn(ij) = sgn(τ ) \u0001 sgn(12) \u0001 sgn(τ −1) = \u0000sgn(τ ) \u0001 sgn(τ −1) = \u0000sgn(τ \u0001 τ −1) = \u00001. Ahora es claro que el signo de un ciclo (a1 . . . ad) = (a1a2)(a2a3) \u0001 \u0001 \u0001 (ad−1ad) es (\u00001) d−1. 1.3. Matrices y Determinantes En adelante pondremos K = R ´o C, llamaremos escalares a los elementos de K y consideraremos matrices A = (aij) con coeﬁcientes en K, donde el sub´ındice i indica la ﬁla y el sub´ındice j la columna. Dada una matriz A = (aij) de m ﬁlas y n columnas, su matriz traspuesta es At := (aji), que tiene n ﬁlas y m columnas, y su matriz conjugada es ¯A := (¯aij). Se cumple que (A + B) t = At + Bt, A + B = ¯A + ¯B. Si B = (bjk) es otra matriz de n ﬁlas y r columnas, su producto AB es una matriz m \u0002 r cuyo coeﬁciente cik de la ﬁla i y columna k es cik = nP j=1aijbjk = ai1b1k + ai2b2k + . . . + ainbnk. El producto de matrices es asociativo, distributivo y (AB) t = BtAt, AB = ¯A \u0001 ¯B. 6 La matriz unidad In es la matriz n \u0002 n con todos sus coeﬁcientes nulos, salvo los de la diagonal, que son la unidad. Si A es una matriz m \u0002 n, entonces ImA = A y AIn = A. Una matriz cuadrada A de n columnas se dice que es invertible si existe otra matriz cuadrada B de n columnas tal que AB = In = BA, en cuyo caso tal matriz B es ´unica y se pone B = A−1. Si A y B son matrices invertibles n \u0002 n, entonces (AB) −1 = B−1A−1 , ¯A−1 = A−1 , (At) −1 = (A−1)t . Deﬁnici´on: El determinante de una matriz n \u0002 n cuadrada A = (aij) es el escalar det A = jAj := P σ∈Sn(sgn σ)a1σ(1) . . . anσ(n) y tiene las siguientes propiedades (donde A1, . . . , An denotan las columnas de A): 1. jAj = jAtj, j ¯Aj = jAj. 2. Es lineal en cada columna (y por tanto en cada ﬁla): jA1, . . . , Ai + Bi, . . . , Anj = jA1, . . . , Ai, . . . , Anj + jA1, . . . , Bi, . . . , Anj , jA1, . . . , λAi, . . . , Anj = λjA1, . . . , Ai, . . . , Anj . 3. jAσ(1), . . . , Aσ(n)j = (sgn σ)jA1, . . . , Anj. 4. \f \f \f \f \f \f \f \f a11 0 . . . 0 a21 a22 . . . 0 . . . . . . . . . 0 an1 an2 . . . ann \f \f \f \f \f \f \f \f = a11 . . . ann , jInj = 1. 5. El determinante puede calcularse desarrollando por cualquier ﬁla o columna: jAj = ai1Ai1 + . . . + ainAin = a1jA1j + . . . + anjAnj, donde el adjunto Aij es (\u00001)i+j por el determinante de la matriz que se obtiene elimi- nando la ﬁla i y la columna j de la matriz A. 6. jABj = jAj \u0001 jBj. 7. Las matrices con determinante no nulo son invertibles. Cuando σ = (ij) es una trasposici´on, (3) aﬁrma que el determinante es 0 cuando dos columnas (o dos ﬁlas) son iguales. Luego jA1, . . . , Ai, . . . , Anj = jA1, . . . , Ai + λAj, . . . , Anj , i 6= j, as´ı que un determinante siempre se puede calcular aplicando a las ﬁlas o columnas trans- formaciones elementales (permutarlas, multiplicar una por un escalar no nulo, o sumarle a una el producto de otra por un escalar) hasta que quede en la forma (4). Si A es invertible, de (6) se sigue que su determinante no es nulo, y jA−1j = jAj −1. Regla de Cr´amer (1704-1752): Si A = (A1, . . . , An) es una matriz cuadrada invertible, el sistema de ecuaciones lineales AX = B tiene una ´unica soluci´on, xi = jA1, . . . , B, . . . , Anj jAj , donde la columna B est´a en el lugar i-´esimo. Demostraci´on: Si A es invertible, la ´unica soluci´on de AX = B es X = A−1B. Adem´as, si x1, . . . , xn es la soluci´on del sistema, entonces x1A1 + . . . + xnAn = B y por tanto: jA1, . . . , B, . . . , Anj = P jxjjA1, . . . , Ai−1, Aj, Ai+1, . . . , Anj = xijA1, . . . , Ai, . . . , Anj 7 porque la matriz (A1, . . . , Ai−1, Aj, Ai+1, . . . , An) tiene dos columnas iguales (las columnas i y j) cuando i 6= j. Luego xi = jA1, . . . , B, . . . , Anj/jA1, . . . , Ai, . . . , Anj. q.e.d. Poniendo B = (1, 0, . . . , 0)t, . . . , (0, . . . , 0, 1)t en la regla de Cr´amer, vemos que A−1 = 1 jAj 0 @A11 . . . A1n . . . . . . . . . An1 . . . Ann 1 A t . Deﬁnici´on: El rango (por columnas) de una matriz A es el m´aximo n´umero de columnas de A linealmente independientes, y se denota rg A. Teorema de Rouch´e-Frob¨enius (1832-1910, 1849-1917): Un sistema de ecuaciones linea- les AX = B es compatible si y s´olo si rgA = rg(AjB) . Deﬁnici´on: Los menores de orden r de una matriz A son los determinantes de las matrices formadas con los coeﬁcientes de r ﬁlas y r columnas de A. Teorema del Rango: Dadas r columnas de una matriz, son linealmente independientes si y s´olo si con ellas se puede formar un menor de orden r no nulo. Por tanto, el rango de una matriz es el mayor orden de sus menores no nulos, y el rango por ﬁlas coincide con el rango por columnas. Sistemas de Ecuaciones Lineales: Una forma de resolver un sistema de ecuaciones linea- les AX = B compatible es ﬁjar un menor no nulo de A de orden r = rgA = rg(AjB), eliminar las ecuaciones que no entren en ese menor e igualar a par´ametros las n \u0000 r indeterminadas que no entren en ese menor, obteniendo as´ı un sistema de Cr´amer. Por ´ultimo, si X0 es una soluci´on particular, AX0 = B, entonces todas las soluciones se obtienen sum´andole las soluciones del sistema homog´eneo AY = 0; i.e., las soluciones son X = X0 + Y , donde AY = 0. En efecto, B = A(X0 + Y ) = AX0 + AY = B + AY si y s´olo si AY = 0. 1.4. Grupos Dados dos conjuntos X, Y , no necesariamente distintos, el producto directo o carte- siano X \u0002 Y denota el conjunto de parejas ordenadas (x, y) donde x 2 X, y 2 Y . Deﬁnici´on: Una operaci´on (interna) en un conjunto G es una aplicaci´on G \u0002 G ! G, y la imagen de un par (a, b) 2 G \u0002 G se denota a \u0003 b, a \u0001 b, a + b, ´o ab sin m´as. Deﬁnici´on: Un conjunto G con una operaci´on G \u0002 G · \u0000! G es un grupo cuando 1. La operaci´on es asociativa: a \u0001 (b \u0001 c) = (a \u0001 b) \u0001 c, 8a, b, c 2 G. 2. Existe un elemento 1 2 G, llamado neutro o unidad, tal que a \u0001 1 = 1 \u0001 a = a, 8a 2 G. 3. Para cada a 2 G existe a −1 2 G, llamado inverso de a, tal que a \u0001 a −1 = a−1 \u0001 a = 1. y se dice que el grupo es conmutativo o abeliano cuando a \u0001 b = b \u0001 a, 8a, b 2 G. Notaci´on: En un grupo pondremos a 0 := 1, a 2 := a \u0001 a, a 3 := a \u0001 a \u0001 a,..., a −2 := a −1 \u0001 a−1, a−3 := a−1 \u0001 a −1 \u0001 a −1,..., de modo que a n+m = a na m, 8n, m 2 Z. Cuando el grupo es conmutativo, a menudo usaremos la notaci´on aditiva: ponemos a + b en vez de a \u0001 b, el neutro se denota 0, el inverso \u0000a (y se llama opuesto de a) y pondremos a \u0000 b := a + (\u0000b), 0 \u0001 a := 0, 2a := a + a, 3a := a + a + a,..., (\u00002)a := \u0000a \u0000 a,..., de modo que (n + m)a = na + ma, 8n, m 2 Z. 8 Ejemplos: 1. Con la suma usual, C es un grupo conmutativos, y el neutro es el n´umero 0. Con el producto usual, C̸=0 := fx 2 C : x 6= 0g es un grupo conmutativo, y el neutro es el n´umero 1. 2. El conjunto Sn de las permutaciones de n elementos, con la composici´on de aplicaciones, es un grupo, llamado grupo sim´etrico n-´esimo, y el neutro es la permutaci´on identidad. 3. El conjunto Mm×n(K) de las matrices de m ﬁlas y n columnas con coeﬁcientes en K (= R ´o C), con la suma de matrices, es un grupo conmutativo, y el neutro es la matriz que tiene todos los coeﬁcientes nulos. 4. El conjunto Gl(n, K) := fA 2 Mn×n(K) : det(A) 6= 0g, con el producto de matrices, es un grupo, llamado grupo lineal n-´esimo, y el neutro es la matriz In. 5. Los polinomios en una indeterminada x y coeﬁcientes en K, con la suma de polinomios, forman un grupo conmutativo. El neutro es el polinomio de coeﬁcientes nulos. 6. Fijado un conjunto X y un grupo abeliano (G, +), el conjunto formado por las aplicaciones f : X ! G, con la suma de aplicaciones, (f + h)(x) := f (x) + h(x), forman un grupo conmutativo. El neutro es la aplicaci´on constante 0, y el opuesto de f : X ! G es la aplicaci´on (\u0000f )(x) := \u0000\u0000f (x)\u0001. 7. Dados dos grupos G1, G2, su producto directo o cartesiano G1 \u0002 G2, con la operaci´on (a1, a2) \u0001 (b1, b2) := (a1b1, a2b2), tambi´en es un grupo. El neutro es la pareja (1, 1), y (a1, a2) −1 = (a −1 1 , a−1 2 ). Este grupo G1 \u0002 G2 es conmutativo cuando G1 y G2 lo son. En particular K n es grupo conmutativo con la operaci´on (λ1, . . . , λn) + (µ1, . . . , µn) := (λ1 + µ1, . . . , λn + µn), el neutro es la sucesi´on nula 0 = (0, . . . , 0), y \u0000(λ1, . . . , λn) = (\u0000λ1, . . . , \u0000λn). Deﬁnici´on: Un subconjunto H de un grupo G es un subgrupo si cumple las siguientes condiciones (de modo que H, con la operaci´on de G, tambi´en es un grupo): 1. ab 2 H, 8a, b 2 H 2. 1 2 H. 3. a −1 2 H, 8a 2 H. Ejemplos: 1. Z, Q y R son subgrupos del grupo C. 2. f\u00061g, R+ := fx 2 R : x > 0g y R̸=0 son subgrupos del grupo C̸=0. 3. Si A 2 Mm×n(K), las soluciones del sistema de ecuaciones lineales homog´eneo AX = 0 forman un subgrupo de K n. 4. El grupo lineal Gl(n, R), el grupo ortogonal O(n) := fA 2 Gl(n, R)) : AtA = Ing y el grupo unitario U (n) := fA 2 Gl(n, C) : ¯AtA = Ing son subgrupos de Gl(n, C). 5. Fijado n 2 N, los polinomios de grado \u0014 n forman un subgrupo Pn de los polinomios. 6. Si H, H ′ son subgrupos de un grupo G, entonces H \\ H ′ tambi´en es un subgrupo de G. 9 Deﬁnici´on: Una aplicaci´on f : G ! G′ entre grupos es morﬁsmo de grupos cuando f (a \u0001 b) = f (a) \u0001 f (b), 8a, b 2 G. En tal caso, como f (1) = f (1 \u0001 1) = f (1) \u0001 f (1), tendremos que f (1) es el neutro de G′, y como 1 = f (1) = f (a −1 \u0001 a) = f (a −1) \u0001 f (a), tendremos que f (a −1) = f (a)−1, 8a 2 G. Teorema 1.2 Si f : G ! G′ y h : G′ ! G′′ son morﬁsmos de grupos, entonces su composi- ci´on h \u000e f : G ! G′′ tambi´en es morﬁsmo de grupos. Demostraci´on: Si a, b 2 G, se cumple que (h \u000e f )(ab) = h(f (ab)) = h(f (a) \u0001 f (b)) = h(f (a)) \u0001 h(f (b)) = (h \u000e f )(a) \u0001 (h \u000e f )(b). Teorema 1.3 Sea f : G ! G′ un morﬁsmo de grupos. Su n´ucleo Ker f = fa 2 G : f (a) = 1g es un subgrupo de G, y su imagen Im f = fa ′ 2 G′ : a ′ = f (a), 9a 2 Gg = ff (a)ga∈G es un subgrupo de G′. Demostraci´on: Veamos que Ker f es un subgrupo de G. Si a, b 2 Ker f , entonces ab 2 Ker f porque f (ab) = f (a)f (b) = 1 \u0001 1 = 1. Si a 2 Ker f , entonces a −1 2 Ker f porque f (a−1) = f (a) −1 = 1−1 = 1. Por ´ultimo, 1 2 Ker f porque f (1) = 1. Veamos ahora que Im f es un subgrupo de G′. Si a ′, b′ 2 Im f , por deﬁnici´on existen a, b 2 G tales que a ′ = f (a) y b ′ = f (b), as´ı que a′b ′ = f (a)f (b) = f (ab) 2 Im f . Si a ′ 2 Im f , entonces a′ = f (a) para alg´un a 2 G, y (a ′)−1 = f (a) −1 = f (a −1) 2 Im f . Por ´ultimo, 1 2 Im f porque f (1) = 1. Teorema 1.4 Un morﬁsmo de grupos f : G ! G′ es inyectivo si y s´olo si Ker f = 1. Demostraci´on: Si f es inyectivo y a 2 Ker f , entonces f (a) = 1 = f (1); luego a = 1. Rec´ıprocamente, supongamos que Ker f = 1. Si f (a) = f (b), donde a, b 2 G, entonces f (a −1b) = f (a) −1f (b) = 1; luego a −1b 2 Ker f = 1, as´ı que a−1b = 1 y por tanto a = b. Ejemplos: 1. La aplicaci´on f : C̸=0 ! R+, f (z) = jzj, es morﬁsmo de grupos porque jzuj = jzj \u0001 juj; luego su n´ucleo U (1) = fz 2 C : jzj = 1g es un subgrupo de C̸=0. 2. El signo sgn : Sn ! f\u00061g es morﬁsmo de grupos porque sgn(στ ) = (sgn σ)(sgn τ ), as´ı que el grupo alternado An = fσ 2 Sn : sgn σ = 1g es un subgrupo de Sn. 3. El determinante Gl(n, K) ! K̸=0 es morﬁsmo de grupos porque jABj = jAj \u0001 jBj, y el grupo especial Sl(n, K) := fA 2 Gl(n, K) : jAj = 1g es un subgrupo de Gl(n, K). 4. Si a 2 K, la aplicaci´on f : K ! K, f (x) = ax, es morﬁsmo de grupos: a(x + y) = ax + ay. 5. La aplicaci´on f : C ! C̸=0, f (z) = ez, es morﬁsmo de grupos: ez+z′ = ezez′. 6. Si G es un grupo conmutativo y n 2 N, la aplicaci´on f : G ! G, f (a) = a n, es morﬁsmo de grupos porque (ab)n = a nb n. En particular, cuando G = C̸=0, vemos que las ra´ıces n-´esimas de la unidad complejas forman un subgrupo fz 2 C : zn = 1g de C̸=0. 7. Sean H y H ′ dos subgrupos de un grupo conmutativo G. La aplicaci´on s : H1 \u0002 H2 ! G, s(h1, h2) = h1 + h2; es morﬁsmo de grupos, as´ı que su imagen H1 + H2 := fg 2 G : g = h1 + h2; 9h1 2 H1, h2 2 H2g = fh1 + h2gh1∈H1,h2∈H2 tambi´en es un subgrupo de G. 10 2. Espacios Vectoriales Deﬁniciones: Un conjunto A con dos operaciones internas, que denotaremos + y \u0001, es un anillo cuando 1. (A, +) es un grupo conmutativo. 2. El producto es asociativo: a \u0001 (b \u0001 c) = (a \u0001 b) \u0001 c, 8a, b, c 2 A. 3. Existe un elemento 1 2 A tal que a \u0001 1 = 1 \u0001 a = a, 8a 2 G. 4. Se cumple la propiedad distributiva: a(b + c) = ab + ac, 8a, b, c 2 A. y se dice que un anillo es conmutativo cuando a \u0001 b = b \u0001 a, 8a, b 2 A. Un anillo conmutativo K es un cuerpo cuando para cada elemento no nulo a 2 K existe a −1 2 K tal que a \u0001 a −1 = 1. Por ejemplo, Q, R, C y fpar=0,impar=1g, con las operaciones usuales, son cuerpos; mientras que el anillo M2×2(R) no es conmutativo, y el anillo conmutativo Z no es cuerpo. En adelante pondremos K = R ´o C (o en general un cuerpo), y llamaremos escalares a los elementos de K. Dar una operaci´on externa de K en un conjunto E es dar una aplicaci´on K \u0002 E ! E, y la imagen de un par (λ, e) 2 K \u0002 E se suele denotar λ \u0001 e, ´o λe sin m´as. 2.1. Espacios Vectoriales y Subespacios Vectoriales Deﬁnici´on: Un K-espacio vectorial es un grupo conmutativo E (cuya operaci´on denota- mos aditivamente +) con una operaci´on externa K \u0002 E ! E que cumple 1. λ(e + v) = λe + λv para todo λ 2 K, e, v 2 E. 2. (λ + µ)e = λe + µe para todo λ, µ 2 K, e 2 E. 3. (λµ)e = λ(µe) para todo λ, µ 2 K, e 2 E. 4. 1 \u0001 e = e para todo vector e 2 E. Los elementos de E se llaman vectores o puntos, y pondremos v \u0000 e := v + (\u0000e), de modo que e \u0000 e = 0, y muchas otras reglas del c´alculo usuales son v´alidas: 0 \u0001 e = λ \u0001 0 = 0, λ(\u0000e) = (\u0000λ)e = \u0000(λe), λ(v \u0000 e) = λv \u0000 λe, (λ \u0000 µ)e = λe \u0000 µe,... Deﬁnici´on: Un subgrupo V de un espacio vectorial E es un subespacio vectorial de E cuando λv 2 V para todo λ 2 K y v 2 V , de modo que V , con el producto por escalares que tenemos en E, tambi´en es un espacio vectorial. Como 0 = 0\u0001e, \u0000e = (\u00001)\u0001e, para que un subconjunto no vac´ıo V \u0012 E sea un subespacio vectorial, es necesario y suﬁciente que sea estable por sumas y producto por escalares: v + v′, λv 2 V , 8v, v′ 2 V , λ 2 K. Ejemplos: 1. En la Geometr´ıa Eucl´ıdea cl´asica, ﬁjado un origen O, los puntos forman un espacio vec- torial real cuando se suman con la regla del paralelogramo, y se multiplican por escalares seg´un la proporci´on de segmentos: e + vv e O λe Las rectas y planos que pasan por el origen O son subespacios vectoriales. 11 2. El grupo K n = K\u0002 n. . . \u0002K, con la operaci´on externa α \u0001 (λ1, . . . , λn) = (αλ1, . . . , αλn) es un K-espacio vectorial. El vector 0 es (0, . . . , 0) y \u0000(λ1, . . . , λn) = (\u0000λ1, . . . , \u0000λn). Si A 2 Mm×n(K), las soluciones del sistema de ecuaciones lineales homog´eneo AX = 0 forman un subespacio vectorial de K n. 3. Fijados dos n´umeros naturales positivos m y n, el producto usual de matrices por escalares λ(aij) = (λaij) deﬁne una estructura de K-espacio vectorial en el grupo Mm×n(K). El vector 0 es la matriz con todos sus coeﬁcientes nulos, y \u0000(aij) = (\u0000aij). 4. El espacio vectorial con un ´unico vector (necesariamente el vector nulo) se denota 0. Todo espacio vectorial E admite los subespacios vectoriales triviales 0 y E. 5. Sean V y W dos subespacios vectoriales de un K-espacio vectorial E. Su intersecci´on V \\ W := fe 2 E : e 2 V y e 2 W g es un subespacio vectorial de E, y su suma V + W := fe 2 E : e = v + w, 9v 2 V, w 2 W g = fv + wgv∈V,w∈W tambi´en es un subespacio vectorial de E. Si un subespacio vectorial F de E contiene a V y a W , entonces tambi´en contiene a V + W , porque F es un subgrupo de E. 6. Si e es un vector de un K-espacio vectorial E, entonces hei = Ke := fv 2 E : v = λe, 9λ 2 Kg = fλegλ∈K es un subespacio vectorial de E, y diremos que est´a generado por el vector e. Si V es un subespacio vectorial de E y e 2 V , entonces Ke \u0012 V . 7. Si e1, . . . , en son vectores de un espacio vectorial E, entonces he1, . . . , eni := Ke1 + . . . + Ken = fλ1e1 + . . . + λnengλ1,...,λn∈K es un subespacio vectorial de E, y diremos que est´a generado por e1, . . . , en. Si V es un subespacio vectorial de E y e1, . . . , en 2 V , entonces he1, . . . , eni \u0012 V . Luego he1, . . . , eni = hv1, . . . , vmi , e1, . . . , en 2 hv1, . . . , vmi y v1, . . . , vm 2 he1, . . . , eni. En particular, para cualesquiera escalares λ1, . . . , λn 2 K se cumple que he1, . . . , eni = he1, . . . , en, λ1e1 + . . . + λneni. 8. Los polinomios en una indeterminada x y coeﬁcientes en K, con la suma de polinomios y el producto usual por escalares, forman un K-espacio vectorial donde el 0 es el polinomio de coeﬁcientes nulos. Fijado n 2 N, los polinomios de grado \u0014 n forman un subespacio vectorial Pn = K + Kx + . . . + Kxn = fa0 + a1x + . . . + anxng. 9. Fijado un conjunto X, las funciones ψ : X ! K forman un K-espacio vectorial, con las operaciones (ψ + ϕ)(x) := ψ(x) + ϕ(x) , (λψ)(x) := λ\u0000ψ(x) \u0001. El vector 0 es la funci´on que se anula en todo punto de X, y (\u0000ψ)(x) = \u0000(ψ(x)). Cuando K = C y X = [a1, b1] \u0002 [a2, b2] \u0002 [a3, b3] es un producto de intervalos en R3, cada funci´on continua compleja ψ(x, y, z) : X ! C (no id´enticamente nula) describe el estado de una part´ıcula cu´antica conﬁnada en X, entendiendo que la probabilidad de encontrar a la part´ıcula en cierta regi´on Ω \u0012 X es Z Ω jψ(x1, x2, x3)j 2dx1dx2dx3 Z X jψ(x1, x2, x3)j2dx1dx2dx3 \u0001 12 De hecho una funci´on ψ(x1, x2, x3) y λψ(x1, x2, x3), 0 6= λ 2 C, describen el mismo comportamiento, as´ı que el estado de la part´ıcula viene dado por la recta Cψ = hψi, y puede ser representado por cualquier funci´on λψ(x1, x2, x3), 0 6= λ 2 C. En Mec´anica Cu´antica, la funci´on (dependiente del tiempo) que describe una onda plana es ψ(t, x1, x2, x3) = e(k1x1+k2x2+k3x3−ωt)i, donde k1, k2, k3, ω 2 R. En este caso, como jψ(t, x1, x2, x3)j = 1, la probabilidad es proporcional al volumen de Ω. 10. En general, ﬁjado un conjunto X y un K-espacio vectorial E, las aplicaciones f : X ! E forman un K-espacio vectorial, con las operaciones (f + h)(x) := f (x) + h(x) , (λf )(x) := λ\u0000f (x) \u0001, y el 0 es la aplicaci´on f : X ! E tal que f (x) = 0 2 E en todo punto x 2 X. 11. Todo espacio vectorial complejo es tambi´en, de modo natural, un espacio vectorial real, porque todo n´umero real es un n´umero complejo; pero los conceptos (como el subespa- cio vectorial Ke generado por un vector e) son muy distintos seg´un que los escalares considerados sean C o R. 12. Si E y E′ son dos K-espacios vectoriales, su producto directo E \u0002 E′ es un K-espacio vectorial, con las operaciones (e, e′) + (v, v′) := (e + v, e′ + v′), λ(e, e′) := (λe, λe′), y el 0 es la pareja (0, 0). 13. Dado un espacio vectorial E, si dibujamos una ﬂecha con origen en un punto p y ﬁnal en otro q, nos referimos al vector ⃗pq := q \u0000 p. Dados tres puntos a, b, c 2 E, si ponemos e = b \u0000 a, v = c \u0000 b, tendremos que e + v = c \u0000 b + b \u0000 a = c \u0000 a: a b c e e + v v Deﬁnici´on: Dado un subespacio vectorial V de un espacio vectorial E y un punto p 2 E, la subvariedad lineal de E que pasa por p con direcci´on V es p + V := fe 2 E : e = p + v, 9v 2 V g = fp + vgv∈V . Una propiedad fundamental de las subvariedades lineales es que si una subvariedad lineal p+V pasa por un punto q 2 E, entonces p+V = q +V . Por tanto, dos subvariedades lineales de igual direcci´on o son disjuntas o coinciden. En efecto, si q 2 p + V , tendremos q = p + v para alg´un vector v 2 V ; luego q + V = p + v + V = p + V, pues v + V = V (todo vector v′ 2 V es v′ = v + (v′ \u0000 v) y v′ \u0000 v 2 V ). Ejemplo: Sea A 2 Mm×n(K) y B 2 K m. Si el sistema de ecuaciones lineales AX = B es compatible, sus soluciones forman una subvariedad lineal de K n, de direcci´on AX = 0, porque si X0 es una soluci´on particular, AX0 = B, entonces todas las soluciones del sistema AX = B son X = X0 + Y , donde AY = 0. 13 2.2. Teor´ıa de la Dimensi´on Deﬁniciones: Diremos que unos vectores e1, . . . , en de un espacio vectorial E lo generan, o que forman un sistema de generadores de E cuando todo vector de E es una combinaci´on lineal de e1, . . . , en con coeﬁcientes en K: Ke1 + . . . + Ken = E. Diremos que e1, . . . , en son linealmente dependientes si λ1e1 + . . . + λnen = 0 para ciertos escalares λ1, . . . , λn no todos nulos (si admiten una relaci´on de dependencia lineal). En caso contrario diremos que son linealmente independientes. Es decir, e1, . . . , en son linealmente independientes cuando la ´unica combinaci´on lineal nula es la que tiene todos los coeﬁcientes nulos: λ1, . . . , λn 2 K y λ1e1 + . . . + λnen = 0 ) λ1 = . . . = λn = 0. Deﬁnici´on: Diremos que una sucesi´on de vectores e1, . . . , en de un espacio vectorial E es una base de E cuando tales vectores sean linealmente independientes y generen E. En tal caso, cada vector e 2 E se escribe de modo ´unico como combinaci´on lineal e = x1e1 + . . . + xnen con coeﬁcientes en K. En efecto, si e = x1e1 + . . . + xnen = y1e1 + . . . + ynen, tendremos que 0 = x1e1 + . . . + xnen \u0000 (y1e1 + . . . + ynen) = (x1 \u0000 y1)e1 + . . . + (xn \u0000 yn)en, y xi \u0000 yi = 0 para todo ´ındice i, porque los vectores e1, . . . , en son linealmente independientes. Diremos que (x1, . . . , xn) 2 K n son las coordenadas del vector e en la base e1, . . . , en, y pondremos e = (x1, . . . , xn) cuando la base e1, . . . , en se sobrentiende. N´otese que e1 = (1, 0, . . . , 0), . . . , en = (0, . . . , 0, 1). Adem´as, las coordenadas de una suma de vectores e + v se obtienen sumando las coordenadas de e y v, y que las coordenadas de λe se obtienen multiplicando por λ las coordenadas de e. Ejemplos: 1. Los vectores e1 = (1, 0, . . . , 0), e2 = (0, 1, . . . , 0), . . . , en = (0, . . . , 0, 1) forman una base de K n, llamada base usual de K n. Las coordenadas de un vector e = (a1, . . . , an) de K n en esta base son precisamente (a1, . . . , an), porque e = a1e1 + . . . + anen. 2. Una base de M2×2(K) est´a formada por las matrices U11 = \u0012 1 0 0 0 \u0013 , U12 = \u00120 1 0 0 \u0013, U21 = \u00120 0 1 0 \u0013, U22 = \u00120 0 0 1 \u0013 y las coordenadas en esta base de una matriz \u0012 a11 a12 a21 a22 \u0013 son (a11, a12, a21, a22). Igualmente, las matrices m \u0002 n que tienen todos sus coeﬁcientes nulos, excepto uno que es la unidad, deﬁnen una base de Mm×n(K). 3. Los polinomios 1, x, . . . , xn forman una base del K-espacio vectorial Pn = K + . . . + Kxn. Las coordenadas de un polinomio a0 + a1x + . . . + anxn en esta base son (a0, . . . , an). 4. Todo vector no nulo e 2 E es linealmente independiente, porque λe = 0 ) λ = 0; luego e es una base del subespacio vectorial Ke que genera. 5. Sean α1, . . . , αn n´umeros complejos distintos entre s´ı. Las funciones eα1x, . . . , eαnx de una variable real x, son linealmente independientes, y forman por tanto una base del subespacio vectorial que generan Ceα1x + . . . + Ceαnx. 14 Para demostrarlo procedemos por inducci´on sobre n, y es claro cuando n = 1 porque e αx 6= 0. Cuando n > 1, si ϕ(x) = λ1eα1x + . . . + λneαnx = 0, derivando 0 = ϕ′(x) = α1λ1eα1x + . . . + αnλneαnx. 0 = αnϕ(x) \u0000 ϕ′(x) = (αn \u0000 α1)λ1eα1x + . . . + (αn \u0000 αn−1)λn−1eαn−1x. Por hip´otesis de inducci´on (αn \u0000 α1)λ1 = . . . = (αn \u0000 αn−1)λn−1 = 0, y vemos que λ1 = . . . = λn−1 = 0 porque αi 6= αj cuando i 6= j. Luego 0 = ϕ(x) = λneαnx, y concluimos que tambi´en λn = 0. El siguiente resultado nos da el signiﬁcado geom´etrico de la independencia lineal: Teorema 2.1 Unos vectores no nulos e1, . . . , en 2 E son linealmente dependientes si y s´olo si alguno de ellos es combinaci´on lineal de los anteriores. Demostraci´on: Si e1, . . . , en admiten una relaci´on de dependencia lineal λ1e1+. . .+λnen = 0, tomamos el mayor ´ındice i tal que λi 6= 0. Tenemos que i > 1 porque e1 6= 0, y ei = \u0000 λ1 λi e1 \u0000 λ2 λi e2 \u0000 . . . \u0000 λi−1 λi ei−1. Rec´ıprocamente, si existe alg´un ´ındice i tal que ei = µ1e1 + . . . + µi−1ei−1, tendremos que µ1e1 + . . . + µi−1ei−1 \u0000 ei = 0, y los vectores e1, . . . , en son linealmente dependientes. Lema Fundamental: Sea E un espacio vectorial generado por n vectores e1, . . . , en. Si r vectores v1, . . . , vr 2 E son linealmente independientes, entonces r \u0014 n. Demostraci´on: Procedemos por reducci´on al absurdo. Supongamos que r > n. Como v1 es combinaci´on lineal de e1, . . . , en, los vectores v1, e1, . . . , en son linealmente dependientes y generan E. Por el teorema anterior, alguno es combinaci´on lineal de los anteriores y, despu´es de eliminarlo, los vectores siguen generando E. Reordenando e1, . . . , en podemos suponer que es e1, de modo que v1, e2, . . . , en generan E. Ahora v1, v2, e2, . . . , en son linealmente dependientes y generan E, as´ı que alg´un vector es combinaci´on lineal de los anteriores (no puede ser v2 porque los vectores v1, v2 son indepen- dientes) y, despu´es de eliminarlo, los vectores siguen generando E. Reordenando e2, . . . , en podemos suponer que es e2, de modo que v1, v2, e3, . . . , en generan E. Repitiendo el razonamiento obtendremos que los vectores v1, . . . , vn generan E, de modo que vn+1 = λ1v1 + . . . + λnvn. Absurdo, v1, . . . , vn, vn+1 son linealmente independientes. Ejemplo: Si cierto menor de orden r de una matriz A no es nulo, y el rango de A es mayor que r, por el Lema fundamental alguna columna no est´a en el subespacio vectorial que generan las columnas correspondientes al menor, y por tanto esas r + 1 columnas son linealmente independientes (y lo mismo es cierto para las ﬁlas): el menor puede ampliarse hasta obtener un menor no nulo de orden r + 1. Teorema 2.2 Todas las bases de un espacio vectorial tienen igual n´umero de vectores. Demostraci´on: Sean e1, . . . , en y v1, . . . , vr dos bases de un espacio vectorial E. Como v1, . . . , vr 2 E = Ke1 + . . . + Ken son linealmente independientes, por el lema fundamental r \u0014 n. Como e1, . . . , en 2 E = Kv1 + . . . + Kvr son linealmente independientes, tambi´en tenemos que n \u0014 r; luego n = r. Deﬁnici´on: El espacio vectorial E = 0 tiene dimensi´on 0, y la dimensi´on de un espacio vectorial E 6= 0 es el n´umero de vectores de cualquier base de E, y se denota dimKE. Cuando ninguna familia ﬁnita de vectores de un espacio vectorial E 6= 0 sea una base de E, diremos que E tiene dimensi´on inﬁnita. 15 La dimensi´on de una subvariedad lineal X = p + V es la de su direcci´on V , y diremos que dos subvariedades lineales p + V , q + W de E de la misma dimensi´on son paralelas cuando tienen la misma direcci´on, V = W . Las subvariedades lineales de dimensi´on 1 y 2 se llaman rectas y planos respectivamente. Ejemplos: 1. dimKK n = n, dimKMm×n(K) = mn, y dim Pn = n + 1. 2. dimC(Ceα1x + . . . + Ceαnx) = n, cuando α1, . . . , αn 2 C son distintos entre s´ı. 3. De acuerdo con el lema fundamental, en un espacio vectorial de dimensi´on n no puede haber m´as de n vectores linealmente independientes. 4. Si un vector e no es nulo, dimK(Ke) = 1. Si adem´as v /2 Ke, entonces e y v son linealmente independientes por 2.1; luego forman una base de Ke + Kv, y dimK(Ke + Kv) = 2. 5. Por dos puntos distintos p y q = p + e pasa una ´unica recta, p + Ke = fp + λe = (1 \u0000 λ)p + λqgλ∈K, formada por todas las combinaciones lineales αp + βq, con α + β = 1. En efecto, si una recta a + V pasa por p, entonces a + V = p + V . Si adem´as pasa por q = p + e, entonces e 2 V y Ke = V , pues en caso contrario existir´ıa v 2 V , v /2 Ke, de modo que e, v 2 V son linealmente independientes, en contra de que dim V = 1. En el caso real, K = R, se deﬁne adem´as el segmento de extremos p y q como el conjunto formado por los puntos a = p + λe = (1 \u0000 λ)p + λq, con 0 \u0014 λ \u0014 1, y diremos que tal punto a divide al segmento en la proporci´on λ : (1 \u0000 λ) porque a \u0000 p = λ 1−λ (q \u0000 a): p a q λe (1 \u0000 λ)e Diremos que m = p + 1 2 e = p+q 2 es el punto medio entre p y q, porque m \u0000 p = q \u0000 m Teorema 2.3 Todo sistema de generadores e1, . . . , en de un espacio vectorial E 6= 0 con- tiene una base de E. Por tanto n \u0015 dim E, y si adem´as n = dim E, entonces los vectores e1, . . . , en ya forman una base de E. Demostraci´on: Sea d el m´aximo n´umero de vectores linealmente independientes que podamos extraer entre e1, . . . , en. Reordenando los vectores si fuera necesario, podemos suponer que e1, . . . , ed son linealmente independientes. Ahora ed+1, . . . , en 2 he1, . . . , edi, porque si ej /2 he1, . . . , edi, entonces e1, . . . , ed, ej son linealmente independientes por 2.1, lo que contradice la elecci´on del n´umero d. Luego E = he1, . . . , ed, . . . eni = he1, . . . , edi, de modo que E = he1, . . . , edi, y concluimos que e1, . . . , ed es una base de E. Por ´ultimo, si n = dim E, entonces los vectores e1, . . . , en ya forman una base de E; porque una base de E no puede tener menos de n vectores seg´un 2.2. Corolario 2.4 Sea e1, . . . , em una base de un espacio vectorial E. Si A 2 Mm×n(K) es la matriz que tiene por columnas las coordenadas de v1, . . . , vn 2 E en tal base, se cumple que dim (Kv1 + . . . + Kvm) = rg A. Demostraci´on: Pongamos r = rg A y d = dim (Kv1 + . . . + Kvn). Como v1, . . . , vn contiene una base vi1 , . . . , vid de Kv1 +. . .+Kvn, las columnas i1, . . . , id de la matriz A son linealmente independientes (unos vectores son linealmente independientes si y s´olo si sus coordenadas son linealmente independientes en K m) y por tanto d \u0014 r. Pero tambi´en r \u0014 d, porque, por el lema fundamental, en Kv1 + . . . + Kvn no puede haber d + 1 vectores linealmente independientes. 16 Teorema 2.5 Sea E 6= 0 un espacio vectorial de dimensi´on ﬁnita. Toda familia de vectores e1, . . . , en 2 E linealmente independiente se puede ampliar hasta obtener una base de E. Si adem´as n = dim E, entonces e1, . . . , en ya forman una base de E. Demostraci´on: A˜nadimos vectores de E hasta obtener una familia linealmente independiente e1, . . . , en, e′ 1, . . . , e′ s que no pueda ampliarse de modo que lo siga siendo (el proceso termina porque, en virtud del lema fundamental, siempre n + s \u0014 dim E). Ahora e1, . . . , en, e′ 1, . . . , e′ s ya es una base de E por 2.1. Por ´ultimo, si n = dim E = n + s, entonces s = 0 y e1, . . . , en ya es base de E. Ecuaciones Param´etricas e Impl´ıcitas: Fijada una base de un espacio vectorial de dimensi´on ﬁnita E, dar ecuaciones param´etricas de un subespacio vectorial V de E es dar las coordenadas de un sistema de generadores de V (mejor si forman una base de V ), y dar ecuaciones impl´ıcitas de V es dar un sistema homog´eneo de ecuaciones lineales AX = 0 (mejor si son linealmente independientes) cuyas soluciones X t = (x1, . . . , xn) sean las coordenadas de los vectores de V . Dar ecuaciones param´etricas de una subvariedad lineal X = p + V de E es dar las coordenadas de un punto p de X y de un sistema de generadores de la direcci´on V de X, y dar ecuaciones impl´ıcitas de X es dar un sistema de ecuaciones lineales AX = B cuyas soluciones sean las coordenadas de los puntos de X, Pasar de ecuaciones impl´ıcitas a param´etricas es sencillamente resolver el sistema, y rec´ıprocamente, dadas las ecuaciones param´etricas de una subvariedad lineal X de dimensi´on d en un espacio vectorial de dimensi´on n, un m´etodo para hallar ecuaciones impl´ıcitas de X es despejar los d par´ametros en funci´on de las coordenadas del punto de X (usando d ecuaciones) para despu´es sustituir esos valores en las restantes n \u0000 d ecuaciones. Un m´etodo alternativo para hallar ecuaciones impl´ıcitas de un subespacio vectorial V de base v1, . . . , vr es usar que, de acuerdo con 2.1, un vector e 2 E est´a en V si y s´olo si los vectores v1, . . . , vr, e son linealmente dependientes. Por tanto, dadas ecuaciones param´etricas de V (donde suponemos que la matriz A = (aij) ya es de rango r = dim V ) 0 B @ x1 ... xn 1 C A = λ1 0 B @ a11 ... an1 1 C A + . . . + λr 0 B @ a1r ... anr 1 C A el vector de coordenadas (x1, . . . , xn) est´a en V precisamente cuando r = rg 0 @a11 . . . a1r x1 . . . . . . . . . . . . an1 . . . anr xn 1 A es decir, cuando se anulen todos los menores de orden r + 1 (y basta considerar los que ampl´ıen a un menor no nulo de orden r dado) lo que nos da ecuaciones impl´ıcitas de V . Teorema 2.6 Sea E un espacio vectorial de dimensi´on ﬁnita. Para todo subespacio vectorial V de E se cumple que dim V \u0014 dim E , y s´olo se da la igualdad cuando V = E. Demostraci´on: La dimensi´on de V tambi´en es ﬁnita, porque si v1, . . . , vr es una familia linealmente independiente en V que ya no pueda ampliarse con un vector de V de modo que lo siga siendo (existe porque, si n = dim E, por el lema fundamental en E no puede haber m´as de n vectores linealmente independientes) entonces v1, . . . , vr es una base de V por 2.1, de modo que r = dim V . 17 Como v1, . . . , vr 2 E son linealmente independientes, seg´un 2.5 tenemos que r \u0014 dim E, y que si r = dim E, entonces v1, . . . , vr tambi´en es una base de E, de modo que E = Kv1 + . . . + Kvr = V. Ejemplo: Sea E un espacio vectorial de dimensi´on m, y sea A 2 Mm×n(K) la matriz que tiene por columnas las coordenadas de unos vectores v1, . . . , vn 2 E en cierta base de E. Por el teorema, v1, . . . , vn generan E cuando m = dim hv1, . . . , vni = rg A. Por otra parte, son linealmente independientes cuando n = dim hv1, . . . , vni = rg A. Luego forman una base de E cuando rg A = m = n; es decir, cuando jAj 6= 0. Teorema de Rouch´e-Frob¨enius (1832-1910, 1849-1917): Un sistema de ecuaciones linea- les AX = B es compatible si y s´olo si rgA = rg(AjB) . Demostraci´on: Sean A1, . . . , An las columnas de A, de modo que el sistema AX = B puede escribirse x1A1 + . . . + xnAn = B, y la condici´on de que sea compatible signiﬁca que en K m tenemos que B 2 hA1, . . . , Ani; es decir, que hA1, . . . , Ani = hA1, . . . , An, Bi. Ahora bien, el teorema 2.6 aﬁrma que hA1, . . . , Ani = hA1, . . . , An, Bi , dimhA1, . . . , Ani = dimhA1, . . . , An, Bi y, de acuerdo con 2.4, esta ´ultima condici´on signiﬁca que rg A = rg (AjB). 2.3. Suma Directa Teorema 2.7 Sean V y W dos subespacios vectoriales de dimensi´on ﬁnita de un espacio vectorial E. Se cumple que dim (V + W ) = dim V + dim W \u0000 dim (V \\ W ). Demostraci´on: Tomemos una base u1, . . . , ud de V \\ W , que ampliamos hasta obtener una base u1, . . . , ud, v1, . . . , vm de V y una base u1, . . . , ud, w1, . . . , wn de W . Tenemos que d = dim (V \\ W ), d + m = dim V y d + n = dim W ; as´ı que basta ver que los vectores u1, . . . , ud, v1, . . . , vm, w1, . . . , wn forman una base de V + W . Los vectores u1, . . . , ud, v1, . . . , vm, w1, . . . , wn son linealmente independientes: Si una combinaci´on lineal es nula, P i λiui + P j αjvj + P k βkwk = 0, entonces P j αjvj = \u0000 P i λiui \u0000 P k βkwk 2 V \\ W = hu1, . . . , udi, P k βkwk = \u0000 P i λiui \u0000 P j αjvj 2 V \\ W = hu1, . . . , udi. Como hu1, . . . , udi \\ hv1, . . . , vmi = 0 = hu1, . . . , udi \\ hw1, . . . , wni, concluimos quePj αjvj = 0 y P k βkwk = 0, y por tanto tambi´en P i λiui = 0. Luego todos los coeﬁcientes λi, αj, βk son nulos, pues las familias u1, . . . , ud y v1, . . . , vm y w1, . . . , wn son linealmente independientes. Los vectores u1, . . . , ud, v1, . . . , vm, w1, . . . , wn generan V + W : Como para todo vector e 2 E se cumple que Ke + Ke = Ke, tenemos que V + W = Ku1 + . . . + Kud + Kv1 + . . . + Kvm + Ku1 + . . . + Kud + Kw1 + . . . + Kwn = Ku1 + . . . + Kud + Kv1 + . . . + Kvm + Kw1 + . . . + Kwn. Corolario 2.8 Si E y E′ son dos K-espacios vectoriales de dimensi´on ﬁnita, dim (E \u0002 E′) = dim E + dim E′. 18 Demostraci´on: Pongamos V = E \u0002 0, W = 0 \u0002 E′. Tenemos que dim E = dim V , dim E′ = dim W , y terminamos porque E \u0002 E′ = V + W y 0 = V \\ W . Deﬁnici´on: Sea E un espacio vectorial. Diremos que la suma V1+. . .+Vr de unos subespacios vectoriales V1, . . . , Vr de E es directa si cada vector e 2 V1 + . . . + Vr descompone de modo ´unico en la forma e = v1 + . . . + vr, donde vi 2 Vi. En tal caso, el subespacio vectorial V1 + . . . + Vr se denota V1 \b . . . \b Vr. Diremos que dos subespacios vectoriales V y W de E son suplementarios (o que W es un suplementario de V en E) cuando E = V \bW ; i.e., cuando cada vector de E descompone, y de modo ´unico, en suma de un vector de V y otro de W . Ejemplos: 1. Si e1, . . . , en es una base de un espacio vectorial E, entonces cada vector e 2 E descom- pone de modo ´unico como combinaci´on lineal e = λ1e1 + . . . + λnen; luego E = Ke1 \b . . . \b Ken, y un suplementario de Ke1 \b . . . \b Ker en E es Ker+1 \b . . . \b Ken. 2. Para hallar un suplementario de un subespacio vectorial V de E basta ampliar una base v1, . . . , vr de V hasta obtener una base v1, . . . , vr, w1, . . . , ws de E, pues un suplementario de V = Kv1 \b . . . \b Kvr en E es Kw1 \b . . . \b Kws. 3. Sean p + V y q + W dos subvariedades lineales de un espacio vectorial E. Dar un punto de corte es dar vectores v 2 V , w 2 W tales que p + v = q + w; es decir, q \u0000 p = v \u0000 w. Por tanto, la condici´on necesaria y suﬁciente para que se corten es que q \u0000 p 2 V + W , y el punto de corte es ´unico cuando la suma V \b W es directa. Teorema 2.9 La condici´on necesaria y suﬁciente para que la suma de dos subespacios vec- toriales V y W de un espacio vectorial E sea directa es que V \\ W = 0. Demostraci´on: Si e 2 V \\ W , tenemos que 0 = 0 + 0 = e + (\u0000e), donde 0, e 2 V y 0, \u0000e 2 W . Si la suma de V y W es directa, la unicidad de la descomposici´on del vector 0 en suma de un vector de V y otro de W implica que e = 0. Luego V \\ W = 0. Rec´ıprocamente, si V \\ W = 0 y un vector e 2 V + W admite dos descomposiciones e = v + w = v′ + w′ , donde v, v′ 2 V, w, w′ 2 W entonces v′ \u0000 v = w \u0000 w′ 2 W . Como v′ \u0000 v 2 V , se sigue que v′ \u0000 v 2 V \\ W = 0. Luego 0 = v′ \u0000 v = w \u0000 w′, y concluimos que v = v′ y w = w′. Es decir, tal descomposici´on es ´unica, as´ı que la suma de V y W es directa. Corolario 2.10 Si la suma de dos subespacios vectoriales V y W de E es directa, entonces dim (V \b W ) = dim V + dim W. Demostraci´on: De acuerdo con 2.9 tenemos que V \\ W = 0. Corolario 2.11 Sea E un espacio vectorial de dimensi´on n y sea V un subespacio vectorial de E de dimensi´on d. Un suplementario de V en E es cualquier subespacio vectorial W de E de dimensi´on n \u0000 d que cumpla que V \\ W = 0. Demostraci´on: Como V \\ W = 0, tenemos que dim (V \b W ) = d + (n \u0000 d) = dim E; luego V \b W = E por 2.6. 19 3. Aplicaciones Lineales Deﬁnici´on: Una aplicaci´on f : E ! E′ entre dos K-espacios vectoriales es K-lineal si es morﬁsmo de grupos, f (e + v) = f (e) + f (v), 8e, v 2 E, y f (λ \u0001 e) = λ \u0001 f (e) para todo λ 2 K, e 2 E, y, cuando K = C, es antilineal si es morﬁsmo de grupos y f (λ \u0001 e) = ¯λ \u0001 f (e), 8λ 2 K, e 2 E. Toda aplicaci´on lineal f cumple que f (λ1e1 + . . . + λnen) = λ1f (e1) + . . . + λnf (en), y toda aplicaci´on antilineal f cumple que f (λ1e1 + . . . + λnen) = ¯λ1f (e1) + . . . + ¯λnf (en). Teorema 3.1 Sean E, E′ dos K-espacios vectoriales, y e1, . . . , en una base de E. Dados n vectores e′ 1, . . . , e′ n 2 E′, se cumple que la aplicaci´on f : E \u0000! E′ , f (x1, . . . , xn) := f (x1e1 + . . . + xnen) = x1e′ 1 + . . . + xne′ n, es K-lineal, y es la ´unica aplicaci´on K-lineal f : E ! E′ tal que f (e1) = e′ 1, . . . , f (en) = e′ n. Demostraci´on: Veamos que la aplicaci´on f : E ! E′, f ( P i xiei) = P i xie′ i, es lineal: f \u0000P i xiei + P i yiei\u0001 = f \u0000Pi(xi + yi)ei\u0001 = P i(xi + yi)e′ i = = P i xie′ i + P i yie′ i = f \u0000P i xiei\u0001 + f \u0000Pi yiei\u0001. f \u0000λ P i xiei\u0001 = f \u0000P i λxiei\u0001 = P i λixie′ i = λ P i xie′ i = λf \u0000P i xiei\u0001. Ahora, si h : E ! E′ es otra aplicaci´on K-lineal que cumple e′ 1 = h(e1), . . . , e′ n = h(en), entonces h(x1e1 + . . . + xnen) = x1h(e1) + . . . + xnh(en) = x1e′ 1 + . . . + xne′ n = f (x1e1 + . . . + xnen). Ejemplos: 1. Dados e1, . . . , en 2 E, la aplicaci´on f : K n ! E, f (x1, . . . , xn) = x1e1 + . . . + xnen, es K-lineal seg´un 3.1. Su imagen es Im f = Ke1 + . . . + Ken; as´ı que f es epiyectiva cuando e1, . . . , en generan E. La condici´on de que e1, . . . , en sean linealmente indepen- dientes signiﬁca que Ker f = 0; as´ı que f es inyectiva cuando e1, . . . , en son linealmente independientes. Por tanto, f es biyectiva cuando e1, . . . , en forman una base de E. 2. Seg´un 3.1, toda aplicaci´on lineal f : Rn ! R es f (x1, . . . , xn) = a1x1 + . . . + anxn para ciertos n´umeros reales a1, . . . , an; y las ´unicas aplicaciones f : R ! R que son lineales son las funciones f (x) = ax, donde a 2 R. 3. Dada una matriz A 2 Mm×n(K), la aplicaci´on f : K n ! K m, f (X) = AX, (es decir, f (x1, . . . , xn) = x1A1 + . . . + xnAn, donde A1, . . . , An son las columnas de A) es lineal seg´un 3.1. Su n´ucleo Ker f est´a formado por todas las soluciones de la ecuaci´on homog´enea AX = 0, y la condici´on B 2 Im f signiﬁca que el sistema AX = B es compatible. 4. Sea E el espacio vectorial complejo formado por las funciones complejas de variable real ψ(x) = u(x) + iv(x) inﬁnitamente derivables. La derivada deﬁne una aplicaci´on C- lineal d dx : E \u0000! E, d dx ψ := u′ + iv′. Adem´as, ﬁjada una funci´on ϕ 2 E, la aplicaci´on T : E \u0000! E, T (ψ) = ϕψ, tambi´en es C-lineal. 5. La traza tr : Mn×n(K) ! K, tr (aij) = a11 + . . . + ann, es una aplicaci´on lineal. 6. La aplicaci´on 0 : E ! E′, que transforma cualquier vector e 2 E en el vector nulo 0 2 E′, es lineal. 7. Sea f : E ! E′ una aplicaci´on K-lineal, y sea V un subespacio vectorial de E. La apli- caci´on f jV : V ! E′, llamada restricci´on de f a V , tambi´en es K-lineal. 20 Teorema 3.2 Sean f, h : E ! E′ dos aplicaciones K-lineales y α 2 K. Tambi´en son K- lineales las aplicaciones f + h : E \u0000! E′ , (f + h)(e) = f (e) + h(e), αf : E \u0000! E′ , (αf )(e) = α \u0001 f (e). Por tanto, el conjunto de todas las aplicaciones K-lineales de E en E′ es un subespacio vectorial del espacio vectorial formado por todas las aplicaciones de E en E′. Demostraci´on: Si e, v 2 E y λ 2 K, se cumple que (f + h)(e + v) = f (e + v) + h(e + v) = f (e) + f (v) + h(e) + h(v) = (f + h)(e) + (f + h)(v), (f + h)(λe) = f (λe) + h(λe) = λf (e) + λh(e) = λ(f (e) + h(e)) = λ(f + h)(e), (αf )(e + v) = αf (e + v) = α(f (e) + f (v)) = αf (e) + αf (v) = (αf )(e) + (αf )(v), (αf )(λe) = αf (λe) = αλf (e) = λ(αf (e)) = λ(αf )(e). Teorema 3.3 Si dos aplicaciones f : E ! E′ y h : E′ ! E′′ son K-lineales, tambi´en lo es su composici´on h \u000e f : E ! E′′, (h \u000e f )(e) = h \u0000f (e) \u0001. Demostraci´on: Como f y h son morﬁsmos de grupos, tambi´en lo es h \u000e f por 1.2. Adem´as, para todo λ 2 K, e 2 E tenemos que (hf )(λe) = h \u0000f (λe)\u0001 = h \u0000λ \u0001 f (e) \u0001 = λ \u0001 h(f (e)) = λ \u0001 (hf )(e). 3.1. Matriz de una Aplicaci´on Lineal Sea f : E ! E′ una aplicaci´on lineal entre dos espacios vectoriales de dimensi´on ﬁnita, y ﬁjemos una base e1, . . . , en de E, y una base e′ 1, . . . , e′ m de E′. Seg´un 3.1, la aplicaci´on lineal f : E ! E′ est´a totalmente determinada por los vectores f (e1), . . . , f (en) 2 E′, y para ciertos escalares aij 2 K tendremos que f (ej) = a1je ′ 1 + . . . + amje′ m , 1 \u0014 j \u0014 n. Deﬁnici´on: Llamaremos matriz de una aplicaci´on lineal f : E ! E′ en las bases e1, . . . , en de E y e′ 1, . . . , e′ m de E′ a la matriz A = (aij) 2 Mm×n(K) cuyas columnas est´an formadas por las coordenadas de los vectores f (e1), . . . , f (en) en la base e ′ 1, . . . , e′ m. Si (x1, . . . , xn) son las coordenadas de un vector e 2 E en la base e1, . . . , en, tendremos que las coordenadas (x′ 1, . . . , x′ m) del vector f (e) = f (x1e1 + . . . + xnen) = x1f (e1) + . . . + xnf (en) 2 E′ en la base e′ 1, . . . , e′ m ﬁjada en E′ vienen dadas por las siguientes ecuaciones 8 >>< >>: x′ 1 = a11x1 + a12x2 + . . . + a1nxn x′ 2 = a21x1 + a22x2 + . . . + a2nxn . . . . . . . . . . . . . . . . . . x′ m = am1x1 + am2x2 + . . . + amnxn , 0 B @ x ′ 1 ... x′ m 1 C A = 0 @ a11 . . . a1n . . . . . . . . . am1 . . . amn 1 A 0 B @ x1 ... xn 1 C A Es decir, si X denota las coordenadas del vector e en la base e1, . . . , en, puestas en columna, entonces las coordenadas X ′ de f (e) en la base e′ 1, . . . , e′ m son X ′ = AX (3) Ejemplos: Sea A 2 Mm×n(K). La matriz de la aplicaci´on lineal f : K n ! K m, f (X) = AX, en las bases usuales de K n y K m es A. 21 La matriz de la identidad Id : E ! E, en cualquier base de E, es la matriz unidad I. Si A y B son las matrices de dos aplicaciones lineales f, h : E ! E′ en ciertas bases de E y E′, entonces la matriz de f + h en tales bases es A + B, y la matriz de λf es λA, 8λ 2 K. Matriz de la Composici´on: Dadas aplicaciones K-lineales f : E ! E′ y g : E′ ! E′′, si ﬁjamos bases en E, E′ y E′′, tendremos las matrices A, B, C de f, g, g \u000e f en tales bases. Si X son las coordenadas de un vector e 2 E, entonces AX son las coordenadas de f (e), y BAX son las coordenadas de g(f (e)) = (g \u000e f )(e). Luego BAX = CX para todo X 2 K n, y vemos que C = BA: La matriz de la composici´on g \u000e f es el producto BA. Cambio de Base: Sea e1, . . . , en una base de un espacio vectorial E. Si consideramos una nueva base v1, . . . , vn de E, tendremos escalares bij 2 K tales que vj = b1je1 + . . . + bnjen , 1 \u0014 j \u0014 n, (4) y llamaremos matriz de cambio de base a la matriz B = (bij) 2 Mn×n(K) cuyas columnas est´an formadas por las coordenadas de los vectores de la nueva base en la antigua. Es decir, B es la matriz de la identidad Id : E ! E cuando en el espacio de salida se considera la nueva base v1, . . . , vn y en el de llegada la base antigua e1, . . . , en. De acuerdo con 3, si Y son las coordenadas de un vector e 2 E en la nueva base, y X son las coordenadas de Id(e) = e en la base antigua, tendremos que X = BY . Por otra parte, tambi´en tenemos una matriz de cambio de base C 2 Mn×n(K) cuando se considera que v1, . . . , vn es la base inicial de E y que e1, . . . , en es la nueva base, de modo que Y = CX. Luego X = BCX y Y = CBY , y como estas igualdades son v´alidas para cualesquiera columnas X, Y , se concluye que BC = CB = I. Es decir, la matriz B es invertible, y su inversa es la matriz C. En resumen, la relaci´on entre las bases e1, . . . , en y v1, . . . , vn, y las coordenadas X e Y de un mismo vector e 2 E en esas bases, es 0 B @ v1 ... vn 1 C A = Bt 0 B @ e1 ... en 1 C A , 0 B @ x1 ... xn 1 C A = B 0 B @ y1 ... yn 1 C A , X = BY . (5) Veamos ahora c´omo se transforma la matriz de una aplicaci´on lineal cuando se cambian las bases. Sea f : E ! E′ una aplicaci´on lineal y sea A 2 Mm×n(K) su matriz en ciertas bases e1, . . . , en y e′ 1, . . . , e′ m de E y E′ respectivamente. Consideremos nuevas bases v1, . . . , vn y v′ 1, . . . , v′ m de E y E′ respectivamente, las correspondientes matrices de cambio de base B 2 Mn×n(K) y C 2 Mm×m(K), y sea ˜A 2 Mm×n(K) la matriz de f en estas nuevas bases de E y E′. Vamos a determinar ˜A en funci´on de A, B y C. Sean X e Y las coordenadas de un vector e 2 E en las bases e1, . . . , en y v1, . . . , vn respectivamente, y sean X ′ e Y ′ las coordenadas de f (e) 2 E′ en las bases e′ 1, . . . , e′ m y v′ 1, . . . , v′ m respectivamente. De acuerdo con 3 y 5 tendremos que X ′ = AX , Y ′ = ˜AY , X = BY , X ′ = CY ′. ˜AY = Y ′ = C −1X ′ = C −1AX = C −1ABY. Como esta igualdad es v´alida para cualquier columna Y 2 K n, vemos que ˜A = C −1AB . (6) 3.2. N´ucleo e Imagen de una Aplicaci´on Lineal Teorema 3.4 Sea f : E ! E′ una aplicaci´on lineal. Su n´ucleo Ker f = fe 2 E : f (e) = 0g es un subespacio vectorial de E, y su imagen Im f = fe′ 2 E′ : e ′ = f (e), 9e 2 Eg = ff (e)ge∈E es un subespacio vectorial de E′. 22 Demostraci´on: Como f es morﬁsmo de grupos, de acuerdo con 1.3 tenemos que Ker f es un subgrupo de E, y que Im f es un subgrupo de E′. Ahora, si e 2 Ker f y λK, entonces f (λe) = λf (e) = λ \u0001 0 = 0, as´ı que λe 2 Ker f . Si e ′ 2 Im f , existe e 2 E tal que e′ = f (e); luego λe′ = λf (e) = f (λe) 2 Im f , 8λ 2 K. Proposici´on 3.5 Si f : E ! E′ es una aplicaci´on lineal, y e1, . . . , en es una base de E, Im f = hf (e1), . . . , f (en)i. Por tanto, si A es la matriz de f en ciertas bases de E y E′, sus columnas son las coordenadas de un sistema de generadores de Im f , y se cumple que dim (Im f ) = rg A. En particular f es epiyectiva si y s´olo si rg A = dim E′. Demostraci´on: Como f (P i λiei) = P i λif (ei), tenemos que Im f = hf (e1), . . . , f (en)i. Ahora bien, como las columnas de A son las coordenadas de f (e1), . . . , f (en) en cierta base de E′, de acuerdo con 2.4 tenemos que dim hf (e1), . . . , f (en)i = rg A. Por ´ultimo, la aplicaci´on f es epiyectiva si y s´olo si Im f = E′. Ejemplo: Sea f : E ! E′ una aplicaci´on lineal, y sea A = (aij) su matriz en ciertas bases de E y E′. El n´ucleo de f est´a formado por los vectores de E cuyas coordenadas X en la base ﬁjada cumplen que AX = 0, 0 @ a11 a12 . . . a1n . . . . . . . . . . . . am1 am2 . . . amn 1 A 0 B B B B @ x1 ... ... xn 1 C C C C A = 0 B @ 0 ... 0 1 C A (ecuaciones impl´ıcitas de Ker f ) y, de acuerdo con 3.5, si las columnas de A se denotan A1, . . . , An, la imagen de f est´a formada por los vectores de E′ cuyas coordenadas X ′ cumplen que X ′ = λ1A1 + . . . + λnAn para ciertos escalares λ1, . . . , λn: 0 B @ x ′ 1 ... x′ m 1 C A = λ1 0 B @ a11 ... am1 1 C A + . . . + λn 0 B @ a1n ... amn 1 C A (ecuaciones param´etricas de Im f ) Deﬁnici´on: Una aplicaci´on K-lineal f : E ! E′ es un isomorﬁsmo cuando es biyectiva, y en tal caso la aplicaci´on inversa f −1 : E′ ! E tambi´en es K-lineal y por supuesto biyectiva, as´ı que f −1 tambi´en es un isomorﬁsmo. En efecto, si e′, v′ 2 E′, entonces e ′ = f (e) y v′ = f (v), donde e, v 2 E, de modo que f −1(e′ + v′) = f −1\u0000f (e) + f (v) \u0001 = f −1\u0000f (e + v)\u0001 = e + v = f −1(e′) + f −1(v′), f −1(λe′) = f −1\u0000λf (e) \u0001 = f −1\u0000f (λe) \u0001 = λe = λ \u0001 f −1(e′). Ejemplos: 1. Los isomorﬁsmos transforman vectores linealmente independientes en vectores linealmen- te independientes, y sistemas de generadores en sistemas de generadores; luego bases en bases. Por tanto, si dos K-espacios vectoriales E y E′ son isomorfos, tendremos que dim E = dim E′. 23 2. Si e1, . . . , en es una base de un K-espacio vectorial E, entonces la aplicaci´on lineal f : K n \u0000! E , f (x1, . . . , xn) = x1e1 + . . . + xnen, es un isomorﬁsmo. El isomorﬁsmo inverso f −1 : E ! K n asigna a cada vector e 2 E sus coordenadas (x1, . . . , xn) en la base e1, . . . , en. Por tanto, todo K-espacio vectorial de dimensi´on n es isomorfo a K n. 3. Si V1, . . . , Vn son subespacios vectoriales de un espacio vectorial E, la aplicaci´on s : V1 \u0002 . . . \u0002 Vn \u0000! V1 + . . . + Vn , s(v1, . . . , vn) = v1 + . . . + vn , es lineal y epiyectiva. Adem´as esta aplicaci´on lineal s es inyectiva precisamente cuando la suma es directa, de modo que en tal caso V1 \u0002 . . . \u0002 Vn ' V1 \b . . . \b Vn. Teorema de Isomorf´ıa: Sea f : E ! E′ una aplicaci´on lineal, y sea V un suplementario del n´ucleo de f en E; es decir, E = V \b (Ker f ). La aplicaci´on f jV : V ! Im f , f jV (v) = f (v), es un isomorﬁsmo lineal, y por tanto dim E = dim (Ker f ) + dim (Im f ). Demostraci´on: La aplicaci´on lineal f jV es inyectiva porque su n´ucleo es nulo: Si v 2 V y f (v) = 0, entonces v 2 V \\ (Ker f ) = 0. Veamos que f jV es epiyectiva: Si e′ 2 Im f , tendremos que e′ = f (e) para alg´un vector e 2 E = V + (Ker f ), de modo que e = v + w, donde v 2 V , w 2 Ker f , y por tanto e′ = f (e) = f (v + w) = f (v) + f (w) = f (v) = f jV (v). Corolario 3.6 Si A es la matriz de una aplicaci´on lineal f : E ! E′ en ciertas bases, dim (Ker f ) = (no de columnas) \u0000 rg A. En particular f es inyectiva si y s´olo si rg A = dim E. Demostraci´on: dim (Ker f ) = dim E \u0000 dim (Im f ) 3.5 = dim E \u0000 rg A. Adem´as, la aplicaci´on f es inyectiva si y s´olo si Ker f = 0, de acuerdo con 1.4. Corolario 3.7 Sea A 2 Mm×n(K). Las soluciones del sistema homog´eneo AX = 0 forman un subespacio vectorial V = fX 2 K n : AX = 0g de K n de dimensi´on dim V = n \u0000 rg A y las soluciones de un sistema no homog´eneo compatible AX = B forman una subvariedad lineal de K n de direcci´on AX = 0 y dimensi´on n \u0000 rg A. Demostraci´on: La matriz A deﬁne una aplicaci´on lineal f : K n ! K m, f (X) = AX, y V es precisamente el n´ucleo de f . La matriz de f en las bases usuales de K n y K m es A, as´ı que 3.6 aﬁrma que la dimensi´on de V es n \u0000 rg A. Por ´ultimo, si un sistema AX = B es compatible, las soluciones se obtienen sumando a una soluci´on particular X0 las soluciones de la ecuaci´on homog´enea AX = 0; luego forman la subvariedad lineal X0 + V y, por tanto, su dimensi´on es dim V = n \u0000 rg A. 24 En adelante, los escalares ser´an K = R ´o C. 4. Geometr´ıa Eucl´ıdea 4.1. Producto Escalar Deﬁniciones: Dar un producto escalar en un K-espacio vectorial E es asignar a cada par de vectores e, v 2 E un escalar, que denotaremos e \u0001 v ´o hejvi, de modo que 1. Es lineal a la derecha y antilineal a la izquierda: hejλv + µv′i = λhejvi + µhejv′i , hλe + µe′jvi = ¯λhejvi + ¯µhe′jvi. 2. hvjei = hejvi. 3. Es deﬁnido-positivo: hejei \u0015 0, y s´olo se da la igualdad cuando e = 0. y se dice que dos vectores e, v 2 E son ortogonales cuando hejvi = 0 (y por tanto hvjei = 0). Un isomorﬁsmo lineal T : E ! E es una isometr´ıa o un operador unitario cuando conserva el producto escalar: hejvi = hT (e)jT (v)i, 8e, v 2 E. El m´odulo de un vector e 2 E es el n´umero real kek := +phejei \u0015 0 (positivo cuando e 6= 0), de modo que kek2 = hejei, y kλek = jλj \u0001 kek. La distancia entre dos puntos p, q 2 E es d(p, q) := kq \u0000 pk. Ejemplos: 1. En la Geometr´ıa Eucl´ıdea cl´asica, ﬁjado un origen O, los puntos forman un espacio vectorial real de dimensi´on 3, dotado de un producto escalar (toda vez que se haya ﬁjado la unidad de longitud): el producto escalar de dos vectores es el producto de sus longitudes por el coseno del ´angulo que forman. 2. El producto escalar usual en K n es h(x1, . . . , xn)j(y1, . . . , yn)i := ¯x1y1 + . . . + ¯xnyn. 3. Fijado un intervalo [a, b] ˆ R, un producto escalar en el espacio vectorial complejo for- mado por las funciones continuas ψ : [a, b] ! C, ψ(x) = u(x) + v(x)i, es hψjϕi := Z b a ψ(x)ϕ(x) dx , kψk2 = hψjψi = Z b a jψ(x)j 2 dx. 4. En Mec´anica Cu´antica, el espacio de estados de un sistema cu´antico es un espacio vectorial complejo E (generalmente de dimensi´on inﬁnita) con un producto escalar, un estado del sistema es un subespacio vectorial unidimensional de E (normalmente representado por alguno de sus vectores de m´odulo 1, condici´on que lo determina salvo un factor eiθ, θ 2 R), y la evoluci´on temporal del estado del sistema viene dada, seg´un la ecuaci´on de Schr¨odinger (1886-1961), o de Dirac (1902-1984) en el caso relativista, por una familia de operadores unitarios Ut : E ! E, t 2 R, que cumple Ut+s = Ut \u000e Us. Proyecci´on Ortogonal sobre una Recta: Fijada una recta Kv, todo vector e 2 E des- compone en suma e = λv + w, donde hvjwi = 0 y λ = (v \u0001 e)/(v \u0001 v). we v λv Ke0 \u0014 Proyecc. ortog. de e sobre Kv \u0015 = v \u0001 e v \u0001 v v 25 Teorema de Pit´agoras (s. VI a.C.): ke + vk 2 = kek 2 + kvk2 , cuando hejvi = 0. Desigualdad de Cauchy-Schwarz (1789-1857, 1843-1921): je \u0001 vj \u0014 kek \u0001 kvk. Desigualdad triangular: ke + vk \u0014 kek + kvk. Demostraci´on: (1) Tenemos que v \u0001 \u0000e \u0000 λv\u0001 = v \u0001 e \u0000 λ(v \u0001 v) = 0 cuando λ = v·e v·v . (2) Tenemos que ke + vk 2 = (e + v) \u0001 (e + v) = e \u0001 e + e \u0001 v + v \u0001 e + v \u0001 v = kek 2 + 0 + 0 + kvk 2. (3) Es obvia cuando v = 0. Cuando v 6= 0, ponemos e = λv + w, con v \u0001 w = 0. Por el teorema de Pit´agoras kek 2 = jλj 2kvk 2 + kwk 2, luego jλj \u0001 kvk \u0014 kek, y je \u0001 vj = j(λv + w) \u0001 vj = j(λv) \u0001 vj = jλj \u0001 kvk2 \u0014 kek \u0001 kvk. (4) Basta tomar ra´ız cuadrada en la siguiente consecuencia de la desigualdad anterior: ke + vk2 = (e + v) \u0001 (e + v) = e \u0001 e + v \u0001 v + e \u0001 v + e \u0001 v \u0014 \u0014 e \u0001 e + v \u0001 v + 2je \u0001 vj \u0014 kek 2 + kvk 2 + 2kek \u0001 kvk = (kek + kvk)2. Deﬁnici´on: En el caso real, K = R, cuando los vectores e y v no son nulos, por la desigualdad de Cauchy-Schwarz tenemos que \u00001 \u0014 e·v ∥e∥·∥v∥ \u0014 1, y se llama medida en radianes del ´angulo que forman e y v al ´unico n´umero real α entre 0 y π que cumple e \u0001 v = kek \u0001 kvk cos α. Ejemplos: 1. El ´angulo es α = π 2 justo cuando e y v son ortogonales, e \u0001 v = 0. 2. En el caso de un tri´angulo rect´angulo, el coseno de un ´angulo es el cociente del cateto contiguo por la hipotenusa: a b c e v \u0000 ev α 0 = e \u0001 (v \u0000 e) = e \u0001 v \u0000 e 2 , e \u0001 v = kek2 cos α = e \u0001 v kek \u0001 kvk = kek 2 kek \u0001 kvk = kek kvk 3. Dados escalares no nulos λ, µ 2 R, el ´angulo β que forman λe y µv cumple que cos β = (λe) \u0001 (µv) kλek \u0001 kµvk = λµ(e \u0001 v) jλµjkek \u0001 kvk = (cos α cuando λµ > 0 \u0000 cos α cuando λµ < 0 as´ı que β = α cuando λµ > 0, y β = π \u0000 α cuando λµ < 0. 4.2. Espacios Vectoriales Eucl´ıdeos Deﬁniciones: Llamaremos espacio vectorial eucl´ıdeo 1, en honor de Euclides (c. 325- 265 a.C.), a todo espacio vectorial E de dimensi´on ﬁnita dotado de un producto escalar, y diremos que una base u1, . . . , un de E es ortonormal si los vectores de la base son de m´odulo 1 y mutuamente ortogonales: huijuji = δij := (1 cuando i = j 0 cuando i 6= j 1En el caso complejo se llaman espacios vectoriales herm´ıticos, en honor de Hermite (1822-1901). 26 En una base ortonormal el producto escalar de dos vectores e, v 2 E de coordenadas X t = (x1, . . . , xn), Y t = (y1, . . . , yn) es hejvi = (x1u1 + . . . + xnun) \u0001 (y1u1 + . . . + ynun) = ¯x1y1 + . . . + ¯xnyn, hejvi = ¯X tY . hejei = P i ¯xixi = P i jxij 2, hejei = ¯X tX. Ejemplos: 1. En K n, con el producto escalar usual (x1, . . . , xn) \u0001 (y1, . . . , yn) = ¯x1y1 + . . . + ¯xnyn, la base u1 = (1, 0, . . . , 0), . . . , un = (0, . . . , 0, 1) es ortonormal. 2. Sean a1, . . . , an n´umeros enteros distintos. Las funciones u1 = e2πia1x, . . . , un = e2πianx forman una base ortonormal de Ce2πia1x + . . . + Ce2πianx cuando el producto escalar es hψjϕi = Z 1 0 ψ(x)ϕ(x) dx, he2πiaj xje2πiakxi = Z 1 0 e2πi(ak−aj )x dx = 8 >>< >>: Z 1 0 dx = 1 , j = k. \u0014 e2πi(ak−aj )x 2πi(ak \u0000 aj) \u00151 0 = 0 , j 6= k. En el espacio herm´ıtico Ceia1x + . . . + Ceianx, hψjϕi = R 2π 0 ψ(x)ϕ(x) dx, el mismo c´alculo prueba que he iaixjeiaj xi = 2πδij, as´ı que una base ortonormal de Ceia1x + . . . + Ceianx est´a deﬁnida por las funciones u1 = 1√2π e ia1x, . . . , un = 1√2π e ianx. 3. Un producto escalar en Mm×n(K) es hAjBi := tr( ¯AtB), y una base ortonormal de M2×2(K) est´a formada por las matrices U11 = \u00121 0 0 0 \u0013, U12 = \u00120 1 0 0 \u0013, U21 = \u00120 0 1 0 \u0013, U22 = \u0012 0 0 0 1 \u0013 . 4. Un isomorﬁsmo T : E ! E, de matriz A en una base ortonormal u1, . . . , un, es una isometr´ıa cuando ¯X tY = ( ¯A ¯X) t(AY ) = ¯X t ¯AtAY para todo par de columnas X, Y 2 K n; i.e. cuando A es una matriz unitaria: ¯AtA = In, o lo que es lo mismo, A−1 = ¯At. Si v1, . . . , vn es otra base de E, y B = (bij) es la matriz de cambio de base, vj = P i bijui, entonces (vi \u0001 vj) = ¯BtB, as´ı que la nueva base v1, . . . , vn tambi´en es ortonormal si y s´olo si B es una matriz unitaria, ¯BtB = In. 5. En el espacio vectorial herm´ıtico E = C cos x + Csen x, hψjϕi = R 2π 0 ψ(x)ϕ(x) dx, vamos a calcular la proyecci´on ortogonal de ψ(x) = cos x sobre la recta Cϕ que genera la funci´on ϕ(x) = sen x. Para ello ﬁjamos en E la base ortonormal u1 = 1√2π eix, u2 = 1√2π e−ix. ψ(x) = cos x = 1 2 (eix + e−ix) = √2π 2 u1 + √2π 2 u2 = \u0010 √2π 2 , √2π 2 \u0011 ϕ(x) = sen x = 1 2i (eix \u0000 e−ix) = \u0000 √2π 2 iu1 + √2π 2 iu2 = \u0010\u0000 √2π 2 i, √2π 2 i \u0011 hϕjψi hϕjϕi ϕ = π 2 i \u0000 π 2 i π 2 + π 2 ϕ = 0. Es decir, las funciones cos x y sen x son ortogonales. Como k cos xk 2 = hψjψi = π y ksen xk2 = hϕjϕi = π, vemos que las funciones v1 = 1√π cos x = 1√2 u1 + 1√ 2 u2 v2 = 1√π sen x = \u0000 i√2 u1 + i√2 u2 27 forman una base ortonormal de E; luego la matriz de cambio de base B es unitaria: B = \u00121/ p2 \u0000i/p2 1/ p2 i/p2 \u0013 , B−1 = ¯Bt = \u00121/p2 1/ p2 i/ p2 \u0000i/ p2 \u0013 . Teorema 4.1 Todo espacio vectorial eucl´ıdeo E 6= 0 admite bases ortonormales. Demostraci´on (M´etodo de Gram-Schmidt (1850-1916, 1876-1959): Dada una base e1, . . . , en de E, cada uno de los siguientes vectores vi v1 = e1 , hv1i = he1i v2 = e2 \u0000 v1 \u0001 e2 v1 \u0001 v1 v1 , hv1, v2i = he1, e2i v3 = e3 \u0000 v1 \u0001 e3 v1 \u0001 v1 v1 \u0000 v2 \u0001 e3 v2 \u0001 v2 v2 , hv1, v2, v3i = he1, e2, e3i . . . . . . . . . . . . . . . . . . , . . . . . . . . . . . . vn = en \u0000 v1 \u0001 en v1 \u0001 v1 v1 \u0000 . . . \u0000 vn−1 \u0001 en vn−1 \u0001 vn−1 vn−1 , hv1, . . . , vni = he1, . . . , eni es no nulo, porque ei+1 /2 he1, . . . , eii = hv1, . . . , vii, y ortogonal a los anteriores por cons- trucci´on; es decir, vj \u0001 vi = 0 cuando j < i. Adem´as v1, . . . , vn es una base de E porque hv1, . . . , vni = he1, . . . , eni = E. Luego los vectores u1 = v1 ∥v1∥ , . . . , un = vn ∥vn∥ forman una base ortonormal de E. Ejemplo: Una hip´otesis fundamental de la Mec´anica Cl´asica es que el espacio (ﬁjado un origen y una unidad de longitud) es un espacio vectorial eucl´ıdeo real de dimensi´on 3, y una base ortonormal suele denotarse ⃗i,⃗j, ⃗k. 4.3. Proyecci´on Ortogonal Deﬁnici´on: El ortogonal de un subespacio vectorial V de un espacio eucl´ıdeo E es V ⊥ := fe 2 E : hvjei = 0 para todo vector v 2 V g. Teorema 4.2 El ortogonal V ⊥ es un subespacio vectorial de E de dimensi´on dim V ⊥ = dim E \u0000 dim V y cada vector de E descompone de modo ´unico en suma de un vector de V y otro de V ⊥: E = V \b V ⊥. Demostraci´on: Sea v1, . . . , vd una base de V , y consideremos el n´ucleo de la aplicaci´on lineal f : E \u0000! K d , f (e) = (v1 \u0001 e, . . . , vd \u0001 e), Ker f = fe 2 E : v1 \u0001 e = . . . = vd \u0001 e = 0g = (Kv1 + . . . + Kvd) ⊥ = V ⊥. En efecto, si un vector e es ortogonal a ciertos vectores, v1 \u0001 e = . . . = vd \u0001 e = 0, entonces e 2 (Kv1 + . . . + Kvd) ⊥ porque tambi´en es ortogonal a todas las combinaciones lineales: (λ1v1 + . . . + λdvd) \u0001 e = ¯λ1(v1 \u0001 e) + . . . + ¯λd(vd \u0001 e) = 0. Luego V ⊥ es un subespacio vectorial de E y, por el Teorema de Isomorf´ıa, dim E = dim V ⊥ + dim (Im f ) \u0014 dim V ⊥ + d = dim V ⊥ + dim V, (7) donde la desigualdad se debe a que la imagen de f es un subespacio vectorial de Rd. 28 Por otra parte, si v 2 V ⊥ \\ V , entonces v \u0001 v = 0; luego v = 0, porque el producto escalar es deﬁnido-positivo, y vemos que V ⊥ \\ V = 0. Luego la suma de V y V ⊥ es directa por 2.9 y, de acuerdo con 2.10, tenemos que dim V ⊥ + dim V 2.7 = dim (V ⊥ \b V ) \u0014 dim E (8) y comparando con (7) concluimos que dim (V ⊥ \b V ) = dim V ⊥ + dim V = dim E. Adem´as 2.6 permite concluir que V ⊥ \b V = E. Corolario 4.3 (V ⊥) ⊥ = V . Demostraci´on: Tenemos que V \u0012 (V ⊥)⊥ por deﬁnici´on de V ⊥, y coinciden porque tienen la misma dimensi´on: dim (V ⊥)⊥ = dim E \u0000 dim (V ⊥) = dim V . Corolario 4.4 Dos subespacios vectoriales son iguales cuando sus ortogonales coinciden. Deﬁnici´on: Si V es un subespacio vectorial de un espacio vectorial eucl´ıdeo E, de acuerdo con 4.2 tenemos que E = V ⊥ \b V . La aplicaci´on PV : E ! E, PV (v + w) = v, donde v 2 V y w 2 V ⊥, es lineal y se llama proyecci´on ortogonal sobre V . Es decir, cada vector e 2 E descompone de modo ´unico en suma, e = v + w, de un vector v 2 V y otro w 2 V ⊥, y por deﬁnici´on PV (e) = v, PV ⊥ (e) = w. N´otese que e = PV (e) + PV ⊥(e), de modo que PV ⊥ (e) = e \u0000 PV (e), y PV + PV ⊥ = Id. Proposici´on 4.5 Si u1, . . . , ud es una base ortonormal de V , se cumple que PV (e) = hu1jeiu1 + . . . + hudjeiud. Demostraci´on: Pongamos PV (e) = P j λjuj. Como e \u0000 PV (e) 2 V ⊥, tenemos que ui \u0001 e = ui \u0001 PV (e) = ui \u0001 \u0000P j λjuj\u0001 = P j λj(ui \u0001 uj) = P j λjδij = λi. Ejemplo: Cuando V = Kv es una recta, el vector u = v/kvk deﬁne una base ortonormal de V , y volvemos a obtener la f´ormula de la proyecci´on ortogonal de un vector e 2 E sobre la recta que genera un vector v: PV (e) = (u \u0001 e)u = 1 kvk2 (v \u0001 e)v = v \u0001 e v \u0001 v v. Nota: El teorema 4.2 permite dar otra demostraci´on de la existencia de bases ortonormales: Tomemos un vector no nulo v 2 E y pongamos u = v/kvk, de modo que kuk = 1. Procedamos por inducci´on sobre n = dim E. Si n = 1, tenemos que E = Ku, y u es una base ortonormal de E. Si n > 1, de acuerdo con 4.2, tenemos que dim (Ku) ⊥ = n \u0000 1, y por hip´otesis de inducci´on existe una base ortonormal u2, . . . , un de (Ku) ⊥. Estos vectores u, u2, . . . , un son de m´odulo 1 y mutuamente ortogonales, y forman una base de E de acuerdo con el siguiente lema: Lema 4.6 Si unos vectores no nulos v1, . . . , vr son mutuamente ortogonales, hvijvji = 0 cuando i 6= j, entonces son linealmente independientes. Demostraci´on: Si P i λivi = 0, entonces 0 = vj \u0001 (P i λivi) = P i λi(vj \u0001 vi) = λj(vj \u0001 vj) para todo ´ındice j = 1, . . . , r. Concluimos que λj = 0 porque vj \u0001 vj 6= 0. 29 5. Endomorﬁsmos Recu´erdese que los escalares son K = R ´o C. Sea p(x) un polinomio no constante con coeﬁcientes complejos. En este cap´ıtulo usaremos el siguiente teorema fundamental, que se demostrar´a en el curso de Variable Compleja: Teorema de D’Alembert (1717-1783): Todo polinomio no constante con coeﬁcientes com- plejos admite alguna ra´ız compleja. Regla de Ruﬃni (1765-1822): Si α es una ra´ız compleja de p(x), entonces p(x) = (x \u0000 α)q(x). Demostraci´on: Dividiendo p(x) por x \u0000 α tendremos que p(x) = (x \u0000 α)q(x) + r, donde el resto r es de grado menor que 1; luego constante. Sustituyendo x = α en esta igualdad obtenemos que el resto es nulo: 0 = p(α) = (α \u0000 α)q(α) + r = r. Deﬁnici´on: Si α es una ra´ız compleja de p(x), llamaremos multiplicidad de tal ra´ız al mayor n´umero natural m tal que (x \u0000 α)m divida a p(x). Las ra´ıces de multiplicidad 1 se denominan simples. Consideremos una ra´ız compleja α1 de p(x), que tendr´a cierta multiplicidad m1, de modo que p(x) = (x \u0000 α1)m1 q1(x), donde el polinomio q1(x) ya no admite la ra´ız α1. Tomemos una ra´ız compleja α2 de q1(x), que tendr´a cierta multiplicidad m2, de modo que p(x) = (x \u0000 α1)m1 (x \u0000 α2) m2q2(x). Por el teorema de D’Alembert, podemos proceder as´ı hasta que el factor qi(x) sea constante, y obtenemos una descomposici´on p(x) = c(x \u0000 α1) m1(x \u0000 α2)m2 . . . (x \u0000 αr)mr , (9) donde α1, . . . , αr son las ra´ıces complejas de p(x), los exponentes m1, . . . , mr son sus res- pectivas multiplicidades, y c es constante. Esta descomposici´on muestra que el n´umero de ra´ıces complejas de un polinomio no constante, contadas con su multiplicidad, coincide siempre con el grado del polinomio. 5.1. Valores y Vectores Propios Deﬁnici´on: Los endomorﬁsmos u operadores lineales de un K-espacio vectorial E son las aplicaciones K-lineales T : E ! E. Si S, T : E ! E son endomorﬁsmos, su suma S + T , el producto λT por un escalar λ, y su producto ST := S \u000e T tambi´en son endomorﬁsmos. Deﬁniciones: Dado un endomorﬁsmo T de un K-espacio vectorial E, diremos que un escalar α 2 K es un valor propio o autovalor de T si existe alg´un vector no nulo e 2 E tal que T (e) = αe, en cuyo caso diremos que e es un vector propio o autovector de T de valor propio α y que el subespacio propio del valor propio α es el subespacio vectorial (3.4) Vα := fe 2 E : T (e) = αeg = fe 2 E : αId(e) \u0000 T (e) = 0g = Ker (αId \u0000 T ) 6= 0. El espectro de T es el conjunto formado por los valores propios de T (puede ser vac´ıo). Ejemplos: 1. Un endomorﬁsmo T tiene el valor propio 0 si y s´olo si Ker T 6= 0, es decir, cuando T no es inyectivo, en cuyo caso V0 = Ker T . 30 2. Fijemos una base ortonormal u, v en un plano eucl´ıdeo real E. a) El espectro del giro de ´angulo recto G : E ! E, G(xu + yv) = \u0000yu + xv, es vac´ıo. b) El espectro de la simetr´ıa S : E ! E, S(xu + yv) = xu \u0000 yv, respecto de la recta Ru es f\u00061g, y V1 = Ru, V−1 = Rv. c) El espectro de la proyecci´on ortogonal P : E ! E, P (xu + yv) = xu, sobre la recta Ru es f0, 1g, y V0 = Rv, V1 = Ru. d ) El espectro de la homotecia H : E ! E, H(xu + yv) = axu + ayv, donde a 2 R es un n´umero real ﬁjado, es fag, y Va = E. 3. Sea E el espacio vectorial complejo formado por las funciones ψ : R ! C inﬁnitamente derivables. Todo n´umero complejo α es un valor propio del operador lineal d dx : E ! E porque d dx (eαx) = αeαx. Si otra funci´on ψ(x) 2 E cumple que ψ′ = αψ, entonces d dx \u0012 ψ(x) eαx \u0013 = αψ(x)eαx \u0000 ψ(x)αeαx e2αx = 0, y vemos que ψ(x) = λeαx para alg´un λ 2 C. En este caso el espectro es C, y Vα = Ceαx siempre es de dimensi´on 1. Fijada una base e1, . . . , en de E, un endomorﬁsmo T : E ! E est´a determinado por su matriz A = (aij) 2 Mn×n(K) en tal base, y de acuerdo con 3.6 tenemos que dim Ker (αId \u0000 T ) = n \u0000 rg(αIn \u0000 A). Deﬁnici´on: Diremos que el polinomio caracter´ıstico de T es el polinomio c(x) := jxIn \u0000Aj = \f \f \f \f \f \f \f \f x \u0000 a11 \u0000a12 . . . \u0000a1n \u0000a21 x \u0000 a22 . . . \u0000a2n . . . . . . . . . . . . \u0000an1 \u0000an2 . . . x \u0000 ann \f \f \f \f \f \f \f \f = xn \u0000\u0010 nP i=1 aii\u0011 xn−1 +. . .+(\u00001)njAj, que s´olo depende de T y no de la base ﬁjada en el espacio vectorial E (en particular, los escalares tr T := a11 + . . . + ann y det T := jAj no dependen de la base ﬁjada, y reciben el nombre de traza y determinante de T ), pues si se considera una nueva base en E, y B es la matriz del cambio de base, seg´un 6 (p´agina 22) la matriz ˜A de T en la nueva base es ˜A = B−1AB (10) jxIn \u0000 ˜Aj = jxB−1InB \u0000 B−1ABj = jB−1(xIn \u0000 A)Bj = jB−1j \u0001 jxIn \u0000 Aj \u0001 jBj = jxIn \u0000 Aj. Teorema 5.1 Los valores propios de un endomorﬁsmo son las ra´ıces en K de su polinomio caracter´ıstico. Demostraci´on: Un escalar α 2 K es valor propio de T cuando dim Ker (αId \u0000 T ) 6= 0. Luego α es valor propio de T si y s´olo si rg(αIn \u0000 A) < n; i.e., c(α) = jαIn \u0000 Aj = 0. Corolario 5.2 El n´umero de valores propios de un endomorﬁsmo de un espacio vectorial de dimensi´on n siempre es menor o igual que n. Demostraci´on: El grado del polinomio caracter´ıstico es n, y el n´umero de ra´ıces en K de un polinomio siempre est´a acotado por el grado del polinomio. Corolario 5.3 Todo endomorﬁsmo de un espacio vectorial complejo de dimensi´on ﬁnita n \u0015 1 tiene alg´un valor propio. 31 Demostraci´on: El polinomio caracter´ıstico no es constante, porque es de grado n, as´ı que tiene alguna ra´ız compleja por el Teorema de D’Alembert. Corolario 5.4 Todo endomorﬁsmo T : E ! E de un espacio vectorial complejo de dimen- si´on ﬁnita admite una base en que su matriz A = (aij) es triangular (aij = 0 cuando i > j). Es decir, para cada matriz A 2 Mn×n(C) existe alguna matriz invertible B 2 Mn×n(C) tal que la matriz B−1AB es triangular. Demostraci´on: Procedemos por inducci´on sobre n = dim E, y cuando n = 1, toda matriz es triangular. Cuando n > 1, de acuerdo con 5.3, el endomorﬁsmo T tiene alg´un valor propio α 2 C, y consideramos un vector propio e1 2 E de valor propio α, y una base e1, . . . , en en E. La matriz A de T en esta base es de la forma A = \u0012α . . . 0 An−1 \u0013 (11) para cierta matriz An−1 2 M(n−1)×(n−1)(C). Por hip´otesis de inducci´on existe una matriz invertible Bn−1 2 M(n−1)×(n−1)(C) tal que la matriz B−1 n−1An−1Bn−1 es triangular. Terminamos porque tambi´en es triangular la matriz B−1AB = \u0012 1 0 0 B−1 n−1 \u0013 \u0012α . . . 0 An−1 \u0013 \u00121 0 0 Bn−1 \u0013 = \u0012α . . . 0 B−1 n−1An−1Bn−1 \u0013 . Teorema de Hamilton-Cayley (1805-1865 y 1821-1895): El polinomio caracter´ıstico c(x) = xn + . . . + c1x + c0 de un endomorﬁsmo T de un K-espacio vectorial de dimensi´on ﬁnita E siempre anula al endomorﬁsmo: c(T ) := T n + . . . + c1T + c0Id = 0. Demostraci´on: Si A 2 Mn×n(K) es la matriz de T en una base de E, la matriz del endo- morﬁsmo c(T ) es c(A) = An + . . . + c1A + c0I = 0, as´ı que el teorema aﬁrma que c(A) = 0, donde c(x) = jxI \u0000 Aj; luego basta probarlo en el caso K = C, y en tal caso procedemos por inducci´on sobre n = dim E. Si n = 1, entonces A = (a). Luego c(x) = x \u0000 a y c(A) = A \u0000 aI = 0. Si n > 1, ﬁjando una base adecuada podemos suponer que A es de la forma 11. Luego c(x) = (x \u0000 α)cn−1(x), donde cn−1(x) = jxIn−1 \u0000 An−1j y, por hip´otesis de inducci´on, cn−1(An−1) = 0. Ahora Ar = \u0012αr . . . 0 Ar n−1 \u0013 , c(A) = (A \u0000 αIn)cn−1(A) = \u0012 0 . . . 0 C \u0013 \u0012cn−1(α) . . . 0 0 \u0013 = 0. 5.2. Diagonalizaci´on de Endomorﬁsmos Deﬁnici´on: Un endomorﬁsmo T de un K-espacio vectorial de dimensi´on ﬁnita E es dia- gonalizable si existe alguna base e1, . . . , en de E formada por vectores propios de T ; i.e., T (ej) = αjej para ciertos escalares αj 2 K, lo que signiﬁca que la matriz de T en tal base es diagonal (todos sus coeﬁcientes son nulos, salvo quiz´as los de la diagonal): D = 0 B B @ α1 0 . . . 0 0 α2 . . . 0 . . . . . . . . . . . . 0 0 . . . αn 1 C C A De acuerdo con 10, un endomorﬁsmo T de matriz A es diagonalizable si existe alguna matriz invertible B tal que D = B−1AB es una matriz diagonal. Teorema 5.5 Sean α1, . . . , αm valores propios de un endomorﬁsmo T , distintos entre s´ı. La suma de los subespacios propios Vα1, . . . , Vαm es directa, y por tanto dim Vα1 + . . . + dim Vαm = dim (Vα1 \b . . . \b Vαm) \u0014 dim E. 32 Demostraci´on: Sea Vα1 \u0002 . . . \u0002 Vαm el espacio vectorial formado por las sucesiones de vec- tores (v1, . . . , vm), donde vi 2 Vαi . Por deﬁnici´on de suma directa, hemos de probar que es biyectiva la siguiente aplicaci´on lineal (que siempre es epiyectiva): s : Vα1 \u0002 . . . \u0002 Vαm \u0000! Vα1 + . . . + Vαm , s(v1, . . . , vm) = v1 + . . . + vm. Es decir, tenemos que ver que Ker s = 0. Procedemos por inducci´on sobre m. Cuando m = 1, es cierto porque s : Vα1 ! Vα1 , s(v1) = v1, es claramente biyectiva. Si m > 1 y v1 + . . . + vm = 0, donde vi 2 Vαi, tendremos que 0 = T (0) = T (v1 + . . . + vm) = T (v1) + . . . + T (vm) = α1v1 + . . . + αmvm y restando la igualdad 0 = αm(v1 + . . . + vm) = αmv1 + . . . + αmvm vemos que 0 = (α1 \u0000 αm)v1 + . . . + (αm−1 \u0000 αm)vm−1. Por hip´otesis de inducci´on, (α1 \u0000 αm)v1 = . . . = (αm−1 \u0000 αm)vm−1 = 0. Como αi 6= αj cuando i 6= j, concluimos que v1 = 0, . . . , vm−1 = 0, y por tanto que tambi´en 0 = v1 + . . . + vm−1 + vm = 0 + . . . + 0 + vm = vm. Por ´ultimo, dim (Vα1 \b . . . \b Vαm ) = dim Vα1 + . . . + dim Vαm , de acuerdo con 2.10. Criterio de Diagonalizaci´on: Sea fα1, . . . , αrg el espectro de un endomorﬁsmo T de un K-espacio vectorial E de dimensi´on ﬁnita n. Las siguientes condiciones son equivalentes: 1. T es diagonalizable; i.e. E admite una base formada por vectores propios de T . 2. Todo vector no nulo de E descompone, y de modo ´unico salvo el orden, en suma de vectores propios de valores propios distintos. Es decir, E = Vα1 \b . . . \b Vαr . 3. Todas las ra´ıces complejas del polinomio caracter´ıstico c(x) est´an en K, y la multipli- cidad mi de cada ra´ız x = αi coincide con la dimensi´on di del subespacio propio Vαi . Demostraci´on: (1 ) 2) Si T es diagonalizable, por deﬁnici´on E admite una base e1, . . . , en formada por vectores propios de T , de modo que e1, . . . , en 2 Vα1 \b . . . \b Vαr . Luego E = he1, . . . , eni \u0012 Vα1 \b . . . \b Vαr , y concluimos que Vα1 \b . . . \b Vαr = E. (2 ) 1 y 3) Si Vα1 \b . . . \b Vαr = E, considerando una base ei 1, . . . , ei di en cada subespacio propio Vαi, donde di = dim Vαi , obtenemos vectores propios e1 1, . . . , e1 d1 , . . . , er 1, . . . , er dr que forman una base2 de E porque generan Vα1 \b . . . \b Vαr = E y d1 + . . . + dr = dim Vα1 + . . . + dim Vαr = dim (Vα1 \b . . . \b Vαr ) = dim E. Adem´as, la matriz de T en tal base de vectores propios e1 1, . . . , e1 d1, . . . , er 1, . . . , er dr es diagonal D = diag(α1, d1. . . α1, . . . , αr dr. . . αr), de modo que c(x) = jxIn \u0000 Dj = (x \u0000 α1)d1 . . . (x \u0000 αr) dr , y las ra´ıces complejas de c(x) son α1, . . . , αr 2 K, con multiplicidades d1, . . . , dr. (3 ) 2) Como el n´umero de ra´ıces complejas de un polinomio, contadas con su multipli- cidad, coincide con el grado, tenemos que dim E = gr c(x) = m1 + . . . + mr = dim Vα1 + . . . + dim Vαr = dim (Vα1 \b . . . \b Vαr ). Nota: Las ecuaciones impl´ıcitas de Vαi = Ker (αiId \u0000 T ) son (αiIn \u0000 A)X = 0, donde A es la matriz de T en alguna base de E; as´ı que di = dim Vαi = n \u0000 rg (αiIn \u0000 A). 2Por tanto, cuando T es diagonalizable, para hallar una base de E formada por vectores propios de T , basta juntar bases de los subespacios propios Vαi . 33 Corolario 5.6 Si todas las ra´ıces complejas del polinomio caracter´ıstico c(x) est´an en K y son simples, entonces T es diagonalizable y dim Vα = 1 para cualquier valor propio α. Demostraci´on: Como 1 \u0014 dim Vα para todo valor propio α, siempre se cumple que r \u0014 dim Vα1 + . . . + dim Vαr \u0014 dim E = gr c(x). Cuando todas las ra´ıces de c(x) est´an en K y son simples, tenemos que r = gr c(x). Luego dim Vαi = 1 para todo ´ındice i = 1, . . . , r, y dim (Vα1 \b . . . \b Vαr ) = dim E. Concluimos que Vα1 \b . . . \b Vαr = E y T es diagonalizable. 5.3. Operadores Lineales Autoadjuntos Deﬁnici´on: Sea E un espacio vectorial dotado de un producto escalar. Un operador lineal T : E ! E es autoadjunto o herm´ıtico, en honor de Hermite (1822-1901), cuando hT (e)jvi = hejT (v)i , 8e, v 2 E. Si la dimensi´on de E es ﬁnita y A es la matriz de T en una base ortonormal de E, esto signiﬁca que ¯X t ¯AtY = ¯X tAY para cualesquiera columnas X, Y 2 K n; es decir, que la matriz A es herm´ıtica o autoadjunta, A = ¯At (sim´etrica, A = At, en el caso real). Teorema Espectral: Todos los valores propios de un endomorﬁsmo autoadjunto T : E ! E son reales, y dos vectores propios de T de valores propios distintos siempre son ortogonales. Si adem´as E es de dimensi´on ﬁnita, admite una base ortonormal formada por vectores propios de T . Luego T es diagonalizable, y toda ra´ız compleja x = α de su polinomio carac- ter´ıstico c(x) es real y de multiplicidad igual a la dimensi´on del subespacio propio Vα. Demostraci´on: Si α es un valor propio de T , entonces existe alg´un vector no nulo e 2 E tal que T (e) = αe. Como e \u0001 e 6= 0, concluimos que α es real (es decir, que ¯α = α): α(e \u0001 e) = e \u0001 (αe) = e \u0001 T (e) = T (e) \u0001 e = (αe) \u0001 e = ¯α(e \u0001 e). Adem´as, si v 2 E es un vector propio de T de valor propio β 6= α, como ¯α = α, tenemos que e \u0001 v = 0 porque α(e \u0001 v) = ¯α(e \u0001 v) = (αe) \u0001 v = T (e) \u0001 v = e \u0001 T (v) = e \u0001 (βv) = β(e \u0001 v). Cuando dim E < 1, en el caso complejo vemos que todas las ra´ıces complejas del po- linomio caracter´ıstico son reales, porque son valores propios de T ; es decir, si A es una matriz compleja herm´ıtica (y las matrices reales sim´etricas lo son) entonces todas las ra´ıces complejas del polinomio c(x) = jxI \u0000 Aj son reales. Se sigue la existencia alg´un vector propio u 2 E (incluso en el caso real), y despu´es de dividirlo por su m´odulo, podemos suponer que kuk = 1. Ahora, para probar la existencia de una base ortonormal de vectores propios, procedemos por inducci´on sobre n = dim E. Cuando n = 1, tenemos que E = Ku, y u es ya una base ortonormal de E. Cuando n > 1, consideramos la recta Ku y su ortogonal V := (Ku) ⊥, que es de dimensi´on n \u0000 1, y queda invariante por T . En efeto, si v 2 V , entonces tambi´en T (v) 2 V porque T (v) \u0001 u = v \u0001 T (u) = v \u0001 (αu) = α(v \u0001 u) = 0. Luego T induce un operador lineal T jV : V ! V , T jV (v) = T (v), que es herm´ıtico, y por hip´otesis de inducci´on, V admite una base ortonormal u1, . . . , un−1 formada por vectores propios de T jV ; luego tambi´en de T . Los vectores propios u1, . . . , un−1, u son de m´odulo 1 y mutuamente ortogonales, y forman una base de E porque son linealmente independientes: u /2 Ku1 + . . . + Kun−1 = V . 34 Corolario 5.7 Sea fα1, . . . , αrg el espectro de un operador herm´ıtico T : E ! E. Todo vector ψ 2 E descompone, de modo ´unico, en suma ψ = ψα1 + . . . + ψαr , donde ψαi 2 Vαi. Adem´as, ψαi es la proyecci´on ortogonal de ψ sobre Vαi. Demostraci´on: Tal descomposici´on existe y es ´unica porque Vα1 \b . . . \b Vαr = E, al ser T diagonalizable, y la proyecci´on ortogonal de ψ sobre Vα1 es ψα1 porque ψα2 , . . . , ψαr 2 V ⊥ α1 , de acuerdo con el Teorema Espectral. Teorema 5.8 Si dos operadores lineales herm´ıticos S, T de un espacio vectorial eucl´ıdeo E conmutan, ST = T S, entonces E admite una base ortonormal formada por vectores propios de S y de T . Demostraci´on: Sea fα1, . . . , αrg el espectro de T , y Vα1 , . . . , Vαr los correspondientes subes- pacios propios. Pongamos di := dim Vαi. Si vi 2 Vαi, entonces T (vi) = αivi, de modo que T (S(vi)) = S(T (vi)) = S(αivi) = αiS(vi), y vemos que tambi´en S(vi) 2 Vαi. Es decir, S(Vαi) \u0012 Vαi, de modo que S induce por restricci´on un operador herm´ıtico SjV : Vαi ! Vαi. Por el Teorema Espectral, existe una base ortonormal u i 1, . . . , ui di de Vαi formada por vectores propios de S (y tambi´en de T porque est´an en Vαi). Estos vectores propios u 1 1, . . . , u1 d1 , . . . , ur 1, . . . , ur dr son de m´odulo 1, mutuamente ortogo- nales por el Teorema Espectral, y, al ser T es diagonalizable, forman una base de E (ver la nota al pie de la p´agina 33). Ejemplos: 1. En Mec´anica Cu´antica, el espacio de estados de un sistema es un espacio vectorial comple- jo E (generalmente de dimensi´on inﬁnita) con un producto escalar y los distintos estados del sistema son los subespacios vectoriales Cψ de dimensi´on 1, normalmente representados por un vector ψ de m´odulo 1 (esto lo determina salvo un factor eiθ, θ 2 R). Cada magnitud escalar est´a descrita por un operador lineal autoadjunto T : E ! E, y al medir esa magnitud, los ´unicos valores que se pueden obtener son los del espectro de T , que siempre son n´umeros reales por el Teorema Espectral. Si el estado est´a representado por un vector propio de valor propio α, entonces el valor de la medici´on es α; pero si est´a representado por una suma ψ = ψα1 + . . . + ψαr de vectores propios de valores propios distintos α1, . . . , αr (descomposici´on que es ´unica seg´un 5.5, y que en dimensi´on ﬁnita siempre existe por el Teorema Espectral) entonces los posibles valores de la medici´on son α1, . . . , αr, y la probabilidad P(αi) de medir αi es P(αi) = kψαik 2 kψk2 = hψαijψαii hψjψi \u0001 Si el resultado de la medici´on es αi, el estado del sistema pasa a ser ψαi, la proyecci´on ortogonal del vector ψ sobre el subespacio propio Vαi. (Aunque el representante ψ est´e normalizado, kψk = 1, el representante ψαi del nuevo estado no, cuando P(αi) 6= 1). El teorema de Pit´agoras, kψk 2 = P i kψαik 2, aﬁrma que 1 = P(α1) + . . . + P(αr). En particular, en ese estado el valor medio de las observaciones es rP i=1αiP(αi) = rP i=1 hψαijαiψαii hψjψi = h P i ψαij P i αiψαii hψjψi = hψjT (ψ)i hψjψi = hT (ψ)jψi hψjψi \u0001 35 2. En el caso de una part´ıcula puntual de masa m, su estado est´a determinado (ﬁjado el origen del espacio, la unidad de longitud y una base ortonormal) por una funci´on compleja ψ : R3 ! C de 3 variables reales, llamada funci´on de onda, que se anula fuera de la regi´on donde est´a conﬁnada la part´ıcula. El producto escalar de funciones que se considera es hψjϕi = Z R3 ψ(x, y, z)ϕ(x, y, z) dxdydz , y la probabilidad de encontrar a la part´ıcula en cierta regi´on Ω es Z Ω jψ(x, y, z)j 2dxdydz Z R3 jψ(x, y, z)j 2dxdydz \u0001 Las coordenadas de la posici´on de la part´ıcula vienen dadas por los operadores lineales X(ψ) = xψ, Y (ψ) = yψ, Z(ψ) = zψ (herm´ıticos porque xψ = x ¯ψ, yψ = y ¯ψ, zψ = z ¯ψ) y los momentos en las direcciones de los ejes vienen dados por los operadores lineales Px := ~ i ∂ ∂x = \u0000i~ ∂ ∂x , Py := ~ i ∂ ∂y = \u0000i~ ∂ ∂y , Pz := ~ i ∂ ∂z = \u0000i~ ∂ ∂z que son herm´ıticos de acuerdo con la f´ormula de integraci´on por partes: hPxψjϕi = Z R3 i~ ∂ ¯ψ ∂x ϕ dxdydz = Z R3 ∂(i~ ¯ψϕ) ∂x dxdydz \u0000 Z R3 ¯ψi~ ∂ϕ ∂x dxdydz = 0 + hψjPxϕi donde el primer sumando es nulo seg´un la regla de Barrow, pues las funciones son nulas fuera de cierta regi´on. La energ´ıa cin´etica viene dada por el operador lineal herm´ıtico 1 2m (P 2 x + P 2 y + P 2 z ) = \u0000 ~ 2 2m \u0012 ∂2 ∂x2 + ∂2 ∂y2 + ∂2 ∂z2 \u0013 y en presencia de fuerzas que se deriven de un potencial V (x, y, z) (que es una funci´on con valores reales, de modo que el operador lineal ψ 7! V ψ es autoadjunto) la energ´ıa viene dada por el operador lineal herm´ıtico H = V (x, y, z) \u0000 ~2 2m \u0012 ∂2 ∂x2 + ∂2 ∂y2 + ∂2 ∂z2 \u0013 . La evoluci´on temporal del estado de la part´ıcula viene dada por ciertos operadores lineales Ut, t 2 R, donde Ut \u000e Us = Ut+s y U0 = Id; y se supone que la funci´on compleja Ψ(t, x, y, z) := (Utψ)(x, y, z) cumple la ecuaci´on de Schr¨odinger (1886-1961) ∂Ψ ∂t = 1 i~ H(Ψ) = \u0000 i ~ H(Ψ). Los operadores lineales Ut son unitarios, hUt(ψ)jUt(ϕ)i = hU0(ψ)jU0(ϕ)i = hψjϕi, porque d dt hUtψjUtϕi = h ∂(Utψ) ∂t jUtϕi + hUtψj ∂(Utϕ) ∂t i = i ~ hH(Utψ)jUtϕi \u0000 i ~ hUtψjH(Utϕ)i = 0. En particular, si kψk = 1, se cumple que kUtψk = 1 en todo instante t. 36 En adelante E denotar´a un K-espacio vectorial de dimensi´on ﬁnita n (y K = R ´o C). 6. El Espacio Dual Deﬁnici´on: El espacio dual de un K-espacio vectorial E es el K-espacio vectorial E∗ formado (teorema 3.2) por todas las aplicaciones K-lineales E ! K, aplicaciones que reciben el nombre de formas lineales sobre E. Teorema 6.1 Sean e1, . . . , en una base de E. Dados n escalares a1, . . . , an 2 K, existe una ´unica forma lineal ω 2 E∗ tal que ω(e1) = a1, . . . , ω(en) = an, y es ω(x1, . . . , xn) := ω(x1e1 + . . . + xnen) = a1x1 + . . . + anxn. Demostraci´on: Es un caso particular de 3.1, cuando E′ = K. Teorema 6.2 Fijada una base e1, . . . , en en E, existe una ´unica base ω1, . . . , ωn de E∗, llamada base dual, tal que ωi(ej) = δij, y las coordenadas de cualquier forma lineal ω 2 E∗ en esta base dual son (ω(e1), . . . , ω(en)). En particular, el dual E∗ es un K-espacio vectorial de dimensi´on dim E∗ = dim E. Demostraci´on: Seg´un 6.1, existen formas lineales ω1, . . . , ωn tales que ωi(ej) = δij, y son las formas lineales ωi(x1e1 + . . . + xnen) = xi, por lo que a menudo pondremos ωi = dxi, pues su valor en un vector e = q \u0000 p es la diferencia entre las i-´esimas coordenadas de p y q. Veamos que dx1, . . . , dxn son linealmente independientes: Si P i λidxi = 0, para todo ´ındice j = 1, . . . , n tenemos que 0 = \u0000P i λidxi\u0001(ej) = P i λi\u0000dxi(ej) \u0001 = P i λiδij = λj. Veamos ahora que dx1, . . . , dxn generan el espacio dual E∗: Dada una forma lineal ω 2 E∗, para todo vector e = P i xiei 2 E se cumple que ω(e) = ω(x1e1 + . . . + xnen) = x1ω(e1) + . . . + xnω(en), ω = ω(e1)dx1 + . . . + ω(en)dxn. Proposici´on 6.3 Sean e1, . . . , en y v1, . . . , vn dos bases de E, y sean dx1, . . . , dxn y dy1, . . . , dyn sus respectivas bases duales. Si B = (bij) es la matriz de cambio de base de e1, . . . , en a v1, . . . , vn, entonces Bt es la matriz de cambio de base de dy1, . . . , dyn a dx1, . . . , dxn. Demostraci´on: La i-´esima coordenada del vector vj en la base e1, . . . , en es bij = dxi(vj). Sea C = (cij) la matriz de cambio de base de dy1, . . . , dyn a dx1, . . . , dxn. La i-´esima coordenada de la forma lineal dxj en la base dy1, . . . , dyn es cij = dxj(vi) = bji. 0 B @ v1 ... vn 1 C A = Bt 0 B @ e1 ... en 1 C A , 0 B @ dx1 ... dxn 1 C A = B 0 B @ dy1 ... dyn 1 C A . Deﬁnici´on: Sea E un espacio vectorial eucl´ıdeo. El bra de un vector e 2 E es la aplicaci´on hej : E \u0000! K que asigna a cada vector v 2 E el escalar hejvi. Esta aplicaci´on es lineal, porque el producto escalar es lineal por la derecha, y la base dual de una base ortonormal u1, . . . , un es justamente hu1j, . . . , hunj; porque huijuji = δij. 37 Teorema 6.4 Sea E un espacio vectorial eucl´ıdeo. La aplicaci´on natural ϕ : E \u0000! E∗ , ϕ(e) = hej, es un isomorﬁsmo antilineal. Demostraci´on: Es antilineal porque he + e ′j = hej + he′j y hλej = ¯λhej. Es inyectiva: Si e 2 Ker ϕ, entonces 0 = ϕ(e) = hej. Luego 0 = hejei, y se sigue que e = 0. Es decir, Ker ϕ = 0, y concluimos que ϕ es inyectiva (1.4). Ahora, por el Teorema de Isomorf´ıa (que es v´alido para aplicaciones antilineales, y con la misma demostraci´on que hemos dado), dim (Im ϕ) = dim E = dim E∗; luego Im ϕ = E∗ y vemos que ϕ es epiyectiva. Otra manera de ver que ϕ es epiyectiva es ﬁjar una base ortonormal u1, . . . , un en E y observar que toda forma lineal ω 2 E∗ es ω = P i λihuij = P i λiϕ(ui) = ϕ( P i ¯λiui). Ejemplos: 1. Dada una funci´on diferenciable f : Rn ! R y un punto p 2 Rn, para cada vector e 2 Rn podemos considerar la derivada en t = 0 de la funci´on h : R ! R, h(t) = f (p + te), que se obtiene al restringir f a la recta que pasa por p con direcci´on Re (df )(e) = 2 4 Derivada de f en el punto p en la direcci´on e 3 5 := l´ım t→0 f (p + te) \u0000 f (p) t de modo que, para valores peque˜nos ε de t se cumple que f (p + εe) \u0000 f (p) ε ˇ k , f (p + εe) ˇ f (p) + kε ; k := (df )(e). As´ı, en el punto p la funci´on f es creciente (resp. decreciente) en la direcci´on e cuando (df )(e) > 0 (resp. (df )(e) < 0), y los m´aximos y m´ınimos locales de f son puntos cr´ıticos, en el sentido de que (df )(e) = 0 para todo vector e 2 Rn. Cuando el punto p no es cr´ıtico y (df )(e) = 0, se dice que en el punto p el vector e es tangente a la hipersuperﬁcie de nivel f (x1, . . . , xn) = f (p) que pasa por p. Tenemos as´ı una forma lineal df : Rn ! R, llamada diferencial de la funci´on f en el punto p 2 Rn, que da el incremento inﬁnitesimal de f en p en cualquier direcci´on e 2 Rn. Por deﬁnici´on ∂f ∂xi (a1, . . . , an) es la derivada de la funci´on f (a1, . . . , ai + t, . . . , an) en t = 0; luego ∂f ∂xi (p) = (df )(ei), donde e1, . . . , en es la base usual de Rn. Seg´un 6.2, en la base dual dx1, . . . , dxn se cumple que df = P i ∂f ∂xi (p)dxi: df = ∂f ∂x1 dx1 + . . . + ∂f ∂xn dxn. 2. Cuando en Rn se considera el producto escalar usual, de acuerdo con 6.4, existe un ´unico vector grad f 2 Rn, llamado gradiente de f en p, tal que ϕ(grad f ) = df ; i.e. hgrad f jei = (df )(e) , grad f = \u0000 ∂f ∂x1 , . . . , ∂f ∂xn \u0001. El punto es cr´ıtico cuando grad f = 0, y cuando el gradiente no es nulo, la direcci´on del hiperplano tangente en p a la hipersuperﬁcie de nivel f (x1, . . . , xn) = f (p) es el ortogonal hgrad f i ⊥, la direcci´on de la recta normal en p es hgrad f i, y el gradiente indica la direcci´on (en p) de m´aximo crecimiento de la funci´on f . 38 3. En F´ısica, An´alisis y Geometr´ıa, casi nunca se considera un vector o una forma lineal aislada, sino campos de vectores o campos de formas lineales; es decir, en cada punto p 2 E se considera un vector e 2 E o una forma lineal ω 2 E∗, que depende del punto p considerado. As´ı, en Mec´anica Cl´asica (ﬁjado el origen de coordenadas y la unidad de longitud) el espacio es un espacio vectorial eucl´ıdeo real de dimensi´on 3, y ﬁjada una base ortonormal ⃗i,⃗j, ⃗k, se identiﬁca con R3. La fuerza viene dada por un campo de vectores ⃗F = F1(x, y, z)⃗i + F2(x, y, z)⃗j + F3(x, y, z)⃗k, y el trabajo es el campo de formas lineales ω ⃗F = F1(x, y, z)dx + F2(x, y, z)dy + F3(x, y, z)dz. 4. Notaci´on de Dirac: Sea E un espacio vectorial eucl´ıdeo. El bra de un vector e 2 E es la forma lineal hej, mientras que el ket jei es el propio vector e, y dos vectores e, v 2 E deﬁnen un operador lineal jeihvj : E ! E, (jeihvj)(u) = jeihvjui := hvjuie. 6.1. Incidencia y Bidualidad Cada vector e 2 E deﬁne una aplicaci´on lineal ψe : E∗ ! K, ψe(ω) = ω(e), as´ı que tenemos una aplicaci´on natural Ψ : E \u0000! E∗∗ , Ψ(e) = ψe. Es decir, Ψ(e)(ω) = ω(e), 8e 2 E, ω 2 E∗. Teorema de Bidualidad: La aplicaci´on natural Ψ : E ! E∗∗, Ψ(e)(ω) = ω(e), es un isomorﬁsmo lineal. Demostraci´on: Veamos primero que la aplicaci´on Ψ es lineal. Si e, v 2 E y λ 2 K, se cumple que ψe+v = ψe + ψv y ψλe = λψe porque, para toda forma lineal ω 2 E∗, ψe+v(ω) = ω(e + v) = ω(e) + ω(v) = ψe(ω) + ψv(ω) = (ψe + ψv)(ω), ψλe(ω) = ω(λe) = λω(e) = λ \u0000ψe(ω) \u0001 = \u0000λψe\u0001(ω). Veamos ahora que Ker Ψ = 0. Si Ψ(e) = 0, entonces ω(e) = 0 para toda forma lineal ω 2 E∗, y de 6.2 se sigue que, en cualquier base de E, todas las coordenadas del vector e son nulas; luego e = 0. Por ´ultimo, del Teorema de Isomorf´ıa se sigue que dim (Im Ψ) = dim E = dim E∗∗. Luego Im Ψ = E∗∗, y Ψ es epiyectiva. Corolario 6.5 Cada base ω1, . . . , ωn de E∗ es la base dual de una ´unica base de E. Demostraci´on: Existe una ´unica base e1, . . . en de E tal que Ψ(e1), . . . , Ψ(en) es la base dual de ω1, . . . , ωn; es decir, tal que δij = Ψ(ei)(ωj) = ωj(ei). Deﬁnici´on: El incidente de un subespacio vectorial V de E es V o = fω 2 E∗ : ω(v) = 0, 8v 2 V g, y el incidente de un subespacio vectorial W de E∗, identiﬁcando E∗∗ con E, es W o = fψe 2 E∗∗ : ψe(ω) = 0, 8ω 2 W g = fe 2 E : ω(e) = 0, 8ω 2 W g. Ejemplo: Cuando E es un espacio vectorial eucl´ıdeo, tenemos un isomorﬁsmo antilineal ϕ : E ! E∗, ϕ(e) = hej, y la condici´on ϕ(e) 2 V o signiﬁca que e 2 V ⊥; i.e., V o = ϕ(V ⊥). Teorema 6.6 El incidente es un subespacio vectorial de dimensi´on dim V o = dim E \u0000 dim V. 39 Demostraci´on: Ampliemos una base e1, . . . , ed de V hasta obtener una base e1, . . . , ed, . . . , en de E, y sea dx1, . . . , dxn su base dual. Una forma lineal ω = P i λidxi est´a en V o cuando λ1 = ω(e1) = 0, . . . , λd = ω(ed) = 0; es decir, cuando ω = λd+1dxd+1 + . . . + λndxn. Luego V o = hdxd+1, . . . , dxni, que es un subespacio vectorial de E∗, de dimensi´on n \u0000 d porque dxd+1, . . . , dxn son linealmente independientes (forman parte de una base de E∗). Corolario 6.7 (V o) o = V . Demostraci´on: (V o)o = fe 2 E : ω(e) = 0, 8ω 2 V og \u0013 V , y V = (V o)o porque dim (V o) o = dim E∗ \u0000 dim V o = dim E \u0000 (dim E \u0000 dim V ) = dim V. Corolario 6.8 Si V y W son subespacios vectoriales de E, se cumple que: 1. V \u0012 W , W o \u0012 V o , y en particular V = W , V o = W o 2. (V + W ) o = V o \\ W o 3. (V \\ W ) o = V o + W o Demostraci´on: Si V \u0012 W , es claro que W o \u0012 V o. Rec´ıprocamente, si W o \u0012 V o, entonces V = (V o) o \u0012 (W o)o = W . 2.– Como V \u0012 V + W y W \u0012 V + W , tenemos que (V + W )o \u0012 V o y (V + W )o \u0012 W o; luego (V + W )o \u0012 V o \\ W o. Adem´as, si ω 2 V o \\ W o, entonces ω(v + w) = ω(v) + ω(w) = 0 para todo vector v + w 2 V + W . Luego V o \\ W o \u0012 (V + W ) o y concluimos que (V + W ) o = V o \\ W o. 3.– Por el primer apartado, para demostrar que (V \\ W )o = V o + W o basta ver que sus incidentes coinciden: \u0000V o + W o\u0001o 3 = (V o) o \\ (W o) o = V \\ W = \u0000(V \\ W ) o\u0001o. 6.2. Adjunto de un Operador Lineal Deﬁnici´on: Sea T : E ! F una aplicaci´on lineal entre espacios vectoriales eucl´ıdeos. Para cada vector v 2 F tenemos una aplicaci´on ηv : E ! K, ηv(e) = hvjT (e)i, que es lineal ηv(λe+µe′) = hvjT (λe+µe ′)i = hvjλT (e)+µT (e ′)i = λhvjT (e)i+µhvjT (e′)i = ληv(e)+µηv(e′). Como el producto escalar deﬁne (6.4) un isomorﬁsmo ϕ : E ! E∗, ϕ(e) = hej, existe un ´unico vector T †(v) 2 E tal que ηv = hT †(v)j; es decir, hvjT (e)i = hT †(v)jei 8e, v 2 E. Como hT (e)jvi = hvjT (e)i y hejT †(v)i = hT †(v)jei, tambi´en tenemos que hT (e)jvi = hejT †(v)i 8e, v 2 E, y en particular Id † = Id y (T †) † = T . Esta aplicaci´on T † : F ! E es lineal, y recibe el nombre de adjunta 3 de T . En efecto, para probar que T †(λv + µv′) = λT †(v) + µT †(v′), seg´un 6.4 basta ver que ambos vectores tienen el mismo producto escalar con cualquier vector e 2 E, y hT †(λv + µv′)jei = hλv + µv′jT (e)i = ¯λhvjT (e)i + ¯µhv′jT (e)i = ¯λhT †(v)jei + ¯µhT †(v′)jei = hλT †(v) + µT †(v′)jei. 3Un endomorﬁsmo T : E → E es autoadjunto cuando ⟨T (e)|v⟩ = ⟨e|T (v)⟩, ∀e, v ∈ E; es decir, T = T †. 40 Proposici´on 6.9 Sea A la matriz de T en ciertas bases ortonormales u1, . . . , un de E y v1, . . . , vm de F . La matriz de T † en tales bases ortonormales es ¯At. Demostraci´on: Sean A = (aij) y (cij) las matrices de T y T † respectivamente; es decir, T (uj) = P i aijvi , T †(vj) = P i cijui. Como las bases duales de u1, . . . , un y v1, . . . , vm son sus respectivos bras, cij = huijT †(vj)i = hT (ui)jvji = hvjjT (ui)i = ¯aji. Teorema 6.10 Ker T † = (Im T )⊥ , Im T † = (Ker T ) ⊥. Demostraci´on: Si v 2 F , la condici´on T †(v) = 0 signiﬁca que 0 = hT †(v)jei = hvjT (e)i para todo vector e 2 E; es decir, que v 2 (Im T )⊥. Ahora Ker T = Ker (T †) † = (Im T †)⊥, y (Ker T ) ⊥ = \u0000(Im T †) ⊥\u0001⊥ = Im T †. Corolario 6.11 dim (Im T †) = dim (Im T ). Demostraci´on: dim (Im T †) = dim (Ker T ) ⊥ = dim E \u0000 dim (Ker T ) = dim Im T . Proposici´on 6.12 Un subespacio vectorial V de E queda invariante por un endomorﬁsmo T : E ! E, i.e. T (V ) \u0012 V , si y s´olo si su ortogonal V ⊥ queda invariante por T †. Demostraci´on: Supongamos que T (V ) \u0012 V . Si w 2 V ⊥, se cumple que hT †(w)jvi = hwjT (v)i = 0 para todo vector v 2 V , porque T (v) 2 V y w 2 V ⊥. Luego T †(w) 2 V ⊥, y vemos que T †(V ⊥) \u0012 V ⊥. Rec´ıprocamente, si T †(V ⊥) \u0012 V ⊥, entonces (T †) †\u0000(V ⊥)⊥\u0001 \u0012 (V ⊥) ⊥ seg´un acabamos de ver, y concluimos que T (V ) \u0012 V porque (T †) † = T y (V ⊥) ⊥ = V . Nota: La condici´on de que una recta Ke quede invariante por un endomorﬁsmo T signiﬁca que T (e) 2 Ke; es decir, que e es un vector propio de T . Por tanto, de acuerdo con 6.12 los subespacios vectoriales de dimensi´on n \u0000 1 invariantes por T son los ortogonales de las rectas generadas por un vector propio de T †. 41 7. Formas Cuadr´aticas Deﬁnici´on: Llamaremos forma cuadr´atica a toda aplicaci´on q : E ! K que, en el sistema de coordenadas (x1, . . . , xn) deﬁnido por una base e1, . . . , en de E, venga dada por un polinomio homog´eneo de grado 2: q(x1e1+ . . . + xnen) = P 1≤i≤j≤n cijxixj. q = P 1≤i≤j≤n cijdxidxj. 7.1. M´etricas Deﬁnici´on: Dada una aplicaci´on S : E \u0002 E ! K, pondremos e \u0001 v := S(e, v), y diremos que S es una m´etrica si es bilineal y sim´etrica: 1. (e + e′) \u0001 v = e \u0001 v + e′ \u0001 v , (λe) \u0001 v = λ(e \u0001 v) ; 8e, e′, v 2 E, λ 2 K. v \u0001 (e + e′) = v \u0001 e + v \u0001 e′ , v \u0001 (λe) = λ(v \u0001 e) ; 8e, e′, v 2 E, λ 2 K. 2. e \u0001 v = v \u0001 e , 8e, v 2 E. Matriz de una M´etrica: Fijada una base e1, . . . , en de E, la matriz de una m´etrica S es la matriz A = (aij), donde aij := ei \u0001 ej. Es una matriz sim´etrica de n ﬁlas y columnas. Fijada la base de E, esta matriz determina totalmente a la m´etrica S, porque el producto e \u0001 v del vector e 2 E de coordenadas X t = (x1, . . . , xn) con el vector v de coordenadas Y t = (y1, . . . , yn) es e \u0001 v = nX i=1 xiei ! \u0001 0 @ nX j=1 yjej 1 A = nX i,j=1 aijxiyj = (x1 . . . xn) 0 @a11 . . . a1n . . . . . . . . . an1 . . . ann 1 A 0 B @ y1 ... yn 1 C A e \u0001 v = X tAY. Adem´as, cualquier matriz sim´etrica A de n ﬁlas y columnas deﬁne una m´etrica mediante la igualdad anterior. Forma Cuadr´atica Asociada a una M´etrica: Cuando los vectores e y v coinciden, vemos que la funci´on q : E ! K, q(e) = e \u0001 e = S(e, e), es una forma cuadr´atica q(x1e1 + . . . + xnen) = X tAX = nP i=1 aiix2 i + P 1≤i<j≤n 2aijxixj, (12) llamada forma cuadr´atica asociada a S. Determina totalmente a la m´etrica porque 1 2 \u0000q(e + v) \u0000 q(e) \u0000 q(v)\u0001 = 1 2 (e \u0001 e + e \u0001 v + v \u0001 e + v \u0001 v \u0000 e \u0001 e \u0000 v \u0001 v) = e \u0001 v, y (12) muestra que toda forma cuadr´atica q : E ! K est´a asociada a una m´etrica. En el caso real (K = R), cuando q(e) > 0 ha de entenderse que +p q(e) = +pe \u0001 e 2 R es la medida del vector e con la m´etrica S (o con su forma cuadr´atica asociada q). Ejemplos: 1. La aplicaci´on nula S : E \u0002 E ! K, S(e, v) = 0, es una m´etrica. Su matriz en cualquier base es la matriz 0 y la forma cuadr´atica asociada q : E ! K es nula: q(e) = 0, 8e 2 E. 2. En el caso real (¡y no en el complejo!) los productos escalares son las m´etricas en que la forma cuadr´atica asociada es deﬁnido-positiva (i.e. positiva en los vectores no nulos), y una base es ortonormal justo cuando la matriz de la m´etrica es la matriz unidad In, es decir, cuando la forma cuadr´atica asociada es dx2 1 + . . . + dx2 n. 42 3. Dada una forma cuadr´atica q : E ! K, su m´etrica asociada S : E \u0002 E ! K induce una m´etrica SjV : V \u0002 V ! K, SjV (v, v′) = S(v, v′), en cada subespacio vectorial V \u0012 E, y la forma cuadr´atica asociada es la restricci´on qjV : V ! K, qjV (v) = q(v), de q a V . Cambio de Base en la Matriz de una M´etrica: Si ˜A = (˜aij) es la matriz de S en otra base ˜e1, . . . , ˜en de E, y B = (bij) es la matriz de cambio de base, las coordenadas ˜X, ˜Y de los vectores e y v en la nueva base cumplen que X = B ˜X, Y = B ˜Y . Luego ˜X t ˜A ˜Y = e \u0001 v = X tAY = ˜X tBtAB ˜Y , y la matriz de S en la nueva base es ˜A = BtAB. Lema 7.1 En el caso real, si el determinante jAj es positivo (resp. negativo, nulo), tambi´en lo es j ˜Aj. Es decir, el signo de jAj no depende de la base, s´olo de la m´etrica. Demostraci´on: Tenemos que j ˜Aj = jBtABj = jBj 2jAj, y jBj 6= 0 porque B es invertible. Teorema 7.2 Sea S una m´etrica sobre un espacio vectorial real E, y sea A = (aij) su matriz en una base e1, . . . , en de E. La m´etrica S es un producto escalar si y s´olo si son positivos todos los menores principales de la matriz A a11 , \f \f \f \fa11 a12 a21 a22 \f \f \f \f , \f \f \f \f \f \f a11 a12 a13 a21 a22 a23 a31 a32 a33 \f \f \f \f \f \f , . . . , \f \f \f \f \f \f a11 . . . a1r . . . . . . . . . ar1 . . . arr \f \f \f \f \f \f , . . . , \f \f \f \f \f \f \f \f a11 . . . a1n . . . . . . . . . . . . . . . . . . an1 . . . ann \f \f \f \f \f \f \f \f = jAj. Demostraci´on: Seg´un el lema, el determinante de la matriz de un producto escalar siempre es positivo, porque lo es en las bases ortonormales. Luego, si S es un producto escalar, todo menor principal es positivo, pues es el determi- nante de la matriz de S en la base e1, . . . , er del espacio vectorial Re1 + . . . + Rer. Rec´ıprocamente, si todos los menores principales son positivos, procediendo por induc- ci´on sobre la dimensi´on n, podemos suponer que S es deﬁnido-positiva en el hiperplano H = Re1 + . . . + Ren−1, que por tanto admite alguna base ortonormal u1, . . . , un−1. Existe un vector no nulo e 2 E tal que u1 \u0001 e = . . . = un−1 \u0001 e = 0, porque la aplicaci´on lineal f : E ! Rn−1, f (e) = (u1 \u0001 e, . . . , un−1 \u0001 e) no puede ser inyectiva. Pero e /2 H, porque e \u0001 H = 0 y S es deﬁnido-positiva en H; luego u1, . . . , un−1, e es una base de E. La matriz de S en esta base es diagonal diag(1, . . . , 1, a), de modo que la forma cuadr´atica es q(x1, . . . , xn) = x2 1 + . . . + x2 n−1 + ax2 n. Por el lema, el signo de a coincide con el de jAj > 0. Luego a > 0, y la forma cuadr´atica dx2 1 + . . . + dx2 n−1 + adx2 n es deﬁnido-positiva. 7.2. Clasiﬁcaci´on de M´etricas Deﬁnici´on: Dada una m´etrica S en un K-espacio vectorial E, diremos que un vector e 2 E es is´otropo si q(e) = e \u0001 e = 0. Si todos los vectores son is´otropos, entonces q = 0, y por tanto S = 0. Lema 7.3 Si e 2 E no es is´otropo, entonces su ortogonal (Ke) ⊥ := fv 2 E : e \u0001 v = 0g es un subespacio vectorial de dimensi´on n \u0000 1 y E = Ke \b (Ke)⊥. Demostraci´on: Consideremos la forma lineal ω : E ! K, ω(v) = e \u0001 v, que no es nula porque ω(e) = e \u0001 e 6= 0; luego su imagen es de dimensi´on 1 y, seg´un el Teorema de Isomorf´ıa, su n´ucleo (Ke) ⊥ es un subespacio vectorial de dimensi´on n \u0000 1. La suma Ke + (Ke) ⊥ es directa porque si λe 2 (Ke)⊥, entonces λ(e \u0001 e) = 0, y λ = 0. Como dim (Ke \b (Ke)⊥) = 1 + (n \u0000 1) = dim E, concluimos que Ke \b (Ke)⊥ = E. 43 Teorema 7.4 Toda m´etrica S admite una base e1, . . . , en en que su matriz es diagonal: ei \u0001 ej = 0 cuando i 6= j. Demostraci´on: Por inducci´on sobre n = dim E, y es obvio cuando n = 1, porque toda matriz 1 \u0002 1 es diagonal. Cuando n > 1, es obvio si q = 0. Si q 6= 0, consideramos un vector no is´otropo e1 2 E. Como dim (Ke1) ⊥ = n \u0000 1, por hip´otesis de inducci´on existe alguna base e2, . . . , en de (Ke1) ⊥ en que la matriz de S es diagonal. Ahora los vectores e1, e2, . . . , en cumplen que e1 \u0001 ej = 0 cuando 1 < j \u0014 n, porque e1 \u0001 e2 = . . . = e1 \u0001 en = 0, y forman una base de E porque lo generan: Ke1 + Ke2 + . . . + Ken = Ke1 + (Ke1) ⊥ = E. Deﬁnici´on: Dada una m´etrica S : E \u0002 E ! K, cada vector e 2 E deﬁne una aplicaci´on ωe : E ! K, ωe(v) = e \u0001 v = S(e, v), que es lineal porque S es lineal a la derecha. Obtenemos as´ı una aplicaci´on natural ϕ : E ! E∗, ϕ(e) = ωe, la polaridad asociada a la m´etrica S (o a su forma cuadr´atica), que es lineal porque S es lineal a la izquierda: ωe+e′(v) = (e + e′) \u0001 v = e \u0001 v + e′ \u0001 v = ωe(v) + ωe′(v) = (ωe + ωe′)(v), ωλe(v) = (λe) \u0001 v = λ(e \u0001 v) = λωe(v) = (λωe)(v), Si A = (aij) es la matriz de S en una base e1, . . . , en de E, y consideramos la base dual dx1, . . . , dxn en E∗, entonces la matriz de ϕ en estas bases tambi´en es A porque, seg´un 6.2, la i-´esima coordenada de ϕ(ej) en la base dual es ϕ(ej)(ei) = ωej (ei) = ej \u0001 ei = aij. El rango de la m´etrica (o de la forma cuadr´atica asociada) es rg S := dim (Im ϕ). De acuerdo con 3.5, el rango de una m´etrica coincide con el rango de su matriz en cualquier base: rg S = rg A. La polaridad es un isomorﬁsmo cuando la m´etrica es de rango m´aximo, rg S = dim E. Teorema 7.5 En el caso complejo, toda m´etrica de rango r admite una base e1, . . . , en en que su matriz es diag(1, r. . ., 1, 0, . . . , 0), y su forma cuadr´atica es dx2 1 + . . . + dx 2 r. Demostraci´on: Seg´un el lema anterior, en alguna base v1, . . . , vn la matriz de la m´etrica es diag(a1, . . . , am, 0, . . . 0), donde los coeﬁcientes ai = vi \u0001 vi no son nulos; luego m = r. Sustituyendo vi por ei = vi/ pai, cuando i = 1, . . . , r, tendremos que ei \u0001 ei = 1, y la matriz de la m´etrica en la base e1, . . . , er, vr+1, . . . , vn es diag(1, r. . ., 1, 0, . . . , 0). Teorema de Inercia de Sylvester (1814-1897): En el caso real, toda m´etrica S de rango r admite una base e1, . . . , en en que su matriz es diag(1, p. . ., 1, \u00001, q. . ., \u00001, 0, . . . , 0), con p + q = r, y su forma cuadr´atica es dx2 1 + . . . + dx2 p \u0000 dx2 p+1 \u0000 . . . \u0000 dx2 p+q. El par (p, q) no depende de la base elegida, y decimos que la m´etrica S (o su forma cuadr´atica) es de signatura (p, q) o de tipo (+, p. . ., +, \u0000, q. . ., \u0000, 0, . . . , 0). Demostraci´on: En alguna base v1, . . . , vn la matriz de la m´etrica es diag(a1, . . . , am, 0, . . . 0), donde los coeﬁcientes ai = vi \u0001 vi no son nulos; luego m = r, y sustituyendo vi por el vector ei = vi/p jaij, cuando i = 1, . . . , r, tenemos que ei \u0001 ei = \u00061, y la matriz de S en la base e1, . . . , er, er+1 = vr+1, . . . , en = vn, despu´es de reordenarla, tiene la forma deseada. Por ´ultimo, para ver que el n´umero p (y por tanto q = r \u0000 p) no depende de la base elegida, basta probar que p coincide con el m´aximo p ′ de las dimensiones de los subespacios vectoriales V+ \u0012 E en que la m´etrica S es deﬁnido-positiva: Si en una base la matriz de S es diag(1, p. . ., 1, \u00001, q. . ., \u00001, 0, . . . , 0), la m´etrica es deﬁnido- positiva en Re1 + . . . + Rep, as´ı que p \u0014 p′. Adem´as, en W = Rep+1 + . . . + Ren tenemos que e \u0001 e \u0014 0, 8e 2 W ; luego W \\ V+ = 0, y n \u0000 p + dim V+ = dim W + dim V+ = dim (W + V+) \u0014 dim E = n. Luego dim V+ \u0014 p; de modo que p′ \u0014 p, y concluimos que p = p′. 44 Corolario 7.6 La signatura de una m´etrica S : E \u0002 E ! R es (p, q), donde p es el m´aximo de las dimensiones de los subespacios vectoriales de E en que S es deﬁnido-positiva, y q es el m´aximo de las dimensiones de los subespacios vectoriales de E en que S es deﬁnido-negativa. Demostraci´on: Sabemos que p es el m´aximo de las dimensiones de los subespacios vectoriales en que S es deﬁnido-positiva. Adem´as, la signatura de \u0000S es (q, p), y \u0000S es deﬁnido-positiva en los subespacios vectoriales en que S es deﬁnido-negativa. Mec´anica Relativista La hip´otesis fundamental de la Teor´ıa de la Relatividad es que los sucesos forman un espacio de dimensi´on 4, y que en cada suceso las trayectorias de la luz en el vac´ıo deﬁnen un cono, dado por los vectores donde se anula una forma cuadr´atica de signatura (1,3), pues en nuestro sistema de coordenadas es dt2 \u0000 1 c2 (dx2 + dy2 + dz2). En general el cono variar´a con el lugar y momento en que se emite la luz (Teor´ıa de la Relatividad General); pero en regiones peque˜nas es razonable suponer que el cono de luz es constante (Teor´ıa de la Relatividad Especial): Fijado un origen del espacio-tiempo, y unidades de longitud y tiempo (y por tanto la velocidad de la luz c > 0), los sucesos forman un espacio-tiempo de Minkowski (1864- 1909) (un espacio vectorial real E de dimensi´on 4, con una m´etrica g de tipo (+, \u0000, \u0000, \u0000), llamada m´etrica de Lorentz (1853-1928).) Las trayectorias de la luz son las rectas p + Re de direcci´on generada por un vector e de tipo luz, i.e. is´otropo, e \u0001 e := g(e, e) = 0. La otra hip´otesis fundamental de la Teor´ıa de la Relatividad Especial es que los sistemas de referencia inerciales son indistinguibles, que la ecuaci´on del cono de luz ha de ser la misma en todos ellos. Un observador inercial o sistema de referencia inercial (con el mismo origen) es una base e0, e1, e2, e3 de E en que la matriz (gij) de g es4 (gij) = 0 B B @ 1 0 0 0 0 \u0000 1 c2 0 0 0 0 \u0000 1 c2 0 0 0 0 \u0000 1 c2 1 C C A , 8 >< >: g00 = 1 g11 = g22 = g33 = \u0000c−2 gij = 0 cuando i 6= j e \u0001 e = t 2 \u0000 c−2(x2 + y2 + z2) , e = te0 + xe1 + ye2 + ze3, y se entiende que las correspondientes coordenadas (t, x, y, z) de un suceso son el tiempo y la posici´on medidos por el observador inercial, as´ı que Re1, Re2, Re3 son las direcciones de los tres ejes espaciales ﬁjados por el observador, Re0 es la direcci´on de las trayectorias de los objetos en reposo relativo para el observador y e0 es la velocidad 4-dimensional del observador inercial: su desplazamiento espacio-temporal por unidad de tiempo. La m´etrica g recibe el nombre de m´etrica del tiempo porque un reloj que se mueve inercialmente de p a q = p + e = p + te0 mide un intervalo de tiempo jtj = pe \u0001 e. El intervalo de tiempo observado entre dos sucesos p, q = p + e, es e0 \u0001 e, y los sucesos son simult´aneos cuando e = q\u0000p es ortogonal a e0: los vectores espaciales o de simultaneidad para un observador inercial e0, e1, e2, e3 son los vectores de (Re0) ⊥ = Re1 + Re2 + Re3, y su geometr´ıa viene dada por la forma cuadr´atica dx2 + dy2 + dz2, que es la restricci´on de la forma cuadr´atica dx2 + dy2 + dz2 \u0000 c 2dt2 asociada a la m´etrica h := \u0000c2g, y los vectores e1, e2, e3 forman una base ortonormal del espacio vectorial eucl´ıdeo \u0000(Re0) ⊥, h\u0001. La m´etrica h recibe el nombre de m´etrica espacial5 porque mide los conceptos espaciales (distancias, ´angulos, etc.) entre sucesos simult´aneos para un observador inercial. 4Adem´as, para cualquier par de observadores inerciales e0, . . . y e′ 0, . . . debe cumplirse que e0 · e′ 0 > 0. 5El cono de luz es absoluto; pero su ecuaci´on, la forma cuadr´atica que lo deﬁne, s´olo est´a bien deﬁnida salvo un factor no nulo. Con todo rigor, en un espacio-tiempo de Minkowski la m´etrica del tiempo g no es absoluta, s´olo es absoluta la recta Q = Rg que genera en el espacio vectorial de las m´etricas sobre E. Fijar una m´etrica g ∈ Q de tipo (1, 3) signiﬁca ﬁjar la unidad de tiempo, y ﬁjar una m´etrica h ∈ Q de tipo (3, 1) signiﬁca ﬁjar la unidad de longitud. En tal caso tendremos que h = −c2g para cierta constante positiva c. 45 Paradoja de los Gemelos: Consideremos un sistema de referencia inercial e0, e1, e2, e3. Si un viajero parte con velocidad constante ve1 durante un tiempo t, y luego regresa con velocidad constante \u0000ve1, el intervalo de tiempo entre la partida y la llegada es 2t. Calculemos el tiempo que marca el reloj de ese viajero. El trayecto de ida viene dado por el vector e = te0 + vte1, y el de vuelta por e′ = te0 \u0000 vte1. Como pe \u0001 e = pe′ \u0001 e′ = t p1 \u0000 (v/c)2, vemos que marca 2tp 1 \u0000 (v/c)2 < 2t, cuando v 6= 0. Como p 1 \u0000 (v/c)2 ˇ 1 \u0000 1 2 (v/c)2 cuando v ˝ c, su reloj se atrasa en un factor ˇ 1 2 (v/c) 2. Si viaja a Marte durante 1 a˜no a 10.000 km/h y luego regresa, v/c ˇ 10−5 y el tiempo propio es ˇ 3 \u0001 10−3 segundos menor que nuestro propio tiempo. Contracci´on de Longitudes: Consideremos una varilla de longitud l, en reposo en un sistema de referencia inercial e0, e1, e2, e3, y calculemos su longitud l′ para un observador inercial que se mueve con velocidad v en la direcci´on de la varilla (digamos e1). Para el nuevo observador el vector espacial que determina la varilla es e = λe0 + le1 para alg´un λ 2 R, y como ha de ser ortogonal a e0 + ve1 0 = e \u0001 (e0 + ve1) = λ \u0000 vl/c2 , λ = vl/c2 , w = l \u0010 v c2 e0 + e1\u0011 l′ = ph(e, e) = lp1 \u0000 (v/c)2 = l\u00001 \u0000 1 2 (v/c)2 \u0000 . . .\u0001 Las longitudes se contraen en un factor ˇ 1 2 (v/c) 2 cuando v ˝ c. Como el di´ametro de la Tierra es de unos 12,700 km, el viajero a Marte del ejemplo anterior observa una contracci´on de unos 0.6 mm en la direcci´on e1. Deﬁnici´on: Diremos que un vector e 2 E es de tipo tiempo cuando e \u0001 e = g(e, e) > 0, y diremos que τ = pe \u0001 e es el tiempo propio entre p y q = p + e. Cuando e \u0001 e < 0, diremos que el vector e es de tipo espacio, y que σ = ph(e, e) es la distancia propia entre los sucesos p y q = p + e. Cuando e es de tipo tiempo, poniendo e0 = 1 τ e, tenemos que e0 \u0001 e0 = 1. Por el Teorema de Inercia, en alguna base u1, u2, u3 de (Re0) ⊥ la matriz de g es diag(\u00001, \u00001, \u00001), y tomando ei = 1 c ui, obtenemos un observador inercial e0, e1, e2, e3 en el que q \u0000 p = e = τ e0; es decir, p y q ocurren en la misma posici´on, con un intervalo de tiempo τ . Cuando es de tipo espacio, poniendo e1 = 1 σ e, tenemos que h(e1, e1) = 1. Por el Teorema de Inercia, en alguna base u0, e2, e3 de (Re1)⊥ la matriz de h es diag(\u00001, 1, 1), y tomando e0 = cu0, obtenemos un observador inercial e0, e1, e2, e3 en el que q \u0000 p = e = σe1; es decir, p y q ocurren al mismo tiempo, a distancia σ. Velocidad Relativa: Para un observador inercial e0, e1, e2, e3, si un m´ovil puntual se mueve con velocidad relativa constante ⃗v = v1e1 +v2e2 +v3e3 y en un instante t0 est´a en la posici´on (x0, y0, z0), entonces en el instante t0 + t est´a en la posici´on (x0 + v1t, y0 + v2t, z0 + v3t), as´ı que su trayectoria (t0 + t, x0 + v1t, y0 + v2t, z0 + v3t) en E es una recta de direcci´on R(e0 +⃗v ): Una velocidad aparente constante ⃗v se corresponde con una trayectoria espacio-temporal de direcci´on he0 + ⃗v i, y las trayectorias de los objetos en reposo son las rectas p + Re0 paralelas a la trayectoria del observador inercial. Transformaciones de Lorentz: Dado un sistema de referencia inercial e0, e1, e2, e3, si otro observador inercial se mueve con velocidad aparente ve1, su velocidad tetradimensional ser´a e′ 0 = γ(e0 + ve1), γ 2 R, y la condici´on e ′ 0 \u0001 e′ 0 = 1 muestra que γ = 1 q 1 \u0000 v2 c2 = c pc2 \u0000 v2 \u0001 46 Un vector ortogonal a e′ 0 y de m´odulo 1 (para h = \u0000c 2g) es e′ 1 = γ\u0000 v c2 e0 + e1\u0001, as´ı que e′ 0 = γ(e0 + ve1) , e′ 1 = γ\u0000 v c2 e0 + e1\u0001 , e′ 2 = e2 , e′ 3 = e3 es un sistema de referencia inercial que se mueve con velocidad aparente v en la direcci´on e1 (y mantiene los otros dos ejes espaciales). Si (t, x, y, z) son las coordenadas de un suceso en el sistema de referencia inicial, las coordenadas (t ′, x′, y′, z′) del mismo suceso para el nuevo observador vienen dadas por una transformaci´on de Lorentz: 0 B B @ t x y z 1 C C A = 0 B B @ γ γ v c2 0 0 γv γ 0 0 0 0 1 0 0 0 0 1 1 C C A 0 B B @ t′ x′ y′ z′ 1 C C A 0 B B @ t′ x′ y′ z′ 1 C C A = 0 B B @ γ \u0000γ v c2 0 0 \u0000γv γ 0 0 0 0 1 0 0 0 0 1 1 C C A 0 B B @ t x y z 1 C C A Velocidad Tetradimensional: Dado un observador inercial e0, e1, e2, e3, un m´ovil puntual ocupar´a en cada instante t una posici´on \u0000x(t), y(t), z(t) \u0001, y su velocidad aparente es ⃗v = v1e1 + v2e2 + v3e3 = \u0000x′(t), y′(t), z′(t) \u0001 = x′(t)e1 + y′(t)e2 + z′(t)e3, La trayectoria del m´ovil en E es \u0000t, x(t), y(t), z(t) \u0001 = te0 + x(t)e1 + y(t)e2 + z(t)e3 y \u00001, x′(t), y′(t), z′(t)\u0001 = e0 + x′(t)e1 + y′(t)e2 + z′(t)e3 = e0 + ⃗v es un vector tangente a la trayectoria del m´ovil en E, y la direcci´on he0 + ⃗v i de la recta tangente a la trayectoria es independiente del observador inercial considerado. La velocidad 4-dimensional del m´ovil es su desplazamiento inﬁnitesimal en el espacio-tiempo por unidad de tiempo; i.e., es el vector U tangente a su trayectoria en E que cumple U \u0001 U = 1. Luego U = γ(e0 + ⃗v), y la condici´on U \u0001 U = 1 muestra 6 que γ = cpc2−v2 1 −v2 2 −v2 3 . En particular la velocidad 4-dimensional del propio observador inercial, y de los objetos en reposo relativo, es e0. Unidades: He aqu´ı las unidades de las magnitudes que hemos introducido: c e0 e1 e2 e3 t x y z g h ⃗v vi v = j⃗v j U γ m s 1 s 1 m 1 m 1 m s m m m s2 m2 1 s m s m s 1 s s 0m0 7.3. Cu´adricas Centrales Deﬁnici´on: En un espacio eucl´ıdeo real E, las cu´adricas (centradas en el origen) son los lugares geom´etricos de ecuaci´on q(x1, . . . , xn) = cte. para alguna forma cuadr´atica no nula q : E ! R, y podemos suponer que la constante es 1 ´o 0 (caso en que se llama cono). Sea S la m´etrica asociada a la forma cuadr´atica q, y ϕ : E ! E∗ la polaridad asociada a S. Como el producto escalar deﬁne un isomorﬁsmo natural ψ : E ∼\u0000! E∗, ψ(e) = hej, tenemos un endomorﬁsmo T = ψ−1 \u000e ϕ : E ! E tal que ϕ(e) = hT (e)j. Es decir, e \u0001 v = ϕ(e)(v) = hT (e)jvi , 8e, v 2 E, 6La soluci´on negativa no es razonable si queremos que los valores positivos de la coordenada t expresen sucesos futuros, como suele ser costumbre. Con todo rigor, en la deﬁnici´on de espacio-tiempo de Minkowski, adem´as de ﬁjar la m´etrica del tiempo g, se debe ﬁjar, en cada recta V = ⟨e⟩ generada por un vector de tipo tiempo, uno de los dos vectores ±e0 que cumplen e0 · e0 = 1 (la 4-velocidad de las trayectorias inerciales de direcci´on V ) de modo que e0 · ¯e0 sea positivo para cualquier par de velocidades e0, ¯e0. 47 y este endomorﬁsmo T es sim´etrico porque la m´etrica S es sim´etrica: hT (e)jvi = e \u0001 v = v \u0001 e = hT (v)jei = hejT (v)i. Nota: Si A = (aij) es la matriz de la m´etrica S en una base ortonormal e1, . . . , en de E, tambi´en es la matriz en tal base del endomorﬁsmo T porque heijT (ej)i = ei \u0001 ej = aij. En particular, el polinomio caracter´ıstico del endomorﬁsmo T es cT (x) = jxI \u0000 Aj. Teorema 7.7 Sea q una forma cuadr´atica sobre un R-espacio vectorial eucl´ıdeo E y S su m´etrica. En alguna base ortonormal u1, . . . , un de E la matriz de S es diag(α1, . . . , αn) y q(x1u1 + . . . + xnun) = α1x2 1 + . . . + αnx 2 n, donde α1, . . . , αn son las ra´ıces del polinomio caracter´ıstico cT (x) del endomorﬁsmo T aso- ciado a S y al producto escalar, repetidas tantas veces como indique su multiplicidad. Demostraci´on: Por el Teorema Espectral E admite una base ortonormal u1, . . . , un formada por vectores propios del endomorﬁsmo T : T (u1) = α1u1 , T (u2) = α2u2 , . . . , T (un) = αnun ; α1, . . . , αn 2 R. En esta base, la matriz de T (y por tanto la matriz de S ya que la base es ortonormal) es diag(α1, . . . , αn), y por tanto cT (x) = (x \u0000 α1) . . . (x \u0000 αn). Como la matriz de S es diag(α1, . . . , αn), la forma cuadr´atica asociada es q(x1u1 + . . . + xnun) = α1x2 1 + . . . + αnx 2 n. Corolario 7.8 La signatura de una m´etrica S es justamente (r+, r−), donde r+ y r− son el n´umero de ra´ıces positivas y negativas, contadas con su multiplicidad, del polinomio ca- racter´ıstico del endomorﬁsmo asociado a S y a un producto escalar. Regla de Descartes (1596-1650): Si un polinomio con coeﬁcientes reales tiene todas sus ra´ıces reales, entonces el n´umero de ra´ıces positivas, contadas con su multiplicidad, coincide con el n´umero de variaciones de signo en la sucesi´on de coeﬁcientes no nulos. C´alculo de la Signatura de una M´etrica: Sea S una m´etrica en un espacio vectorial real E. Si A es la matriz de S en una base e1, . . . , en de E, y consideramos en E el producto escalar para el que e1, . . . , en es una base ortonormal, entonces A es la matriz en tal base del endomorﬁsmo T : E ! E asociado a la m´etrica S y a ese producto escalar auxiliar y, de acuerdo con 7.8, la signatura de S es justamente (r+, r−), donde r+ y r− son el n´umero de ra´ıces positivas y negativas del polinomio jxI \u0000 Aj, contadas con su multiplicidad. Ejemplo: Se llaman ejes de una cu´adrica central q(x1, . . . , xn) = 1 (´o 0) a las rectas generadas por alg´un vector propio del endomorﬁsmo T asociado a la correspondiente m´etrica y al producto escalar. Por el teorema anterior, toda cu´adrica admite un sistema de ejes perpendiculares en los que su ecuaci´on se reduce a x2 1 a2 1 + . . . + x2 p a2 p \u0000 y2 1 b2 1 \u0000 . . . \u0000 y2 q b2 q = 1 (´o 0). donde 1/a 2 1, . . . , 1/a 2 p y \u00001/b 2 1, . . . , \u00001/b 2 q son las ra´ıces positivas y negativas del polinomio caracter´ıstico del endomorﬁsmo asociado a la forma cuadr´atica y al producto escalar, repe- tidas tantas veces como indique su multiplicidad. 48 Cuando n = 2, las c´onicas centrales reciben los siguientes nombres: x2 a2 + y2 b2 = 1 Elipse x2 a2 + y2 b2 = 0 Par de rectas imaginarias concurrentes x2 a2 \u0000 y2 b2 = 1 Hip´erbola x2 a2 \u0000 y2 b2 = 0 Par de rectas concurrentes \u0000 x2 a2 \u0000 y2 b2 = 1 Elipse imaginaria x2 a2 = 1 Par de rectas paralelas x2 a2 = 0 Par de rectas coincidentes \u0000 x 2 a2 = 1 Par de rectas imaginarias paralelas Cuando n = 3, las cu´adricas centrales reciben los siguientes nombres: x2 a2 + y2 b2 + z2 c2 = 1 Elipsoide x2 a2 + y2 b2 + z2 c2 = 0 Cono imaginario x2 a2 + y2 b2 \u0000 z2 c2 = 1 Hiperboloide reglado x2 a2 + y2 b2 \u0000 z2 c2 = 0 Cono x2 a2 \u0000 y2 b2 \u0000 z2 c2 = 1 Hiperboloide no reglado \u0000 x2 a2 \u0000 y2 b2 \u0000 z2 c2 = 1 Elipsoide imaginario x2 a2 + y2 b2 = 1 Cilindro el´ıptico x2 a2 + y2 b2 = 0 Par de planos imaginarios concurrentes x2 a2 \u0000 y2 b2 = 1 Cilindro hiperb´olico x2 a2 \u0000 y2 b2 = 0 Par de planos concurrentes \u0000 x2 a2 \u0000 y2 b2 = 1 Cilindro imaginario x2 a2 = 1 Par de planos paralelos x2 a2 = 0 Par de planos coincidentes \u0000 x2 a2 = 1 Par de planos imaginarios paralelos 49 Determinaci´on de M´aximos y M´ınimos locales En cada punto cr´ıtico p 2 Rn de una funci´on diferenciable f : Rn ! R tenemos una forma cuadr´atica q : Rn \u0000! R , q(e) = \u0014 Segunda derivada de h(t) = f (p + te) en t = 0 \u0015 y la matriz, en la base usual de Rn, de la m´etrica asociada es el hessiano de f en p: Hess f = \u0012 ∂2f ∂xi∂xj \u0013 . Consideremos la signatura (r+, r−) del hessiano de f en el punto p. 1. Cuando r+ = n, el hessiano es deﬁnido positivo, la funci´on presenta un m´ınimo en todas las direcciones, y tiene un m´ınimo local en el punto cr´ıtico p. 2. Cuando r− = n, el hessiano es deﬁnido negativo, la funci´on presenta un m´aximo en todas las direcciones, y tiene un m´aximo local en el punto cr´ıtico p. 3. Cuando r+ \u0015 1 y r− \u0015 1, la funci´on tiene un m´ınimo en una direcci´on y un m´aximo en otra: el punto cr´ıtico p no es ni m´aximo ni m´ınimo local. 4. En los otros casos, el hessiano no decide el comportamiento local de la funci´on. 50 8. Tensores Deﬁnici´on: Dados K-espacios vectoriales E1, . . . , Er, F , una aplicaci´on K-multilineal de E1, . . . , Er en F es una aplicaci´on M : E1 \u0002 . . . \u0002 Er ! F que es K-lineal en cada variable, M (. . . , λei + µvi, . . .) = λM (. . . , ei, . . .) + µM (. . . , vi, . . .) ; 8ei, vi 2 Ei; λ, µ 2 K. La aplicaci´on nula 0 : E1 \u0002 . . . \u0002 Er ! F es multilineal, y si M, ¯M : E1 \u0002 . . . \u0002 Er ! F son aplicaciones K-multilineales, y λ 2 K, entonces M + ¯M y λM tambi´en son K-multilineales. Por tanto, las aplicaciones K-multilineales de E1, . . . , Er en F forman un subespacio vectorial del K-espacio vectorial formado por todas las aplicaciones de E1 \u0002 . . . \u0002 Er en F . Si M : E1 \u0002 . . . \u0002 Er ! F es una aplicaci´on multilineal, ﬁjada una base ei1, . . . , eini en cada uno de los espacios vectoriales Ei, tendremos que M \u0000 n1P j1=1 λ1j1e1j1, . . . , nrP jr=1 λrjr erjr \u0001 = n1P j1=1 . . . nrP jr=1 λ1j1 . . . λrjr M (e1j1, . . . , erjr ). Teorema 8.1 Fijada una base en cada uno de los espacios vectoriales E1, . . . , Er, si dos aplicaciones multilineales M, ¯M : E1 \u0002 . . . \u0002 Er ! F coinciden en las sucesiones formadas con vectores de tales bases, entonces M = ¯M . Deﬁniciones: Las aplicaciones multilineales T : E\u0002 p. . . \u0002E \u0002 E∗\u0002 q. . . \u0002E∗ \u0000! K reciben el nombre de tensores de tipo (p, q) sobre el K-espacio vectorial E, y forman un K-espacio vectorial T q p E. Por convenio, los tensores de tipo (0,0) son los escalares, T 0 0 E = K. Los tensores de tipo (p, 0) se llaman covariantes de orden p, y los tensores de tipo (0, q) se llaman contravariantes de orden q. Ejemplos: 1. Los tensores covariantes de orden 1 son las formas lineales, y los tensores contravariantes de orden 1 son los vectores, T 0 1 E = E∗ y T 1 0 E = E∗∗ = E, donde cada vector e 2 E deﬁne el tensor contravariante e(ω) := ω(e). 2. Las m´etricas son los tensores covariantes de orden 2 sim´etricos. 3. Cada endomorﬁsmo f : E ! E deﬁne un (1,1)-tensor Tf (e, ω) := ω\u0000f (e) \u0001. 4. Si una m´etrica S es de rango m´aximo, rg S = dim E, entonces la polaridad asociada ϕ : E ! E∗, ϕ(e)(v) = e \u0001 v = S(e, v), es un isomorﬁsmo, de modo que cada tensor covariante deﬁne un tensor contravariante y viceversa. En particular la propia m´etrica S deﬁne una m´etrica dual, que es un 2-tensor contravariante sim´etrico: S∗ : E∗ \u0002 E∗ ! K, S∗(ω, η) = S(ϕ−1ω, ϕ−1η). Si A = (aij) es la matriz de S en una base e1, . . . , en de E, sabemos que A es tambi´en la matriz de la polaridad asociada ϕ : E ! E∗ cuando en E∗ se considera la base dual. Por tanto, si ¯X, ¯Y son las coordenadas de ω y η en la base dual, tendremos que S∗(ω, η) = (A−1 ¯X)tA(A−1 ¯Y ) = ¯X tA−1 ¯Y , y vemos que la matriz de la m´etrica dual S∗ en la base dada es justamente A−1. 5. En un espacio de Minkowski E tenemos la m´etrica del tiempo g y la m´etrica espacial h = \u0000c 2g, que son tensores covariantes de orden 2 sim´etricos, y sus m´etricas duales g∗ y h ∗, que son tensores contravariantes de orden 2 sim´etricos. En un sistema de referencia inercial e0, e1, e2, e3, y su base dual dt, dx, dy, dz, sus matrices son g = diag(1, \u0000 1 c2 , \u0000 1 c2 , \u0000 1 c2 ) , h = \u0000c 2g = diag(\u0000c 2, 1, 1, 1) g∗ = diag(1, \u0000c 2, \u0000c2, \u0000c 2) , h∗ = \u0000 1 c2 g∗ = diag(\u0000 1 c2 , 1, 1, 1) 51 6. En Mec´anica Cl´asica, ﬁjado un origen y unidades de longitud y tiempo, los sucesos forman un espacio-tiempo de Galileo: un espacio vectorial real E de dimensi´on 4, dotado de una forma lineal7 no nula ω : E ! R y de un producto escalar h j i deﬁnido en el subespacio vectorial V := Ker ω (y dim V = 3 por el Teorema de Isomorf´ıa). Un observador o sistema de referencia inercial (con trayectoria a trav´es del origen) es una base e0, ⃗e1, ⃗e2, ⃗e3 de E tal que ω(e0) = 1 y ⃗e1, ⃗e2, ⃗e3 es una base ortonormal de V , y se entiende que las correspondientes coordenadas (t, x, y, z) son el tiempo y la posici´on medidos por el observador inercial: ω = dt, las direcciones de los ejes espaciales son R⃗e1, R⃗e2 y R⃗e3, la velocidad 4-dimensional del observador es e0, y Re0 es la direcci´on de las trayectorias de los objetos en reposo relativo para el observador. La velocidad 4-dimensional de un m´ovil puntual de trayectoria (t, x(t), y(t), z(t)), el vector tangente U tal que ω(U ) = 1, es U = (1, x′(t), y′(t), z′(t)) = e0 + ⃗v. La m´etrica g(e, v) := ω(e)ω(v) se llama m´etrica del tiempo porque el intervalo de tiempo entre dos sucesos p y q = p + e es p g(e, e) = jω(e)j. En un sistema de referencia inercial, la matriz de g es diag(1, 0, 0, 0), y la forma cuadr´atica asociada es dt 2. La polaridad ϕ : V ∼\u0000! V ∗ del producto escalar es un isomorﬁsmo, y en V ∗ tenemos una m´etrica S∗(η1, η2) = hϕ−1(η1)jϕ−1(η2)i. La m´etrica espacial dual h ∗ : E∗ \u0002 E∗ ! R, h ∗(ω1, ω2) := S∗(ω1jV , ω2jV ) es un 2-tensor contravariante sim´etrico, y su matriz en un sistema de referencia inercial es diag(0, 1, 1, 1). 8.1. Producto Tensorial Deﬁnici´on: Dado un (p, q)-tensor T y un (r, s)-tensor ¯T sobre un mismo espacio vectorial E, su producto tensorial T ¯T es el (p + r, q + s)-tensor (T ¯T )(e1, . . . , ep+r, ω1, . . . , ωq+s) = T (e1, . . . , ep, ω1, . . . , ωq)\u0001 ¯T (ep+1, . . . , ep+r, ωq+1, . . . , ωq+s) y por convenio λ T = T λ = λT cuando λ 2 T 0 0 E = K. Veamos que T ¯T es un tensor de tipo (p + r, q + s). Para cualquier ´ındice 1 \u0014 i \u0014 p, poniendo ω = (ω1, . . . , ωq), e′ = (ep+1, . . . , ep+r), y ω′ = (ωq+1, . . . , ωq+s), se cumple que (T ¯T )(. . . , λei + µvi, . . . , ep+r, ω1, . . . , ωq+s) = T (. . . , λei + µvi, . . . , ep, ω) ¯T (e′, ω′) = \u0000λT (. . . , ei, . . . , ep, ω) + µT (. . . , vi, . . . , ep, ω)\u0001 ¯T (e′, ω′) = λT (. . . , ei, . . . , ep, ω1, . . . , ωq) ¯T (e′, ω′) + µT (. . . , vi, . . . , ep, ω1, . . . , ωq) ¯T (e′, ω′) = λ(T ¯T )(. . . , ei, . . . , ep+r, ω1, . . . , ωq+s) + µ(T ¯T )(. . . , vi, . . . , ep+r, ω1, . . . , ωq+s). y de modo an´alogo en cualquier otro ´ındice. Propiedades del Producto Tensorial: 1. (λT + µT ′) ¯T = λ(T ¯T ) + µ(T ′ ¯T ) , T (λ ¯T + µ ¯T ′) = λ(T ¯T ) + µ(T ¯T ′). 2. (T ¯T ) T ′ = T ( ¯T T ′). Demostraci´on: Pongamos e = (e1, . . . , ep), e ′ = (ep+1, . . . , ep+r), ω = (ω1, . . . , ωq), ω′ = (ωq+1, . . . , ωq+s). \u0000(λT + µT ′) ¯T \u0001(e, e′, ω, ω′) = (λT + µT ′)(e, ω) ¯T (e′, ω′) = λT (e, ω) ¯T (e′, ω′) + µT ′(e, ω) ¯T (e′, ω′) = λ(T ¯T )(e, ω, e′, ω′) + µ(T ′ ¯T )(e, ω, e′, ω′) = \u0000λ(T ¯T ) + µ(T ′ ¯T )\u0001(e, e′, ω, ω′) 7donde ω(e), con e = q − p, se interpreta como el intervalo de tiempo, con signo, transcurrido entre los sucesos p y q, y se entiende que el vector e es espacial o de simultaneidad cuando ω(e) = 0. 52 e igualmente se prueba que T (λ ¯T + µ ¯T ′) = λ(T ¯T ) + µ(T ¯T ′). Para la propiedad 2, ponemos e ′′ = (ep+r+1, . . . , ep+r+t), ω′′ = (ωq+s+1, . . . , ωq+s+u). \u0000(T ¯T ) T ′\u0001(e, e′, e′′, ω, ω′, ω′′) = (T ¯T )(e, e′, ω, ω′)T ′(e′′, ω′′) = T (e, ω) ¯T (e′, ω′)T ′(e′′, ω′′) = T (e, ω)( ¯T T ′)(e′, e′′, ω′, ω′′) = \u0000T ( ¯T T ′) \u0001(e, e′, e′′, ω, ω′, ω′′). Nota: Por la propiedad 2, en los productos tensoriales iterados pueden omitirse los par´ente- sis. Por ejemplo, el producto tensorial de p formas lineales ω1, . . . , ωp 2 E∗ y q vectores e1, . . . , eq 2 E es un tensor ω1 . . . ωp e1 . . . eq de tipo (p, q): (ω1 . . . ωp e1 . . . eq)(e ′ 1, . . . , e′ p, ω′ 1, . . . , ω′ q) = ω1(e′ 1) . . . ωp(e′ p)ω′ 1(e1) . . . ω′ q(eq). Teorema 8.2 Sea e1, . . . , en una base de E, y dx1, . . . , dxn su base dual. Una base del espacio vectorial T q p E de los tensores de tipo (p, q) sobre E est´a formada por los tensores dxi1 . . . dxip ej1 . . . ejq , 1 \u0014 i1, . . . , ip, j1, . . . , jq \u0014 n, y en particular, dim (T q p E) = (dim E) p+q. Adem´as todo tensor T de tipo (p, q) sobre E es T = nP i1,...,ip,j1,...,jq=1 T j1...jq i1...ip dxi1 . . . dxip ej1 . . . ejq , T j1...jq i1...ip = T (ei1, . . . , eip , dxj1 , . . . , dxjq ). Demostraci´on: Fijemos una sucesi´on i1, . . . , ip, j1, . . . , jq. Como dxi(ej) = δij, (dxi1 . . . dxip ej1 . . . ejq )(ei1, . . . , eip , dxj1, . . . , dxjq ) = 1, y los restantes tensores de la familia dada se anulan en (ei1, . . . , eip , dxj1, . . . , dxjq ). Luego dxi1 . . . dxip ej1 . . . ejq no es combinaci´on lineal de los restantes tensores, y vemos que la familia considerada es linealmente independiente. Para concluir, basta ver que cualquier tensor T 2 T q p E es T = nP i1,...,ip,j1,...,jq=1T (ei1, . . . , eip, dxj1, . . . , dxjq ) dxi1 . . . dxip ej1 . . . ejq , Ahora bien, ambos t´erminos coinciden en cualquier sucesi´on (ek1, . . . , ekp , dxl1 , . . . , dxlq ), pues valen T (ek1, . . . , ekp , dxl1 , . . . , dxlq ); luego coinciden por 8.1. Ejemplos: 1. El producto tensorial no es conmutativo; pero s´ı cumple que e ω = ω e, 8e 2 E, ω 2 E∗. 2. Los vectores son tensores contravariantes, as´ı que las coordenadas de un vector e 2 E en una base e1, . . . , en de E deber´ıan escribirse con super´ındices, y no con sub´ındices; i.e. poner e = x1e1 + . . . + xnen en vez de e = x1e1 + . . . + xnen. Toda forma lineal ω 2 E∗ es ω = u1dx1 + . . . + undxn, con ui = ω(ei). 3. Sea A = (aij) la matriz de una m´etrica S en una base e1, . . . , en de E, y sea dx1, . . . , dxn la base dual. El teorema aﬁrma que S = P i,j Sijdxi dxj, con Sij = S(ei, ej) = aij, S = P ij aijdxi dxj. Cuando e1, . . . , en es una base ortonormal de un espacio vectorial eucl´ıdeo real, las coor- denadas gij del tensor covariante g : E \u0002 E ! R, g(e, v) = hejvi, que deﬁne el producto escalar son gij = δij. Es decir, g11 = . . . = gnn = 1 y las restantes nulas: g = dx1 dx1 + . . . + dxn dxn. 53 4. Fijado un sistema de referencia inercial e0, e1, e2, e3 en un espacio-tiempo de Minkowski E, y su base dual dt, dx, dy, dz, las m´etricas del tiempo y del espacio g, h, y sus m´etricas duales g∗, h∗, son (donde i = 1, 2, 3 y las coordenadas que se omiten son nulas): g = dt dt \u0000 c −2(dx dx + dy dy + dz dz ; g00 = 1, gii = \u0000c−2, g∗ = e0 e0 \u0000 c 2(e1 e1 + e2 e2 + e3 e3) ; g00 = 1, gii = \u0000c2, h = \u0000c 2dt dt + dx dx + dy dy + dz dz ; h00 = \u0000c 2, hii = 1, h ∗ = \u0000c −2e0 e0 + e1 e1 + e2 e2 + e3 e3 ; h 00 = \u0000c −2, hii = 1. 5. Sea e0, ⃗e1, ⃗e2, ⃗e3 un sistema de referencia inercial en un espacio-tiempo de Galileo (E, ω, h j i), y ω = dt, dx, dy, dz su base dual. La m´etrica del tiempo g y la del espacio h ∗ son g = dt dt ; g00 = 1, h ∗ = ⃗e1 ⃗e1 + ⃗e2 ⃗e2 + ⃗e3 ⃗e3 ; h 11 = h 22 = h 33 = 1 y el tensor de masa-momento T de un ﬂuido perfecto de densidad ρ, presi´on p y 4-velocidad media U es T = ρU U + ph ∗. Es un tensor sim´etrico, T (ω1, ω2) = T (ω2, ω1). 6. En Relatividad, el tensor de energ´ıa-impulso T de un ﬂuido perfecto de densidad ρ, presi´on p y 4-velocidad media U se deﬁne de modo que para ﬂuidos en reposo aparente (U = e0) para un observador inercial coincida con el newtoniano: T = ρe0 e0 + p(e1 e1 + e2 e2 + e3 e3) = (ρ + p c2 )U U + ph∗. 7. Sea A = (aij) la matriz de un endomorﬁsmo f : E ! E en una base e1, . . . , en de E, y sea dx1, . . . , dxn la base dual. Como aij = dxi(T (ej)) = Tf (ej, dxi), vemos que las coordenadas del tensor asociado Tf son T i j = aij. En el c´alculo tensorial, al considerar la matriz A de un endomorﬁsmo, es preferible poner A = (a i j), donde el super´ındice indica la ﬁla y el sub´ındice la columna. 8. El tensor T asociado al operador lineal juihvj (notaci´on de Dirac) es T (e, ω) = ω(hvjeiu) = hvjeiω(u) = (u hvj)(e, ω); es decir, es el tensor jui hvj. 9. Dados (p, q)-tensores T y ¯T de coordenadas T j1...jq i1...ip y ¯T j1...jq i1...ip respectivamente, las coor- denadas de los tensores T + ¯T y λT son (T + ¯T ) j1...jq i1...ip = T j1...jq i1...ip + ¯T j1...jq i1...ip , (λT ) j1...jq i1...ip = λ \u0001 T j1...jq i1...ip . 10. Dado un (p, q)-tensor T , de coordenadas T j1...jq i1...ip , y un (r, s)-tensor ¯T , de coordenadas ¯T j1...js i1...ir , las coordenadas T j1...jq+s i1...ip+r del tensor T ¯T son T j1...jq+s i1...ip+r = T j1...jq i1...ip ¯T jq+1...jq+s ip+1...ip+r . 8.2. Contracci´on de ´Indices Teorema 8.3 Existe una ´unica aplicaci´on lineal C 1 1 : T q p E ! T q−1 p−1 E, (p, q \u0015 1), tal que, para cualesquiera formas lineales ω1, . . . , ωp 2 E∗ y vectores v1, . . . , vq 2 E cumple que C 1 1 (ω1 . . . ωp v1 . . . vq) = ω1(v1) ω2 . . . ωp v2 . . . vq. (13) Demostraci´on: Fijemos una base e1, . . . , en de E, y su base dual dx1, . . . , dxn. De acuerdo con 3.1, existe una ´unica aplicaci´on lineal C 1 1 : T q p E ! T q−1 p−1 E tal que, C 1 1 (dxi1 . . . dxip ej1 . . . ejq ) = δi1j1dxi2 . . . dxip ej2 . . . ejq , 54 es decir, que cumple 13 en las sucesiones (dxi1, . . . , dxip , ej1, . . . , ejq ), y hemos de probar que lo cumple en cualquier sucesi´on (ω1, . . . , ωp, v1, . . . , vq). Ahora bien, los dos t´erminos de la igualdad (13) deﬁnen sendas aplicaciones M, ¯M : E∗\u0002 p. . . \u0002E∗ \u0002 E\u0002 q. . . \u0002E \u0000! T q−1 p−1 E, que son multilineales porque el producto tensorial es lineal en cada factor y la aplicaci´on C11 es lineal. Como M y ¯M coinciden en las sucesiones formadas con vectores de la base dada y su base dual, de acuerdo con 8.1 coinciden en todas las sucesiones. q.e.d. Dado un tensor T de coordenadas T j1...jq i1...ip en una base e1, . . . , en, tenemos que C 1 1 T = C 1 1 \u0010 nP i1,...,jq=1 T j1...jq i1...ip dxi1 . . . dxip ej1 . . . ejq \u0011 = nP i1,...,jq=1 T j1...jq i1...ip C 1 1 (dxi1 . . . dxip ej1 . . . ejq ) = nP i1,...,jq=1 T j1...jq i1...ip δi1j1dxi2 . . . dxip ej2 . . . ejq , as´ı que las coordenadas del tensor C 1 1 T son T j2...jq i2...ip = nP i1,j1=1 T j1...jq i1...ip δi1j1 = nP a=1 T aj2...jq ai2...ip , T j2...jq i2...ip = T aj2...jq ai2...ip donde se adopta el convenio de Einstein: hay una suma extendida a todos los posibles valores de los ´ındices repetidos. Ejemplos: 1. An´alogamente se deﬁne la contracci´on C j i : T q p E ! T q−1 p−1 E del ´ındice covariante i con el ´ındice contravariante j, la contracci´on C kl ij del ´ındice covariante i con el ´ındice contrava- riante k y del ´ındice covariante j con el ´ındice contravariante l, etc. Dado un tensor T de coordenadas T mnp ijkl , las coordenadas del tensor C 3 1 T son T mn jkl = T mna ajkl y las coordenadas del tensor C 32 13 T son T m jl = T mba ajbl . 2. Con el convenio de Einstein, si A = (aij) es una matriz m \u0002 n y B = (bij) es una matriz n \u0002 r, los coeﬁcientes cij de la matriz AB son cij = aikbkj. 3. Dada una forma lineal ω 2 E∗ de coordenadas ui y un vector e 2 E de coordenadas x j, las coordenadas del tensor ω e son T j i = uixj, de modo que ω(e) = C 1 1 (ω e) = T i i = uixi. 4. Dada una m´etrica S : E \u0002 E ! K de matriz (aij), y vectores e, v 2 E de coordenadas xi e yj respectivamente, las coordenadas del tensor S e v son T kl ij = aijxkyl, de modo que C 12 12 (S e v) = T ij ij = aijxiyj = S(e, v). 5. Sea f : E ! E un endomorﬁsmo de matriz A = (aij) y sea Tf el tensor asociado, de coordenadas T j i = aji. Se cumple que C 1 1 (Tf ) = T i i = aii = tr A. Adem´as, dado un vector e 2 E de coordenadas xi, las coordenadas del tensor Tf e son T jk i = T j i xk = ajixk, y las coordenadas del vector C 2 1 (Tf e) son T i = T ij j = aijxj. Vemos que f (e) = C 2 1 (Tf e). Igualmente se ve que f (e) = C 1 1 (e Tf ). 55 Subida y Bajada de ´Indices: Fijada una m´etrica, de coordenadas gij, los ´ındices contra- variantes pueden transformarse en ´ındices covariantes; y ﬁjada una m´etrica en el dual, de coordenadas gij, los ´ındices covariantes pueden transformarse en ´ındices contravariantes: T ...... ...i... = giaT ...a... ...... = gaiT ...a... ...... , T ...j... ...... = gjaT ...... ...a... = gajT ...... ...a... . En particular, cada vector xi deﬁne una forma lineal ui = giaxa, y cada forma lineal ui deﬁne un vector xi = giaua. Cuando una m´etrica g, de coordenadas gij, es de rango m´aximo, rg (gij) = dim E, pode- mos bajar ´ındices con g, y subir ´ındices con la m´etrica dual g∗, cuyas coordenadas gij son los coeﬁcientes de la matriz inversa, (gij) = (gij) −1, as´ı que gikgkj = gikgkj = δij. Ejemplos: Fijemos una m´etrica, de coordenadas gij y una m´etrica contravariante de coor- denadas gij. Dado un tensor T de tipo (2,2), de coordenadas T kl ij , decimos que: 1. El tensor de coordenadas T l ijk = giaT al jk (o bien T l kij = gkaT al ij ) se obtiene del tensor T bajando el primer ´ındice contravariante como primer ´ındice covariante. 2. El tensor de coordenadas T l ijk = gkaT al ij se obtiene del tensor T bajando el primer ´ındice contravariante como tercer ´ındice covariante. 3. El tensor de coordenadas T l ijk = gjaT la ik (o bien T k ilj = glaT ka ij ) se obtiene del tensor T bajando el segundo ´ındice contravariante como segundo ´ındice covariante. 4. El tensor de coordenadas Tijkl = gkaglbT ab ij se obtiene del tensor T bajando los dos ´ındices contravariantes como los dos ´ultimos ´ındices covariante. 5. El tensor de coordenadas T jkl i = glaT jk ia (o bien T klj i = gjaT jk ia ) se obtiene del tensor T subiendo el segundo ´ındice covariante como tercer ´ındice contravariante. 6. El tensor de coordenadas T ijkl = giagjbT kl ab se obtiene del tensor T subiendo los dos ´ındices covariantes como los dos primeros ´ındices contravariantes. Deﬁnici´on: Dado un (p, q)-tensor T , con p \u0015 1, su contracci´on interior con un vector e 2 E es el (p \u0000 1, q)-tensor (ieT )(e2, . . . , ep, ω1, . . . , ωq) = T (e, e2, . . . , ep, ω1, . . . , ωq). Sea e1, . . . , en una base de E y dx1, . . . , dxn su base dual. Si las coordenadas de T y e en esta base son T j1...jq i1...ip y xi respectivamente, entonces las coordenadas T j1...jq i2...ip de ieT son T j1...jq i2...ip = (ieT )(ei2, . . . , eip , dxj1, . . . , dxjq ) = T (e, ei2 , . . . , eip , dxj1 , . . . , dxjq ) = T (xaea, ei2 , . . . , eip , dxj1, . . . , dxjq ) = xaT (ea, ei2 , . . . , eip , dxj1, . . . , dxjq ) = xaT j1...jq ai2...ip . Ejemplos: 1. Este c´alculo de las coordenadas muestra que ieT = C 1 1 (e T ). 2. La aplicaci´on E \u0002 T q p E ! T q p−1E, (e, T ) 7! ieT , es bilineal; es decir, iλe+µvT = λieT + µivT , ie(λT + µ ¯T ) = λieT + µie ¯T . 3. Dada una m´etrica S : E \u0002 E ! K y un vector e 2 E, la forma lineal ωe : E ! K, ωe(v) = S(e, v), es precisamente ωe = ieS. 56 9. Tensores Alternados Deﬁnici´on: Dado un tensor covariante T 2 T 0 p E, para cada permutaci´on σ 2 Sp ponemos (σT )(e1, . . . , ep) = T (eσ(1), . . . , eσ(p)). y esta acci´on del grupo Sp sobre los p-tensores covariantes tiene las siguientes propiedades: 1. La aplicaci´on σ : T 0 p E \u0000! T 0 p E es lineal. 2. τ (σT ) = (τ σ)T ; 8σ, τ 2 Sp. 3. σ(ω1 . . . ωp) = ωσ−1(1) . . . ωσ−1(p); 8ω1, . . . , ωp 2 E∗. Demostraci´on: La primera se deja como ejercicio. Veamos las otras dos: \u0000τ (σT )\u0001(e1, . . . , ep) = (σT )(eτ (1), . . . , eτ (p)) = T (eτ (σ1), . . . , eτ (σp)) = T (e(τ σ)(1), . . . , e(τ σ)(p)) = \u0000(τ σ)T \u0001(e1, . . . , ep), (σ(ω1 . . . ωp))(e1, . . . , ep) = ω1(eσ(1)) \u0001 . . . \u0001 ωp(eσ(p)) = ωσ−1(1)(e1) \u0001 . . . \u0001ωσ−1(p)(ep) = (ωσ−1(1) . . . ωσ−1(p))(e1, . . . , ep). Deﬁnici´on: Un p-tensor covariante S es sim´etrico cuando σS = S para toda permutaci´on σ 2 Sp; y un p-tensor covariante Ω es hemisim´etrico o alternado, (es una p-forma lineal) cuando σΩ = (sgn σ)Ω para toda permutaci´on σ 2 Sp: Ω(eσ(1), . . . , eσ(p)) = (sgn σ)Ω(e1, . . . , ep), y convenimos que las 1-formas lineales son las formas lineales ω 2 E∗. Las p-formas lineales forman un subespacio vectorial de T 0 p E. Ejemplo: En el caso de un 2-tensor covariante Ω, la condici´on de que sea alternado signiﬁca que Ω(e, v) = \u0000Ω(v, e), 8e, v 2 E, y en particular Ω(e, e) = 0, 8e 2 E. De hecho, esta ´ultima condici´on equivale a la de ser alternado, pues implica que 0 = Ω(e + v, e + v) = Ω(e, e) + Ω(e, v) + Ω(v, e) + Ω(v, v) = Ω(e, v) + Ω(v, e). Si Ωij son las coordenadas de Ω en una base de E, la condici´on de ser alternado signiﬁca que Ωij = \u0000Ωji, y en particular Ωij = 0 cuando i = j. Ejemplo: En un espacio-tiempo de Minkowski E las fuerzas electromagn´eticas vienen re- presentadas por un endomorﬁsmo ˜F : E ! E, donde se interpreta que para un observador inercial de 4-velocidad e0, la fuerza que act´ua sobre la unidad de carga en reposo es ˜F (e0). Consideremos el 2-tensor covariante F (e, v) := h( ˜F (e), v), donde h es la m´etrica espacial. La fuerza que mide un observador ha de ser un vector espacial, as´ı que F (e0, e0) = 0, y es natural suponer que el tensor F del campo electromagn´etico es una 2-forma lineal. Fijado un observador inercial e0, e1, e2, e3, el vector E := ˜F (e0) = E1e1 + E2e2 + E3e3 recibe el nombre de campo el´ectrico, de modo que Ei = h(E, ei) = h \u0000 ˜F (e0), ei\u0001 = F (e0, ei) = F0i = \u0000Fi0. Adem´as F00 = F11 = F22 = F33 = 0, y las restante coordenadas Fij de F se denotan F12 = \u0000F21 = \u0000 1 c B3 , F13 = \u0000F31 = 1 c B2 , F23 = \u0000F32 = \u0000 1 c B1 , y se dice que el vector espacial B := B1e1 + B2e2 + B3e3 es el campo magn´etico que mide el observador inercial considerado. Consideremos las coordenadas F j i del tensor asociado a ˜F , de modo que ˜F (ei) = F a i ea. 57 Como F (e, v) = h( ˜F (e), v), las coordenadas Fij del tensor F son Fij = F (ei, ej) = h \u0000 ˜F (ei), ej\u0001 = h \u0000F a i ea, ej\u0001 = F a i h(ea, ej) = F a i haj, as´ı que el tensor F se obtiene al bajar, con la m´etrica h, el ´ındice contravariante del tensor asociado a ˜F . Veamos que el tensor F j i se obtiene al subir con la m´etrica dual h ij el segundo ´ındice covariante del tensor Fij: Fiah aj = F b i hbah aj = F b i δbj = F j i , porque hikh kj = δij. Si A = (aij) es la matriz de ˜F , tenemos que aij = F i j = Fjah ai: A = 0 B B @ 0 1 c2 E1 1 c2 E2 1 c2 E3 E1 0 1 c B3 \u0000 1 c B2 E2 \u0000 1 c B3 0 1 c B1 E3 1 c B2 \u0000 1 c B1 0 1 C C A , jλI \u0000 Aj = λ 4 \u0000 1 c2 (kEk 2 \u0000 kBk 2)λ2 \u0000 1 c4 hEjBi2, y vemos que los siguientes escalares no dependen del observador inercial ﬁjado, son inva- riantes escalares del campo electromagn´etico: kEk 2 \u0000 kBk 2 = h(E, E) \u0000 h(B, B) = E2 1 + E2 2 + E2 3 \u0000 B2 1 \u0000 B2 2 \u0000 B2 3 , hEjBi 2 = h(E, B) = (E1B1 + E2B2 + E3B3) 2. Deﬁnici´on: La hemisimetrizaci´on de un p-tensor covariante T 2 T 0 p E es el tensor h(T ) = P σ∈Sp(sgn σ)(σT ) 2 T 0 p E, que ya es alternado, porque para toda permutaci´on τ 2 Sp se cumple que τ \u0000 P σ∈Sp(sgn σ)(σT ) \u0001 = (sgn τ ) P σ∈Spsgn (τ σ)(τ σT ) = (sgn τ )h(T ). Ejemplos: 1. Si ω es una forma lineal, tenemos que h(ω) = ω. 2. Si ω1, ω2 2 E∗, y σ = (12), tenemos que σ(ω1 ω2) = ω2 ω1. Luego h(ω1 ω2) = ω1 ω2 \u0000 ω2 ω1. 3. Tenemos que h(Ω) = p! Ω, para toda p-forma lineal Ω, porque σΩ = (sgn σ)Ω, 8σ 2 Sp, de modo que h(Ω) = P σ∈Sp (sgn σ)(σΩ) = P σ∈Sp (sgn σ) 2Ω = p! Ω. Lema 9.1 Sea H un subgrupo de un grupo G, y a, b 2 G. Si b 2 aH, entonces aH = bH. Por tanto, los conjuntos aH y bH o son disjuntos o coinciden, Demostraci´on: Si b 2 aH, entonces b = ah para alg´un h 2 H, y bH = ahH \u0012 aH. Como a = bh −1 2 bH, tambi´en tenemos que aH \u0012 bH. Luego aH = bH. Por ´ultimo, si c 2 (aH) \\ (bH), tenemos que aH = cH = bH. Lema 9.2 La aplicaci´on h : T 0 p E \u0000! T 0 p E es lineal, su imagen est´a formada por las p- formas lineales, y si h(T ) = 0, entonces para todo tensor ¯T 2 T 0 q E se cumple que h(T ¯T ) = h( ¯T T ) = 0. 58 Demostraci´on: Las aplicaciones σ : T 0 p E ! T 0 p E son lineales; luego tambi´en h = P σ(sgn σ)σ. Adem´as para toda p-forma lineal Ω tenemos que Ω = h \u0000 1 p! Ω\u0001 2 Im h. Finalmente, identiﬁquemos cada permutaci´on σ 2 Sp con una permutaci´on σ 2 Sp+q que deja ﬁjos los n´umeros p + 1, . . . , p + q. Se cumple que P σ∈Sp(sgn σ)σ(T ¯T ) = h(T ) ¯T = 0, P σ∈Sp(sgn τ σ) \u0000τ σ(T ¯T ) \u0001 = 0, para toda τ 2 Sp+q, y vemos que en h(T ¯T ) la suma correspondiente a los elementos de τ Sp es nula. Como Sp es un subgrupo del grupo Sp+q, por el lema Sp+q puede descomponerse en uni´on disjunta Sp+q = τ1Sp [ . . . [ τrSp, y vemos que h(T ¯T ) = 0. Igualmente se prueba que h( ¯T T ) = 0. 9.1. Producto Exterior Deﬁnici´on: El producto exterior Ω ^ ¯Ω de una p-forma lineal Ω con una q-forma lineal ¯Ω es la (p + q)-forma lineal Ω ^ ¯Ω = h(T ) ^ h( ¯T ) := h(T ¯T ), y no depende de los tensores T y ¯T elegidos: si Ω = h(T ′), entonces T ′ = T +S con h(S) = 0, y por el lema anterior h(T ′ ¯T ) = h(T ¯T + S ¯T ) = h(T ¯T ). Cuando p = 0 (´o q = 0), convenimos que λ ^ Ω = Ω ^ λ = λΩ. El subespacio vectorial de T 0 p E formado por las p-formas lineales se denota ΛpE∗. Ejemplos: 1. Dadas formas lineales ω, ω′ 2 E∗, tenemos que ω ^ ω′ = h(ω) ^ h(ω′) = h(ω ω′) = ω ω′ \u0000 ω′ ω. 2. El producto exterior de p formas lineales ω1, . . . , ωp 2 E∗ es: ω1 ^ . . . ^ ωp = h(ω1) ^ . . . ^ h(ωp) = h(ω1 . . . ωp) = P σ∈Sp(sgn σ)σ(ω1 . . . ωp) (ω1 ^ . . . ^ ωp)(e1, . . . , ep) = P σ∈Sp(sgn σ)ω1(eσ(1)) . . . ωp(eσ(p)) = \f \f \f \f \f \f ω1(e1) . . . ω1(ep) . . . . . . . . . ωp(e1) . . . ωp(ep) \f \f \f \f \f \f . 3. Sea e1, . . . , en una base de E, y dx1, . . . , dxn la base dual. Si A es la matriz cuyas columnas son las coordenadas de ciertos vectores v1, . . . , vn 2 E en la base dada, se cumple que (dx1 ^ . . . ^ dxn)(v1, . . . , vn) = jAj. En particular (dx1 ^ . . . ^ dxn)(e1, . . . , en) = 1. Propiedades del Producto Exterior: 1. Es bilineal : (Ω + Ω′) ^ ¯Ω = Ω ^ ¯Ω + Ω′ ^ ¯Ω , (λΩ) ^ ¯Ω = λ(Ω ^ ¯Ω). Ω ^ ( ¯Ω + ¯Ω′) = Ω ^ ¯Ω + Ω ^ ¯Ω′ , Ω ^ (λ ¯Ω) = λ(Ω ^ ¯Ω). 2. Asociativo: (Ω ^ ¯Ω) ^ Ω′ = Ω ^ ( ¯Ω ^ Ω′). 3. ω ^ ω′ = \u0000ω′ ^ ω, y por tanto ω ^ ω = 0, 8ω, ω′ 2 E∗. 59 4. Sea e1, . . . , en una base de E, y dx1, . . . , dxn su base dual. Las p-formas dxi1 ^ . . . ^ dxip , 1 \u0014 i1 < . . . < ip \u0014 n, deﬁnen una base de Λ pE∗, y para toda p-forma Ω se cumple que Ω = P 1≤i1<...<ip≤n Ωi1...ip dxi1 ^ . . . ^ dxip , Ωi1...ip := Ω(ei1, . . . , eip ). Por tanto dim ΛpE∗ = \u0000n p\u0001, y en particular dim ΛnE∗ = 1 y Λ pE∗ = 0 cuando p > n. 5. Anticonmutativo: Ωp ^ Ωq = (\u00001)pqΩq ^ Ωp, 8Ωp 2 Λ pE∗, Ωq 2 Λ qE∗. 6. ie(Ωp ^ Ωq) = (ieΩp) ^ Ωq + (\u00001) pΩp ^ (ieΩq), 8Ωp 2 ΛpE∗, Ωq 2 ΛqE∗. Demostraci´on: Las propiedades (1) y (2) se siguen de la deﬁnici´on, de la linealidad de h y de las correspondientes propiedades del producto tensorial. En cuanto a (3), tenemos que ω ^ ω′ = ω ω′ \u0000 ω′ ω = \u0000ω′ ^ ω. (4) Como la aplicaci´on lineal h : T 0 p E ! ΛpE∗ es epiyectiva, y los tensores dxi1 . . . dxip generan T 0 p E, se sigue que las p-formas h(dxi1 . . . dxip ) = dxi1 ^ . . . ^ dxip generan ΛpE∗. Como dxi1 ^ . . . ^ dxip = 0 cuando alg´un ´ındice est´a repetido, y dxiσ(1) ^ . . . ^ dxiσ(p) = \u0006dxi1 ^ . . . ^ dxip para toda permutaci´on σ, vemos que, dada una p-forma Ω, hay escalares Ωi1...ip tales que Ω = P 1≤i1<...<ip≤n Ωi1...ip dxi1 ^ . . . ^ dxip . Ahora, ﬁjada una sucesi´on de ´ındices 1 \u0014 j1 < . . . < jp \u0014 n, como se cumple que (dxi1 ^ . . . ^ dxip )(ej1 , . . . , ejp ) = 0, salvo cuando la sucesi´on i1 < . . . < ip coincide con la sucesi´on j1 < . . . < jp, en cuyo caso el valor es 1, vemos que Ωj1...jp = Ω(ej1 , . . . , ejp ). Cuando Ω = 0, concluimos que las p-formas dxi1 ^ . . . ^ dxip , 1 \u0014 i1 < . . . < ip \u0014 n, son linealmente independientes. (5) El caso p = q = 1 es (3). El caso Ωp = ω1 ^ . . . ^ ωp, Ωq = θ1 ^ . . . ^ θq se sigue directamente, y el caso general se sigue de la bilinealidad del producto exterior. (6) Si e = 0, es ambos t´erminos son nulos. Si e 6= 0, ﬁjemos una base e = e1, . . . , en y su base dual ω1, . . . , ωn. Podemos suponer que Ωp = ωi1 ^ . . . ^ ωip = ωi1 ^ ωI , y Ωq = ωj1 ^ . . . ^ ωjq = ωj1 ^ ωJ . Cuando i1 > 1 y j1 > 1, ambos t´erminos son nulos. Cuando i1 = 1 y j1 > 1, ie(Ωp ^ Ωq) = ie(ω1 ^ ωI ^ Ωq) = ωI ^ Ωq, (ieΩp) ^ Ωq + (\u00001)pΩp ^ (ieΩq) = ωI ^ Ωq + (\u00001)pΩp ^ 0 = ωI ^ Ωq, y el caso i1 > 1, j1 = 1 es similar. Cuando i1 = 1 y j1 = 1, ie(Ωp ^ Ωq) = ie(ω1 ^ ωI ^ ω1 ^ ωJ ) = ie(0) = 0, (ieΩp) ^ Ωq + (\u00001)pΩp ^ (ieΩq) = ωI ^ Ωq + (\u00001)pω1 ^ ωI ^ ωJ = ωI ^ Ωq + (\u00001) p(\u00001) p−1ωI ^ ω1 ^ ωJ = ωI ^ Ωq \u0000 ωI ^ Ωq = 0. Ejemplo: Fijado un sistema de referencia inercial e0, e1, e2, e3 en un espacio-tiempo de Minkowski, y su base dual dt, dx, dy, dz, el tensor F del campo electromagn´etico es F = E1dt ^ dx + E2dt ^ dy + E3dt ^ dz \u0000 1 c (B3dx ^ dy \u0000 B2dx ^ dz + B1dy ^ dz). 60 Nota: S´olo hemos estudiado tensores alternados covariantes, porque el estudio de los ten- sores contravariantes alternados se reduce inmediatamente a ellos, pues los tensores contra- variantes sobre un espacio vectorial E son los tensores covariantes sobre su dual E∗. Si e1, . . . , en es una base de E, los p-vectores ei1 ^ . . . ^ eip , 1 \u0014 i1 < . . . < ip \u0014 n, deﬁnen una base del espacio vectorial ΛpE formado por los tensores contravariantes alternados de orden p sobre E. Por otra parte, tambi´en podemos considerar los tensores contravariantes de orden p sim´etricos, que forman un subespacio vectorial SpE de T p 0 E, y tenemos un operador de simetrizaci´on s : T p 0 E ! SpE, s(T ) = P σ∈Sp σT . En Mec´anica Cu´antica, si E es el espacio de estados de cierto tipo de part´ıcula, entonces los posibles estados de un sistema formado por p part´ıculas de dicho tipo vienen dados por tensores contravariantes de orden p, entendiendo que e1 e2 . . . ep representa el estado del sistema cuando el estado de la primera part´ıcula es Ce1, el de la segunda es Ce2, etc. El caso es que las part´ıculas elementales se dividen en bosones y fermiones. En los bosones el espacio de estados es SpE, y en los fermiones el espacio de estados es ΛpE. Fijada una base ψ1, . . . , ψn de E, los textos de Mec´anica Cu´antica escriben ja1, . . . , ani = ( s(ψ1 a1. . . ψ1 . . . ψn an. . . ψn) 2 SpE en los bosones h(ψ1 a1. . . ψ1 . . . ψn an. . . ψn) 2 Λ pE en los fermiones donde los n´umeros naturales a1, . . . , an suman p y se llaman n´umeros de ocupaci´on, pues ai indica el n´umero de part´ıculas que est´an en el estado ψi; aunque, al ser tensores sim´etricos o hemisim´etricos, nunca pueda decirse qu´e part´ıcula est´a en el estado ψi (salvo cuando ai = p, en el caso de los bosones). En el caso de los bosones, los n´umeros ai son arbitrarios, con la ´unica condici´on de que a1 + . . . + an = p; pero en el caso de los fermiones los n´umeros ai s´olo pueden tomar los valores 0 y 1 (Principio de Exclusi´on de Pauli) porque ja1, . . . , ani = h(ψ1 a1. . . ψ1 . . . ψn an. . . ψn) = ψ1^ a1. . . ^ψ1 ^ . . . ^ ψn^ an. . . ^ψn 6= 0. 9.2. Formas de Volumen y Orientaciones Teorema 9.3 Sea E un R-espacio vectorial eucl´ıdeo E de dimensi´on n. Si una n-forma Ω cumple que jΩ(u1, . . . , un)j = 1 en alguna base ortonormal u1, . . . , un de E, entonces jΩ(v1, . . . , vn)j = 1 en toda base ortonormal v1, . . . , vn de E. Demostraci´on: Consideremos la base dual dx1, . . . , dxn de u1, . . . , un. Tenemos que Ω = Ω(u1, . . . , un)dx1 ^ . . . ^ dxn = \u0006dx1 ^ . . . ^ dxn. Si B = (bij) es la matriz de cambio de base, vj = P i bijui, tenemos que (dx1 ^ . . . ^ dxn)(v1, . . . , vn) = det\u0000dxi(vj) \u0001 = det(bij) = jBj. La matriz del producto escalar en la nueva base es BtIB (porque K = R); luego BtB = I porque la base v1, . . . , vn es ortonormal, as´ı que jBj 2 = jBtBj = 1, y jBj = \u00061. Deﬁnici´on: Sea E un R-espacio vectorial eucl´ıdeo de dimensi´on n. Las formas de volumen son las n-formas \u0006ΩE que cumplan jΩE(u1, . . . , un)j = 1 en alguna (luego en toda) base ortonormal u1, . . . , un de E, y diremos que jΩE(e1, . . . , en)j es el volumen del paralelep´ıpedo (´area del paralelogramo, cuando n = 2) determinado por los vectores e1, . . . , en 2 E. Si dx1, . . . , dxn la base dual de una base ortonormal, las formas de volumen son ΩE = \u0006dx1 ^ . . . ^ dxn y el volumen del paralelep´ıpedo que determinan unos vectores e1, . . . , en es el valor absoluto del determinante de la matriz cuyas columnas son las coordenadas de e1, . . . , en en alguna base ortonormal de E (luego nulo si y s´olo si e1, . . . , en son linealmente dependientes). 61 Ejemplo: En un plano eucl´ıdeo real E, dado un paralelogramo de lados e, v, podemos ﬁjar una base ortonormal u1, u2 tal que e = bu1, v = au1 + hu2, donde b es la base y h es la altura, y vemos que el ´area S = jΩE(e, v)j del paralelogramo es la base por la altura: S = jΩE(e, v)j = jΩE(bu1, au1 + hu2)j = jbaΩE(u1, u1) + bhΩE(u1, u2)j = bh, y el ´area de un tri´angulo 8 de v´ertices de coordenadas (x0, y0), (x1, y1), (x2, y2) en una base ortonormal es S 2 = \u0006 1 2 \f \f \f \fx1 \u0000 x0 x2 \u0000 x0 y1 \u0000 y0 y2 \u0000 y0 \f \f \f \f . Proposici´on 9.4 Sea T : E ! E un endomorﬁsmo de un R-espacio vectorial eucl´ıdeo E de dimensi´on n, y sea jAj el valor absoluto del determinante de la matriz de T en cualquier base de E. Se cumple que \u0014 Volumen del paralelep´ıpedo de aristas T (e1), . . . , T (en) \u0015 = jAj \u0014 Vol. del paralelep´ıpedo de aristas e1, . . . , en \u0015 . Demostraci´on: Si e1, . . . , en son linealmente dependientes, P λiei = 0, tambi´en tendremos que 0 = T (P i λiei) = P i λiT (ei), y ambos t´erminos son nulos. Si e1, . . . , en forman una base de E, consideramos la base dual ω1, . . . , ωn, y tendremos que ΩE = λω1 ^ . . . ^ ωn, donde λ = ΩE(e1, . . . , en). Consideramos tambi´en la matriz A = (aij) de T en tal base, T (ej) = P i aijei, donde aij = ωi(T (ej)), y terminamos: ΩE\u0000T (e1), . . . , T (en)\u0001 = λ(ω1 ^ . . . ^ ωn)\u0000T (e1), . . . , T (en)\u0001 = λdet \u0000ωi(T ej) \u0001 = λdet(aij). Proposici´on 9.5 Sea e1, . . . , en una base de E, y dx1, . . . , dxn la base dual. Si (gij) denota la matriz del producto escalar en esta base, gij = ei \u0001 ej, las formas de volumen de E son ΩE = \u0006q jgijj dx1 ^ . . . ^ dxn. Demostraci´on: Tenemos que ΩE = \u0006λdx1 ^. . .^dxn, con λ = \u0006ΩE(e1, . . . , en) = \u0006jBj, don- de B es la matriz de cambio de base de una base ortonormal u1, . . . , un a la base e1, . . . , en. Como (gij) = BtIB, concluimos que jgijj = jBj2. Ejemplo: En un plano, el ´area S del paralelogramo que determinan dos vectores e, v es S = s\f \f \f \fe \u0001 e e \u0001 v v \u0001 e v \u0001 v \f \f \f \f = p (e \u0001 e)(v \u0001 v) \u0000 (e \u0001 v)2. Como a2 = e \u0001 e, b2 = v \u0001 v, c2 = (e \u0000 v) \u0001 (e \u0000 v) = a 2 + b 2 \u0000 2(e \u0001 v), donde a, b, c son las longitudes de los lados del tri´angulo que determinan e y v, obtenemos la f´ormula de Her´on (s. I d.C.) para el ´area S′ = 1 2 S de un tri´angulo: S′ = 1 2 qa2b2 \u0000 1 4 (a2 + b2 \u0000 c2)2 = 1 4 p2(a2b2 + a2c2 + b2c2) \u0000 a4 \u0000 b4 \u0000 c4 = 1 4 p(a + b + c)(a + b \u0000 c)(a \u0000 b + c)(\u0000a + b + c) = ps(s \u0000 a)(s \u0000 b)(s \u0000 c) , s = 1 2 (a + b + c). Deﬁniciones: Dar una orientaci´on en un espacio vectorial eucl´ıdeo real E es dar una (de las dos posibles) forma de volumen ΩE, y diremos que una base e1, . . . , en es directa o que est´a bien orientada cuando ΩE(e1, . . . , en) > 0. 8que se deﬁne como la mitad del ´area del paralelogramo; y en dimensi´on 3, el volumen del tetraedro que determinan tres vectores e, v, w es 1/6 del volumen ΩE (e, v, w) del paralelep´ıpedo que determinan e, v, w. 62 Sea ΩE la forma de volumen de un espacio vectorial eucl´ıdeo real orientado E de dimen- si´on 3. Si e, v 2 E, entonces iv(ieΩE) es una forma lineal, y de acuerdo con 6.4 existe un ´unico vector e \u0002 v 2 E, llamado producto vectorial de e y v, tal que (e \u0002 v) \u0001 w = ΩE(e, v, w) , 8w 2 E. Sea u1, u2, u3 una base ortonormal directa de E, y pongamos e = P i xiui, v = P i yiui. Las coordenadas de e \u0002 v son (e \u0002 v) \u0001 ui = ΩE(e, v, ui), i = 1, 2, 3; luego e \u0002 v = \f \f \f \fx2 y2 x3 y3 \f \f \f \f u1 \u0000 \f \f \f \fx1 y1 x3 y3 \f \f \f \f u2 + \f \f \f \fx1 y1 x2 y2 \f \f \f \f u3 = \f \f \f \f \f \f x1 y1 u1 x2 y2 u2 x3 y3 u3 \f \f \f \f \f \f . En particular u1 \u0002 u2 = u3, u2 \u0002 u3 = u1, u3 \u0002 u1 = u2. Propiedades del Producto Vectorial: 1. Es bilineal : (λe + µe′) \u0002 v = λ(e \u0002 v) + µ(e′ \u0002 v), e \u0002 (λv + µv′) = λ(e \u0002 v) + µ(e \u0002 v′). 2. Es anticonmutativo: e \u0002 v = \u0000v \u0002 e , e \u0002 e = 0. 3. El producto vectorial e\u0002v es ortogonal a ambos factores, y e\u0002v 6= 0 si y s´olo si los vectores e y v son linealmente independientes. En tal caso su m´odulo es el ´area del paralelogramo determinado por e y v, y la base e, v, e \u0002 v es directa. Demostraci´on: Para ver que dos vectores de E coinciden, basta ver que tienen el mismo producto escalar con cualquier vector w 2 E. Ahora bien, \u0000(λe + µe′) \u0002 v\u0001 \u0001 w = ΩE(λe + µe′, v, w) = λΩE(e, v, w) + µΩE(e′, v, w), \u0000λ(e \u0002 v) + µ(e′ \u0002 v) \u0001 \u0001 w = λ(e \u0002 v) \u0001 w + µ(e′ \u0002 v) \u0001 w = λΩE(e, v, w) + µΩE(e′, v, w). (e \u0002 v) \u0001 w = ΩE(e, v, w) = \u0000ΩE(v, e, w) = \u0000(v \u0002 e) \u0001 w, y la linealidad por la derecha se sigue de la linealidad por la izquierda y el car´acter alternado. Veamos tambi´en que e \u0002 v es ortogonal a ambos factores: (e \u0002 v) \u0001 e = ΩE(e, v, e) = 0 , (e \u0002 v) \u0001 v = ΩE(e, v, v) = 0. Si e y v son linealmente dependientes, digamos v = λe, entonces e \u0002 v = λe \u0002 e = 0. Si e y v son linealmente independientes, los ampliamos hasta obtener una base e, v, w de E. Como (e \u0002 v) \u0001 w = ΩE(e, v, w) 6= 0, vemos que e \u0002 v 6= 0. Adem´as, si tomamos un vector u de m´odulo 1 ortogonal al plano Re + Rv tendremos que iuΩE es una formas de ´area de tal plano: Si u1, u2 es una base ortonormal de Re + Rv, entonces u, u1, u2 es una base ortonormal en E, as´ı que (iuΩE)(u1, u2) = ΩE(u, u1, u2) = \u00061. Ahora e \u0002 v = λu, donde jλj = ke \u0002 vk; luego \u0014 ´Area del paralelo– gramo de aristas e, v \u0015 = j(iuΩE)(e, v)j = jΩE(u, e, v)j = j(e \u0002 v) \u0001 uj = jλu \u0001 uj = ke \u0002 vk. Por ´ultimo, la base e, v, e \u0002 v es directa porque ΩE(e, v, e \u0002 v) = (e \u0002 v) \u0001 (e \u0002 v) > 0. 9.3. Determinantes Teorema 9.6 Unos vectores e1, . . . , ep 2 E son linealmente independientes si y s´olo si existe alguna p-forma lineal Ω tal que Ω(e1, . . . , ep) 6= 0. En particular, ﬁjada una n-forma no nula Ω en un espacio vectorial E de dimensi´on n, unos vectores e1, . . . , en forman una base de E si y s´olo si Ω(e1, . . . , en) 6= 0. 63 Demostraci´on: Si son linealmente independientes, los ampliamos hasta obtener una base e1, . . . , ep, . . . , en de E. Sea dx1, . . . , dxp, . . . , dxn la base dual. Tenemos que (dx1 ^ . . . ^ dxp)(e1, . . . , ep) = det\u0000dxi(ej)\u0001 = det(δij) = 1 6= 0. Si son linealmente dependientes, alguno (digamos e1) es combinaci´on lineal de los res- tantes, e1 = λ2e2 + . . . + λpep, y para toda p-forma Ω tenemos que Ω(e1, e2, . . . , ep) = λ2Ω(e2, e2, . . . , ep) + . . . + λpΩ(ep, e2, . . . , ep) = 0. Nota: Fijada una base e1, . . . , en en un espacio vectorial E, y su base dual dx1, . . . , dxn, como las formas dxi1 ^ . . . ^ dxip , 1 \u0014 i1 < . . . < ip \u0014 n, deﬁnen una base de las p-formas, vemos que p vectores vj = P i aijei, j = 1, . . . , p son linealmente independientes si y s´olo si para alguna sucesi´on 1 \u0014 i1 < . . . < ip \u0014 n se cumple que 0 6= (dxi1 ^ . . . ^ dxip )(v1, . . . , vp) = \f \f \f \f \f \f dxi1(v1) . . . dxi1(vp) . . . . . . . . . dxip (v1) . . . dxip (vp) \f \f \f \f \f \f = \f \f \f \f \f \f ai11 . . . ai1p . . . . . . . . . aip1 . . . aipp \f \f \f \f \f \f , es decir, que la matriz (aij) tiene alg´un menor de orden p no nulo. Vemos as´ı que el teorema anterior es una expresi´on intr´ınseca del Teorema del Rango. Deﬁnici´on: Sea E un espacio vectorial de dimensi´on n. Como dim ΛnE∗ = \u0000n n\u0001 = 1, todo endomorﬁsmo ΛnE∗ ! ΛnE∗ es la multiplicaci´on por cierto escalar. Ahora bien, cada endomorﬁsmo T : E ! E induce un endomorﬁsmo T ∗ : ΛnE∗ ! Λ nE∗ , (T ∗Ω)(e1, . . . , en) = Ω(T (e1), . . . , T (en)), que ha de ser la multiplicaci´on por un escalar det(T ), llamado determinante de T , Ω\u0000T (e1), . . . , T (en)\u0001 = (T ∗Ω)(e1, . . . , en) = (detT ) Ω(e1, . . . , en) , Ω 2 ΛnE∗. (14) Proposici´on 9.7 El determinante de un endomorﬁsmo T : E ! E coincide con el deter- minante de su matriz A = (aij) en cualquier base, det(T ) = jAj. Demostraci´on: Sea e1, . . . , en una base de E, y dx1, . . . , dxn su base dual. La matriz (aij) de T cumple que aij = dxi(T (ej)), y como (dx1 ^ . . . ^ dxn)(e1, . . . , en) = 1, concluimos: detT = (detT )(dx1 ^ . . . ^ dxn)(e1, . . . , en) = \u0000T ∗(dx1 ^ . . . ^ dxn)\u0001(e1, . . . , en) = (dx1 ^ . . . ^ dxn) \u0000T (e1), . . . , T (en)\u0001 = jdxi(T (ej))j = jaijj. Teorema 9.8 det(T \u000e S) = (detT )(detS). Demostraci´on: (T S) ∗ = S∗T ∗. Teorema 9.9 Un endomorﬁsmo T es un isomorﬁsmo si y s´olo si det(T ) 6= 0. Demostraci´on: Fijada una base e1, . . . , en en E, y una n-forma 0 6= Ω 2 ΛnE∗, se cumple Ω\u0000T (e1), . . . , T (en) \u0001 = (detT ) Ω(e1, . . . , en), de acuerdo con 14. Por 9.6, tenemos que det(T ) 6= 0 si y s´olo si T (e1), . . . , T (en) es una base de E, lo que equivale a que T sea un isomorﬁsmo. Nota: El teorema 9.8 aﬁrma que el determinante de un producto de matrices es el producto de sus determinantes, y 9.9 que las matrices invertibles son las de determinante no nulo. 64 ´Indice alfab´etico adjunto, 7 ´angulo, 26 anillo, 11 conmmutativo, 11 aplicaci´on, 5 antilineal, 20 biyectiva, 5 epiyectiva, 5 inversa, 5 inyectiva, 5 lineal, 20 multilineal, 51 argumento, 4 base, 14 directa, 62 dual, 37 ortonormal, 26 usual, 14 bra, 37 campo el´ectrico, 57 magn´etico, 57 ciclo, 6 ciclos disjuntos, 6 cociente de n´umeros complejos, 3 composici´on, 5 conjugado, 3 cono, 47 contracci´on de ´ındices, 54 interior, 56 convenio de Einstein, 55 coordenadas, 14 cu´adrica, 47 cuerpo, 11 dependencia lineal, 14 determinante, 7, 31, 64 diferencial, 38 dimensi´on, 15 direcci´on, 13 distancia, 25 ecuaciones impl´ıcitas, 17 ecuaciones param´etricas, 17 eje de una cu´adrica, 48 endomorﬁsmo, 30 adjunto, 40 autoadjunto, 34 diagonalizable, 32 escalar, 11 , producto, 25 espacio vectorial, 11 dual, 37 eucl´ıdeo, 26 herm´ıtico, 26 espacio-tiempo de Minkowski, 45 espaciotiempo de Galileo, 52 espectro de un endomorﬁsmo, 30 exponencial compleja, 4 f´ormula de Euler, 4 forma cuadr´atica, 42 de volumen, 61 lineal, 37, 57 generadores, 14 gradiente, 38 grupo, 8 abeliano, 8 alternado, 10 conmmutativo, 8 especial, 10 lineal, 9 ortogonal, 9 sim´etrico, 9 unitario, 9 hemisimetrizaci´on, 58 hessiano, 50 identidad, 5 imagen, 5, 10 independencia lineal, 14 isometr´ıa, 25 isomorﬁsmo, 23 ket, 39 logaritmo, 4 m´etodo de Gram-Schmidt, 28 m´etrica, 42 de Lorentz, 45 del tiempo, 45, 52 65 dual, 51 espacial, 45, 52 m´odulo de un n´umero complejo, 3 vector, 25 matriz, 6 autoadjunta, 34 conjugada, 6 de una m´etrica, 42 herm´ıtica, 34 inversa, 7 invertible, 7 sim´etrica, 34 traspuesta, 6 unidad, 7 unitaria, 27 menor de una matriz, 8 principal, 43 morﬁsmo de grupos, 10 multiplicidad de una ra´ız, 30 n´ucleo, 10 n´umeros complejos, 3 observador inercial, 45, 52 operaci´on externa, 11 operaci´on interna, 8 operador herm´ıtico, 34 lineal, 30 unitario, 25 orientaci´on, 62 ortogonal, 25 paralelismo, 16 parte imaginaria, 3 parte real, 3 permutaci´on, 5 impar, 6 par, 6 plano, 16 polaridad, 44 polinomio caracter´ıstico, 31 producto de matrices, 6 de n´umeros complejos, 3 directo, 13 escalar, 25 exterior, 59 tensorial, 52 proyecci´on ortogonal, 29 punto, 11 cr´ıtico, 38 medio, 16 ra´ız simple, 30 rango, 8, 44 recta, 16 referencia inercial, 45, 52 regla de Cr´amer, 7 Descartes, 48 Ruﬃni, 30 restricci´on, 20 segmento, 16 signatura, 44 signo de una permutaci´on, 6 simetrizaci´on, 61 subespacio incidente, 39 ortogonal, 28 propio, 30 vectorial, 11 subgrupo, 9 subvariedad lineal, 13 suma de n´umeros complejos, 3 de subespacios vectoriales, 12 directa, 19 suplementario, 19 tensor, 51 alternado, 57 contravariante, 51 covariante, 51 de energ´ıa-impulso, 54 de masa-momento, 54 hemisim´etrico, 57 sim´etrico, 57 teorema de D’Alembert, 30 de Hamilton-Cayley, 32 de Rouch´e-Frob¨enius, 8, 18 del rango, 8 espectral, 34 transformaciones elementales, 7 trasposici´on, 6 traza, 20, 31 valor propio, 30 vector, 11 is´otropo, 43 propio, 30 velocidad 4-dimensional, 47, 52 volumen, 61 66","libVersion":"0.2.3","langs":""}